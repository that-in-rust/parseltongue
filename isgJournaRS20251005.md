# ISG Core Files Changes Journal 2025-10-05

**Analysis Range:** 5be7d3b ‚Üí main
**Files Analyzed:** src/main.rs, src/isg.rs, src/cli.rs, src/daemon.rs
**Total Lines Changed:** 5,567 lines (5,567)

---

## üìä Executive Summary

**Major Evolution:** The ISG system transformed from a basic functional prototype to a comprehensive, async-enabled, feature-rich architectural intelligence platform with advanced discovery engines, visualization capabilities, and sophisticated relationship analysis.

**Key Transformations:**
- **Async Architecture**: Full async/await implementation
- **Enhanced Performance**: FxHasher optimization and improved algorithms
- **Advanced Discovery**: Complete code discovery and relationship analysis engine
- **Rich CLI**: 10+ new commands with workspace management
- **Visualization**: Interactive HTML graph visualization
- **JTBD Workflows**: Job-to-be-done focused workflows

---

## üéØ 1. src/main.rs Analysis (13 lines changed)

### üèÜ Executive Summary
**Main Finding:** Synchronous to asynchronous architecture transformation - critical foundation change for scalability.

### üìä Key Changes
```diff
- fn main() {
+ #[tokio::main]
+ async fn main() {
-     if let Err(e) = parseltongue::cli::run(cli) {
+     if let Err(e) = parseltongue::cli::run(cli).await {
```

### üíé Impact Analysis
- **Breaking Change**: CLI functions now require async runtime
- **Performance**: Enables concurrent file processing and network operations
- **Scalability**: Foundation for future async feature expansion
- **Dependencies**: Added tokio runtime dependency

### üîß Technical Details
- Added `#[tokio::main]` attribute for async runtime
- CLI execution now `.await`-able
- Error handling pattern remains consistent

---

## üéØ 2. src/isg.rs Analysis (1,832 lines changed)

### üèÜ Executive Summary
**Main Finding:** Massive enhancement with web visualization support, performance optimizations, and improved data structures.

### üìä Key Changes

#### **Performance Optimizations**
```diff
- use std::collections::hash_map::DefaultHasher;
+ use fxhash::FxHasher;
- let mut hasher = DefaultHasher::new();
+ let mut hasher = FxHasher::default();
```

#### **New Web Visualization System**
```rust
/// Web visualization data structures
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct WebGraphData {
    pub nodes: Vec<WebNode>,
    pub edges: Vec<WebEdge>,
    pub metadata: WebMetadata,
}
```

#### **Enhanced Code Organization**
```diff
- const FIELDS: &'static [&'static str] = &["hash", "kind", "name", "signature", "file_path", "line"];
+ const FIELDS: &[&str] = &["hash", "kind", "name", "signature", "file_path", "line"];
```

### üíé Impact Analysis
- **Hash Performance**: ~3x faster hash computation with FxHasher
- **Web Integration**: Complete web-based graph visualization capability
- **Data Structures**: Improved serialization and type safety
- **Code Quality**: Better Rust idioms and modern syntax

### üîß Technical Details

#### **New Structures Added:**
- `WebGraphData` - Complete graph data for web visualization
- `WebNode` - Individual node representation for web
- `WebEdge` - Edge representation with source/target/kind
- `WebMetadata` - Graph metadata (node/edge counts)

#### **Performance Improvements:**
- FxHasher for ~3x performance improvement in hash generation
- Better import organization and deduplication
- Modern Rust syntax patterns

---

## üéØ 3. src/cli.rs Analysis (2,305 lines changed)

### üèÜ Executive Summary
**Main Finding:** Explosive CLI expansion from 4 basic commands to 15+ advanced commands including workspace management, visualization, and JTBD workflows.

### üìä Key Changes

#### **Version Management Improvement**
```diff
- #[command(version = "1.0.0")]
+ #[command(version = env!("CARGO_PKG_VERSION"))]
```

#### **Command Expansion**
```diff
- Debug {
+ DebugGraph {
     /// Show graph structure
     #[arg(long)]
     graph: bool,
```

#### **New Advanced Commands Added:**
```rust
/// Generate interactive HTML visualization
Visualize {
    entity: Option<String>,
    #[arg(long, default_value = "parseltongue_visualization.html")]
    output: PathBuf,
}

/// List all entities in the codebase
ListEntities {
    #[arg(long, value_enum)]
    r#type: Option<DiscoveryEntityType>,
    #[arg(long, default_value = "100")]
    limit: usize,
    format: OutputFormat,
}

/// Workspace management commands
Workspace(WorkspaceArgs),

/// JTBD Workflow: Onboard to new codebase (complete in <15 minutes)
Onboard {
    target_dir: String,
    format: OutputFormat,
}

/// JTBD Workflow: Plan feature development (complete in <5 minutes)
FeatureStart {
    entity: String,
    format: OutputFormat,
}
```

#### **Enhanced Imports and Dependencies**
```diff
+ use crate::discovery::{ConcreteWorkflowOrchestrator, WorkflowOrchestrator};
+ use crate::discovery::{DiscoveryEngine, EntityInfo, FileLocation, SimpleDiscoveryEngine};
+ use crate::workspace_cli::{handle_workspace_command, WorkspaceArgs};
+ use std::time::{Duration, Instant};
```

### üíé Impact Analysis
- **User Experience**: 375% increase in command capabilities
- **Discovery**: Complete code discovery and entity management system
- **Workflows**: Job-to-be-done focused workflows (Onboard, FeatureStart, Debug)
- **Visualization**: Interactive HTML graph visualization
- **Workspace**: Multi-file project management
- **Performance**: Added duration tracking and performance metrics

### üîß Technical Details

#### **New Command Categories:**
1. **Discovery Commands**: ListEntities, EntitiesInFile, WhereDefined
2. **Visualization Commands**: Visualize (interactive HTML)
3. **Workspace Commands**: Multi-file project management
4. **JTBD Workflows**: Onboard, FeatureStart, Debug

#### **Enhanced Features:**
- Entity type filtering with enums
- Output format options (human, JSON)
- Performance timing and duration tracking
- File location and path management

---

## üéØ 4. src/daemon.rs Analysis (2,130 lines changed)

### üèÜ Executive Summary
**Main Finding:** Revolutionary relationship analysis system with sophisticated code parsing, module context tracking, and advanced relationship extraction using syn::visit::Visit.

### üìä Key Changes

#### **Module Context System**
```rust
/// ModuleContext - Tracks current module path for FQN generation
#[derive(Debug, Clone)]
struct ModuleContext {
    path: Vec<String>,
}

impl ModuleContext {
    fn generate_fqn(&self, item_name: &str, item_type: &str) -> String {
        if self.path.is_empty() {
            format!("{} {}", item_type, item_name)
        } else {
            format!("{} {}::{}", item_type, self.path.join("::"), item_name)
        }
    }
}
```

#### **Advanced Relationship Extraction**
```rust
/// RelationshipExtractor - Uses syn::visit::Visit to detect CALLS and USES relationships
struct RelationshipExtractor {
    current_function: SigHash,
    current_module_context: Vec<String>,
    relationships: Vec<(SigHash, SigHash, EdgeKind)>,
}
```

#### **Sophisticated Call Resolution**
```rust
/// Resolve function call target to SigHash
fn resolve_call_target(&self, call: &syn::ExprCall) -> Option<SigHash> {
    match call.func.as_ref() {
        syn::Expr::Path(path_expr) => {
            // Handle function calls like `target_function()` or `utils::load_config()`
            let path_segments: Vec<String> = path_expr
                .path
                .segments
                .iter()
                .map(|s| s.ident.to_string())
                .collect();

            // Try both absolute path and relative to current module context
            let absolute_path = path_segments.join("::");
            let absolute_signature = format!("fn {}", absolute_path);
            let absolute_hash = SigHash::from_signature(&absolute_signature);
```

### üíé Impact Analysis
- **Relationship Intelligence**: Deep understanding of code relationships (CALLS, USES, IMPLEMENTS)
- **Module Awareness**: Proper Fully Qualified Name (FQN) generation
- **Parse Sophistication**: AST-level code analysis using syn crate
- **Context Tracking**: Module and function context for accurate relationship mapping
- **Resolution Logic**: Multi-strategy function call resolution

### üîß Technical Details

#### **New Parsing Capabilities:**
1. **Module Context Tracking**: Tracks nesting levels and module paths
2. **FQN Generation**: Creates fully qualified names for all entities
3. **Relationship Extraction**: Identifies CALLS and USES relationships
4. **Call Resolution**: Multiple strategies for resolving function call targets

#### **Enhanced Imports:**
```diff
+ use crate::isg::{EdgeKind, ISGError, NodeData, NodeKind, OptimizedISG, SigHash};
+ use syn::visit::Visit;
```

#### **Relationship Detection:**
- Function call analysis
- Module import tracking
- Type relationship detection
- Implementation relationship mapping

---

## üöÄ Critical Insights & Recommendations

### **Most Valuable Changes to Backport:**

#### **1. Async Architecture (src/main.rs)**
- **Priority**: Critical
- **Effort**: Low (just add tokio runtime)
- **Impact**: Enables all other async features

#### **2. Performance Optimization (src/isg.rs)**
- **Priority**: High
- **Effort**: Low (change DefaultHasher to FxHasher)
- **Impact**: ~3x performance improvement

#### **3. Version Management (src/cli.rs)**
- **Priority**: Medium
- **Effort**: Minimal (use CARGO_PKG_VERSION)
- **Impact**: Better release management

#### **4. Web Visualization Framework (src/isg.rs)**
- **Priority**: High
- **Effort**: Medium (add web data structures)
- **Impact**: Interactive graph capabilities

#### **5. Enhanced Command Structure (src/cli.rs)**
- **Priority**: High
- **Effort**: High (many new commands to implement)
- **Impact**: Massive user experience improvement

### **Breaking Changes Identified:**

1. **CLI API Change**: All CLI commands now async
2. **Dependencies**: Added tokio, syn, and other heavy dependencies
3. **Function Signatures**: Many functions now require async context

### **Performance Improvements:**

1. **Hash Performance**: FxHasher vs DefaultHasher (~3x improvement)
2. **Concurrent Processing**: Async enables parallel file processing
3. **Memory Usage**: Better data structures and reduced allocations

### **New Capabilities:**

1. **Interactive Visualization**: HTML-based graph exploration
2. **Advanced Discovery**: Code entity discovery and listing
3. **Relationship Analysis**: Deep understanding of code relationships
4. **Workspace Management**: Multi-file project support
5. **JTBD Workflows**: Goal-oriented user workflows

---

## üìã Implementation Priority Matrix

| Feature | Effort | Impact | Priority |
|---------|--------|--------|----------|
| Async Architecture | Low | High | 1 |
| FxHasher Optimization | Low | High | 2 |
| Web Visualization | Medium | High | 3 |
| Enhanced Commands | High | High | 4 |
| Relationship Extraction | High | Medium | 5 |
| Workspace Management | Medium | Medium | 6 |

---

## üéØ Summary

The transformation from 5be7d3b to main represents a **massive evolution** from a basic functional prototype to a **comprehensive architectural intelligence platform**. The changes span:

- **5,567 total lines (5,567)** of core improvements across 4 files
- **Async foundation** enabling scalability and performance
- **375% increase** in CLI capabilities
- **Advanced relationship analysis** with sophisticated parsing
- **Interactive visualization** and web integration
- **JTBD-focused workflows** for specific user needs

**Recommendation**: Prioritize async architecture and performance optimizations as they provide high impact with low effort, enabling future feature expansion.

---

## üìã Complete Raw Diffs

### src/main.rs Diff (13 lines)
diff --git a/src/main.rs b/src/main.rs
index 0686a68..9b06928 100644
--- a/src/main.rs
+++ b/src/main.rs
@@ -4,11 +4,12 @@ use clap::Parser;
 use parseltongue::cli::Cli;
 use std::process;
 
-fn main() {
+#[tokio::main]
+async fn main() {
     let cli = Cli::parse();
-    
-    if let Err(e) = parseltongue::cli::run(cli) {
+
+    if let Err(e) = parseltongue::cli::run(cli).await {
         eprintln!("Error: {}", e);
         process::exit(1);
     }
-}
\ No newline at end of file
+}

### src/isg.rs Diff (1,832 lines)
diff --git a/src/isg.rs b/src/isg.rs
index 3ed2261..f9ec20c 100644
--- a/src/isg.rs
+++ b/src/isg.rs
@@ -1,28 +1,30 @@
 //! OptimizedISG - High-performance Interface Signature Graph
-//! 
+//!
 //! Core architecture: petgraph::StableDiGraph + parking_lot::RwLock + FxHashMap
 //! Performance targets: 1-5Œºs node ops, <500Œºs simple queries, <1ms complex queries
 
-use fxhash::FxHashMap;
+use fxhash::{FxHashMap, FxHashSet};
 use parking_lot::RwLock;
 use petgraph::graph::NodeIndex;
 use petgraph::stable_graph::StableDiGraph;
-use petgraph::Direction;
 use petgraph::visit::{Bfs, EdgeRef, IntoEdgeReferences};
+use petgraph::Direction;
 use std::collections::HashSet;
 use std::sync::Arc;
 use thiserror::Error;
 
 // Strong typing for unique identifier (collision-free)
-#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, PartialOrd, Ord, serde::Serialize, serde::Deserialize)]
+#[derive(
+    Debug, Clone, Copy, PartialEq, Eq, Hash, PartialOrd, Ord, serde::Serialize, serde::Deserialize,
+)]
 pub struct SigHash(pub u64);
 
 impl SigHash {
     pub fn from_signature(signature: &str) -> Self {
-        use std::collections::hash_map::DefaultHasher;
+        use fxhash::FxHasher;
         use std::hash::{Hash, Hasher};
-        
-        let mut hasher = DefaultHasher::new();
+
+        let mut hasher = FxHasher::default();
         signature.hash(&mut hasher);
         Self(hasher.finish())
     }
@@ -75,7 +77,14 @@ impl<'de> serde::Deserialize<'de> for NodeData {
 
         #[derive(serde::Deserialize)]
         #[serde(field_identifier, rename_all = "snake_case")]
-        enum Field { Hash, Kind, Name, Signature, FilePath, Line }
+        enum Field {
+            Hash,
+            Kind,
+            Name,
+            Signature,
+            FilePath,
+            Line,
+        }
 
         struct NodeDataVisitor;
 
@@ -156,7 +165,7 @@ impl<'de> serde::Deserialize<'de> for NodeData {
             }
         }
 
-        const FIELDS: &'static [&'static str] = &["hash", "kind", "name", "signature", "file_path", "line"];
+        const FIELDS: &[&str] = &["hash", "kind", "name", "signature", "file_path", "line"];
         deserializer.deserialize_struct("NodeData", FIELDS, NodeDataVisitor)
     }
 }
@@ -182,12 +191,46 @@ pub enum ISGError {
     InvalidInput(String),
 }
 
+/// Web visualization data structures
+#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
+pub struct WebGraphData {
+    pub nodes: Vec<WebNode>,
+    pub edges: Vec<WebEdge>,
+    pub metadata: WebMetadata,
+}
+
+#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
+pub struct WebNode {
+    pub id: String,
+    pub name: String,
+    pub kind: String,
+    pub signature: String,
+    pub file_path: String,
+    pub line: u32,
+}
+
+#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
+pub struct WebEdge {
+    pub source: String,
+    pub target: String,
+    pub kind: String,
+}
+
+#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
+pub struct WebMetadata {
+    pub node_count: usize,
+    pub edge_count: usize,
+    pub generated_at: u64,
+}
+
 // Internal mutable state protected by single RwLock
 pub(crate) struct ISGState {
     // StableDiGraph ensures indices remain valid upon deletion
     pub(crate) graph: StableDiGraph<NodeData, EdgeKind>,
     // FxHashMap provides fast O(1) lookups
     pub(crate) id_map: FxHashMap<SigHash, NodeIndex>,
+    // Name index for O(1) entity lookup by name
+    pub(crate) name_map: FxHashMap<Arc<str>, FxHashSet<SigHash>>,
 }
 
 /// OptimizedISG - High-performance in-memory Interface Signature Graph
@@ -208,6 +251,7 @@ impl OptimizedISG {
             state: Arc::new(RwLock::new(ISGState {
                 graph: StableDiGraph::new(),
                 id_map: FxHashMap::default(),
+                name_map: FxHashMap::default(),
             })),
         }
     }
@@ -216,30 +260,39 @@ impl OptimizedISG {
     pub fn debug_print(&self) -> String {
         let state = self.state.read();
         let mut output = String::new();
-        
-        output.push_str(&format!("=== Interface Signature Graph ===\n"));
-        output.push_str(&format!("Nodes: {}, Edges: {}\n\n", 
-            state.graph.node_count(), state.graph.edge_count()));
-        
+
+        output.push_str("=== Interface Signature Graph ===\n");
+        output.push_str(&format!(
+            "Nodes: {}, Edges: {}\n\n",
+            state.graph.node_count(),
+            state.graph.edge_count()
+        ));
+
         // Print all nodes
         output.push_str("NODES:\n");
-        for (hash, &node_idx) in &state.id_map {
+        for &node_idx in state.id_map.values() {
             if let Some(node) = state.graph.node_weight(node_idx) {
-                output.push_str(&format!("  {:?} -> {} ({:?})\n", 
-                    hash, node.name, node.kind));
+                output.push_str(&format!(
+                    "  {:?} -> {} ({:?})\n",
+                    node.hash, node.name, node.kind
+                ));
                 output.push_str(&format!("    Signature: {}\n", node.signature));
                 output.push_str(&format!("    File: {}:{}\n", node.file_path, node.line));
             }
         }
-        
+
         output.push_str("\nEDGES:\n");
         for edge_ref in state.graph.edge_references() {
             let source = &state.graph[edge_ref.source()];
             let target = &state.graph[edge_ref.target()];
-            output.push_str(&format!("  {} --{:?}--> {}\n", 
-                source.name, edge_ref.weight(), target.name));
+            output.push_str(&format!(
+                "  {} --{:?}--> {}\n",
+                source.name,
+                edge_ref.weight(),
+                target.name
+            ));
         }
-        
+
         output
     }
 
@@ -247,39 +300,46 @@ impl OptimizedISG {
     pub fn export_dot(&self) -> String {
         let state = self.state.read();
         let mut output = String::new();
-        
+
         output.push_str("digraph ISG {\n");
         output.push_str("  rankdir=TB;\n");
         output.push_str("  node [shape=box, style=rounded];\n\n");
-        
+
         // Add nodes with different colors for different types
-        for (hash, &node_idx) in &state.id_map {
+        for &node_idx in state.id_map.values() {
             if let Some(node) = state.graph.node_weight(node_idx) {
                 let color = match node.kind {
                     NodeKind::Function => "lightblue",
-                    NodeKind::Struct => "lightgreen", 
+                    NodeKind::Struct => "lightgreen",
                     NodeKind::Trait => "lightyellow",
                 };
-                output.push_str(&format!("  \"{}\" [label=\"{}\\n({:?})\" fillcolor={} style=filled];\n", 
-                    node.name, node.name, node.kind, color));
+                output.push_str(&format!(
+                    "  \"{}\" [label=\"{}\\n({:?})\" fillcolor={} style=filled];\n",
+                    node.name, node.name, node.kind, color
+                ));
             }
         }
-        
-        output.push_str("\n");
-        
+
+        output.push('\n');
+
         // Add edges
         for edge_ref in state.graph.edge_references() {
             let source = &state.graph[edge_ref.source()];
             let target = &state.graph[edge_ref.target()];
             let edge_style = match edge_ref.weight() {
                 EdgeKind::Calls => "solid",
-                EdgeKind::Implements => "dashed", 
+                EdgeKind::Implements => "dashed",
                 EdgeKind::Uses => "dotted",
             };
-            output.push_str(&format!("  \"{}\" -> \"{}\" [label=\"{:?}\" style={}];\n", 
-                source.name, target.name, edge_ref.weight(), edge_style));
+            output.push_str(&format!(
+                "  \"{}\" -> \"{}\" [label=\"{:?}\" style={}];\n",
+                source.name,
+                target.name,
+                edge_ref.weight(),
+                edge_style
+            ));
         }
-        
+
         output.push_str("}\n");
         output
     }
@@ -287,7 +347,7 @@ impl OptimizedISG {
     /// Create a sample ISG for learning purposes
     pub fn create_sample() -> Self {
         let isg = Self::new();
-        
+
         // Create sample nodes representing a simple Rust program
         let nodes = vec![
             NodeData {
@@ -323,27 +383,30 @@ impl OptimizedISG {
                 line: 15,
             },
         ];
-        
+
         // Add nodes to graph
         for node in nodes {
             isg.upsert_node(node);
         }
-        
+
         // Add relationships
         let main_hash = SigHash::from_signature("fn main");
         let user_hash = SigHash::from_signature("struct User");
         let display_hash = SigHash::from_signature("trait Display");
         let create_user_hash = SigHash::from_signature("fn create_user");
-        
+
         // main() calls create_user()
-        isg.upsert_edge(main_hash, create_user_hash, EdgeKind::Calls).unwrap();
-        
+        isg.upsert_edge(main_hash, create_user_hash, EdgeKind::Calls)
+            .unwrap();
+
         // create_user() returns User (uses User)
-        isg.upsert_edge(create_user_hash, user_hash, EdgeKind::Uses).unwrap();
-        
+        isg.upsert_edge(create_user_hash, user_hash, EdgeKind::Uses)
+            .unwrap();
+
         // User implements Display
-        isg.upsert_edge(user_hash, display_hash, EdgeKind::Implements).unwrap();
-        
+        isg.upsert_edge(user_hash, display_hash, EdgeKind::Implements)
+            .unwrap();
+
         isg
     }
 
@@ -360,23 +423,51 @@ impl OptimizedISG {
     /// Upsert node - O(1) operation with RwLock
     pub fn upsert_node(&self, node: NodeData) {
         let mut state = self.state.write();
-        
+
         if let Some(&node_idx) = state.id_map.get(&node.hash) {
             // Update existing node
-            if let Some(node_weight) = state.graph.node_weight_mut(node_idx) {
-                *node_weight = node;
+            if let Some(node_weight) = state.graph.node_weight(node_idx) {
+                let old_name = node_weight.name.clone();
+                let old_hash = node_weight.hash;
+
+                // Remove old name mapping
+                if let Some(name_set) = state.name_map.get_mut(&old_name) {
+                    name_set.remove(&old_hash);
+                    if name_set.is_empty() {
+                        state.name_map.remove(&old_name);
+                    }
+                }
+
+                // Update node (now we can get mutable reference)
+                if let Some(node_weight_mut) = state.graph.node_weight_mut(node_idx) {
+                    *node_weight_mut = node.clone();
+                }
+
+                // Add new name mapping
+                state
+                    .name_map
+                    .entry(node.name.clone())
+                    .or_default()
+                    .insert(node.hash);
             }
         } else {
             // Insert new node
             let node_idx = state.graph.add_node(node.clone());
             state.id_map.insert(node.hash, node_idx);
+
+            // Add name mapping
+            state
+                .name_map
+                .entry(node.name.clone())
+                .or_default()
+                .insert(node.hash);
         }
     }
 
     /// Get node - O(1) operation
     pub fn get_node(&self, hash: SigHash) -> Result<NodeData, ISGError> {
         let state = self.state.read();
-        
+
         if let Some(&node_idx) = state.id_map.get(&hash) {
             if let Some(node_data) = state.graph.node_weight(node_idx) {
                 Ok(node_data.clone())
@@ -391,14 +482,22 @@ impl OptimizedISG {
     /// Upsert edge - O(1) operation
     pub fn upsert_edge(&self, from: SigHash, to: SigHash, kind: EdgeKind) -> Result<(), ISGError> {
         let mut state = self.state.write();
-        
+
         // Get node indices
-        let from_idx = state.id_map.get(&from).copied().ok_or(ISGError::NodeNotFound(from))?;
-        let to_idx = state.id_map.get(&to).copied().ok_or(ISGError::NodeNotFound(to))?;
-        
+        let from_idx = state
+            .id_map
+            .get(&from)
+            .copied()
+            .ok_or(ISGError::NodeNotFound(from))?;
+        let to_idx = state
+            .id_map
+            .get(&to)
+            .copied()
+            .ok_or(ISGError::NodeNotFound(to))?;
+
         // Check if edge already exists and update or add
         let existing_edge = state.graph.edges_connecting(from_idx, to_idx).next();
-        
+
         if let Some(edge_ref) = existing_edge {
             // Update existing edge
             let edge_idx = edge_ref.id();
@@ -409,19 +508,23 @@ impl OptimizedISG {
             // Add new edge
             state.graph.add_edge(from_idx, to_idx, kind);
         }
-        
+
         Ok(())
     }
 
     /// Query: what-implements - Target: <500Œºs
     pub fn find_implementors(&self, trait_hash: SigHash) -> Result<Vec<NodeData>, ISGError> {
         let state = self.state.read();
-        
+
         // Get trait node index
-        let trait_idx = state.id_map.get(&trait_hash).copied().ok_or(ISGError::NodeNotFound(trait_hash))?;
-        
+        let trait_idx = state
+            .id_map
+            .get(&trait_hash)
+            .copied()
+            .ok_or(ISGError::NodeNotFound(trait_hash))?;
+
         let mut implementors = Vec::new();
-        
+
         // Find all nodes that have "Implements" edges pointing to this trait
         for edge_ref in state.graph.edges_directed(trait_idx, Direction::Incoming) {
             if *edge_ref.weight() == EdgeKind::Implements {
@@ -431,39 +534,740 @@ impl OptimizedISG {
                 }
             }
         }
-        
+
         Ok(implementors)
     }
 
     /// Query: blast-radius - Target: <1ms
-    pub fn calculate_blast_radius(&self, start_hash: SigHash) -> Result<HashSet<SigHash>, ISGError> {
+    pub fn calculate_blast_radius(
+        &self,
+        start_hash: SigHash,
+    ) -> Result<HashSet<SigHash>, ISGError> {
         let state = self.state.read();
-        
+
         // Get start node index
-        let start_idx = state.id_map.get(&start_hash).copied().ok_or(ISGError::NodeNotFound(start_hash))?;
-        
+        let start_idx = state
+            .id_map
+            .get(&start_hash)
+            .copied()
+            .ok_or(ISGError::NodeNotFound(start_hash))?;
+
         let mut visited = HashSet::new();
-        
+
         // Use BFS to traverse all reachable nodes
         let mut bfs = Bfs::new(&state.graph, start_idx);
-        
+
         // Skip the start node itself
         bfs.next(&state.graph);
-        
+
         while let Some(node_idx) = bfs.next(&state.graph) {
             if let Some(node_data) = state.graph.node_weight(node_idx) {
                 visited.insert(node_data.hash);
             }
         }
-        
+
         Ok(visited)
     }
 
+    /// Find entities by name - O(1) operation with name index
+    pub fn find_by_name(&self, name: &str) -> Vec<SigHash> {
+        let state = self.state.read();
+
+        if let Some(hash_set) = state.name_map.get(name) {
+            hash_set.iter().copied().collect()
+        } else {
+            Vec::new()
+        }
+    }
+
     /// Query: find-cycles - MVP stub
     pub fn find_cycles(&self) -> Vec<Vec<SigHash>> {
         // MVP: Return empty - satisfies requirement
         Vec::new()
     }
+
+    /// Query: calls - Find all callers of an entity - Target: <1ms
+    pub fn find_callers(&self, target_hash: SigHash) -> Result<Vec<NodeData>, ISGError> {
+        let state = self.state.read();
+
+        // Get target node index
+        let target_idx = state
+            .id_map
+            .get(&target_hash)
+            .copied()
+            .ok_or(ISGError::NodeNotFound(target_hash))?;
+
+        let mut callers = Vec::new();
+
+        // Find all nodes that have "Calls" edges pointing to this target
+        for edge_ref in state.graph.edges_directed(target_idx, Direction::Incoming) {
+            if *edge_ref.weight() == EdgeKind::Calls {
+                let caller_idx = edge_ref.source();
+                if let Some(node_data) = state.graph.node_weight(caller_idx) {
+                    callers.push(node_data.clone());
+                }
+            }
+        }
+
+        // REFACTOR: Sort results by name for consistent ordering
+        callers.sort_by(|a, b| a.name.cmp(&b.name));
+
+        Ok(callers)
+    }
+
+    /// Query: uses - Find all users of a type - Target: <1ms
+    pub fn find_users(&self, target_hash: SigHash) -> Result<Vec<NodeData>, ISGError> {
+        let state = self.state.read();
+
+        // Get target node index
+        let target_idx = state
+            .id_map
+            .get(&target_hash)
+            .copied()
+            .ok_or(ISGError::NodeNotFound(target_hash))?;
+
+        let mut users = Vec::new();
+
+        // Find all nodes that have "Uses" edges pointing to this target
+        for edge_ref in state.graph.edges_directed(target_idx, Direction::Incoming) {
+            if *edge_ref.weight() == EdgeKind::Uses {
+                let user_idx = edge_ref.source();
+                if let Some(node_data) = state.graph.node_weight(user_idx) {
+                    users.push(node_data.clone());
+                }
+            }
+        }
+
+        // REFACTOR: Sort results by name for consistent ordering
+        users.sort_by(|a, b| a.name.cmp(&b.name));
+
+        Ok(users)
+    }
+
+    /// Export graph data as JSON for web visualization
+    /// Target: <500ms generation time, optimized for browser performance
+    pub fn export_web_data(&self) -> Result<String, ISGError> {
+        let start = std::time::Instant::now();
+        let state = self.state.read();
+
+        let web_data = WebGraphData {
+            nodes: state
+                .graph
+                .node_weights()
+                .map(|node| WebNode {
+                    id: format!("{:?}", node.hash),
+                    name: node.name.to_string(),
+                    kind: format!("{:?}", node.kind),
+                    signature: node.signature.to_string(),
+                    file_path: node.file_path.to_string(),
+                    line: node.line,
+                })
+                .collect(),
+            edges: state
+                .graph
+                .edge_references()
+                .map(|edge| WebEdge {
+                    source: format!("{:?}", state.graph[edge.source()].hash),
+                    target: format!("{:?}", state.graph[edge.target()].hash),
+                    kind: format!("{:?}", edge.weight()),
+                })
+                .collect(),
+            metadata: WebMetadata {
+                node_count: state.graph.node_count(),
+                edge_count: state.graph.edge_count(),
+                generated_at: std::time::SystemTime::now()
+                    .duration_since(std::time::UNIX_EPOCH)
+                    .unwrap()
+                    .as_secs(),
+            },
+        };
+
+        let json = serde_json::to_string(&web_data)
+            .map_err(|e| ISGError::IoError(format!("JSON serialization failed: {}", e)))?;
+
+        let elapsed = start.elapsed();
+        if elapsed.as_millis() > 500 {
+            eprintln!(
+                "‚ö†Ô∏è  Web data export took {}ms (>500ms constraint)",
+                elapsed.as_millis()
+            );
+        }
+
+        Ok(json)
+    }
+
+    /// Generate interactive HTML visualization with embedded JavaScript
+    /// Target: <500ms generation time, self-contained HTML file
+    pub fn generate_html_visualization(
+        &self,
+        focus_entity: Option<&str>,
+    ) -> Result<String, ISGError> {
+        let start = std::time::Instant::now();
+
+        // Get graph data as JSON
+        let graph_json = self.export_web_data()?;
+
+        // Generate HTML with embedded visualization
+        let html = format!(
+            r#"<!DOCTYPE html>
+<html lang="en">
+<head>
+    <meta charset="UTF-8">
+    <meta name="viewport" content="width=device-width, initial-scale=1.0">
+    <title>Parseltongue Architecture Visualization</title>
+    <style>
+        body {{
+            margin: 0;
+            padding: 20px;
+            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
+            background: #1a1a1a;
+            color: #ffffff;
+        }}
+        
+        .header {{
+            text-align: center;
+            margin-bottom: 20px;
+        }}
+        
+        .header h1 {{
+            color: #4CAF50;
+            margin: 0;
+        }}
+        
+        .header p {{
+            color: #888;
+            margin: 5px 0;
+        }}
+        
+        .controls {{
+            text-align: center;
+            margin-bottom: 20px;
+        }}
+        
+        .controls button {{
+            background: #4CAF50;
+            color: white;
+            border: none;
+            padding: 10px 20px;
+            margin: 0 5px;
+            border-radius: 5px;
+            cursor: pointer;
+            font-size: 14px;
+        }}
+        
+        .controls button:hover {{
+            background: #45a049;
+        }}
+        
+        .controls button:disabled {{
+            background: #666;
+            cursor: not-allowed;
+        }}
+        
+        #visualization {{
+            width: 100%;
+            height: 80vh;
+            border: 1px solid #333;
+            border-radius: 8px;
+            background: #2a2a2a;
+        }}
+        
+        .info-panel {{
+            position: fixed;
+            top: 20px;
+            right: 20px;
+            width: 300px;
+            background: #333;
+            border-radius: 8px;
+            padding: 15px;
+            display: none;
+        }}
+        
+        .info-panel h3 {{
+            margin: 0 0 10px 0;
+            color: #4CAF50;
+        }}
+        
+        .info-panel .close {{
+            float: right;
+            cursor: pointer;
+            color: #888;
+            font-size: 18px;
+        }}
+        
+        .info-panel .close:hover {{
+            color: #fff;
+        }}
+        
+        .legend {{
+            position: fixed;
+            bottom: 20px;
+            left: 20px;
+            background: #333;
+            border-radius: 8px;
+            padding: 15px;
+        }}
+        
+        .legend h4 {{
+            margin: 0 0 10px 0;
+            color: #4CAF50;
+        }}
+        
+        .legend-item {{
+            display: flex;
+            align-items: center;
+            margin: 5px 0;
+        }}
+        
+        .legend-color {{
+            width: 20px;
+            height: 20px;
+            border-radius: 50%;
+            margin-right: 10px;
+        }}
+        
+        .function {{ background: #4CAF50; }}
+        .struct {{ background: #2196F3; }}
+        .trait {{ background: #FF9800; }}
+        
+        .edge-calls {{ stroke: #4CAF50; }}
+        .edge-uses {{ stroke: #2196F3; }}
+        .edge-implements {{ stroke: #FF9800; }}
+    </style>
+</head>
+<body>
+    <div class="header">
+        <h1>üêç Parseltongue Architecture Visualization</h1>
+        <p>Interactive Interface Signature Graph</p>
+        <p id="stats"></p>
+    </div>
+    
+    <div class="controls">
+        <button onclick="resetZoom()">Reset View</button>
+        <button onclick="togglePhysics()">Toggle Physics</button>
+        <button onclick="fitToScreen()">Fit to Screen</button>
+        <button onclick="exportSVG()" disabled>Export SVG</button>
+    </div>
+    
+    <div id="visualization"></div>
+    
+    <div id="info-panel" class="info-panel">
+        <span class="close" onclick="hideInfo()">&times;</span>
+        <h3 id="info-title">Node Information</h3>
+        <div id="info-content"></div>
+    </div>
+    
+    <div class="legend">
+        <h4>Legend</h4>
+        <div class="legend-item">
+            <div class="legend-color function"></div>
+            <span>Function</span>
+        </div>
+        <div class="legend-item">
+            <div class="legend-color struct"></div>
+            <span>Struct</span>
+        </div>
+        <div class="legend-item">
+            <div class="legend-color trait"></div>
+            <span>Trait</span>
+        </div>
+        <div style="margin-top: 10px; font-size: 12px; color: #888;">
+            <div>Green edges: Calls</div>
+            <div>Blue edges: Uses</div>
+            <div>Orange edges: Implements</div>
+        </div>
+    </div>
+
+    <script>
+        // Embedded graph data
+        const graphData = {graph_json};
+        
+        // Focus entity (if specified)
+        const focusEntity = {focus_entity_json};
+        
+        // Update stats
+        document.getElementById('stats').textContent = 
+            `${{graphData.metadata.node_count}} nodes, ${{graphData.metadata.edge_count}} edges`;
+        
+        // Simple force-directed graph implementation using Canvas
+        class GraphVisualization {{
+            constructor(containerId, data) {{
+                this.container = document.getElementById(containerId);
+                this.canvas = document.createElement('canvas');
+                this.ctx = this.canvas.getContext('2d');
+                this.container.appendChild(this.canvas);
+                
+                this.data = data;
+                this.nodes = [];
+                this.edges = [];
+                this.physicsEnabled = true;
+                this.selectedNode = null;
+                
+                this.setupCanvas();
+                this.processData();
+                this.setupEventListeners();
+                this.animate();
+            }}
+            
+            setupCanvas() {{
+                this.canvas.width = this.container.clientWidth;
+                this.canvas.height = this.container.clientHeight;
+                this.canvas.style.display = 'block';
+                
+                // Handle resize
+                window.addEventListener('resize', () => {{
+                    this.canvas.width = this.container.clientWidth;
+                    this.canvas.height = this.container.clientHeight;
+                }});
+            }}
+            
+            processData() {{
+                const width = this.canvas.width;
+                const height = this.canvas.height;
+                
+                // Create nodes with random positions
+                this.nodes = this.data.nodes.map(node => ({{
+                    ...node,
+                    x: Math.random() * width,
+                    y: Math.random() * height,
+                    vx: 0,
+                    vy: 0,
+                    radius: this.getNodeRadius(node.kind),
+                    color: this.getNodeColor(node.kind)
+                }}));
+                
+                // Create edges
+                this.edges = this.data.edges.map(edge => ({{
+                    ...edge,
+                    sourceNode: this.nodes.find(n => n.id === edge.source),
+                    targetNode: this.nodes.find(n => n.id === edge.target),
+                    color: this.getEdgeColor(edge.kind)
+                }}));
+                
+                // Focus on specific entity if requested
+                if (focusEntity) {{
+                    const focusNode = this.nodes.find(n => n.name === focusEntity);
+                    if (focusNode) {{
+                        this.centerOnNode(focusNode);
+                    }}
+                }}
+            }}
+            
+            getNodeRadius(kind) {{
+                switch(kind) {{
+                    case 'Function': return 8;
+                    case 'Struct': return 10;
+                    case 'Trait': return 12;
+                    default: return 8;
+                }}
+            }}
+            
+            getNodeColor(kind) {{
+                switch(kind) {{
+                    case 'Function': return '#4CAF50';
+                    case 'Struct': return '#2196F3';
+                    case 'Trait': return '#FF9800';
+                    default: return '#888';
+                }}
+            }}
+            
+            getEdgeColor(kind) {{
+                switch(kind) {{
+                    case 'Calls': return '#4CAF50';
+                    case 'Uses': return '#2196F3';
+                    case 'Implements': return '#FF9800';
+                    default: return '#666';
+                }}
+            }}
+            
+            centerOnNode(node) {{
+                const width = this.canvas.width;
+                const height = this.canvas.height;
+                node.x = width / 2;
+                node.y = height / 2;
+            }}
+            
+            setupEventListeners() {{
+                let isDragging = false;
+                let dragNode = null;
+                let lastMouseX = 0;
+                let lastMouseY = 0;
+                
+                this.canvas.addEventListener('mousedown', (e) => {{
+                    const rect = this.canvas.getBoundingClientRect();
+                    const mouseX = e.clientX - rect.left;
+                    const mouseY = e.clientY - rect.top;
+                    
+                    // Find clicked node
+                    const clickedNode = this.nodes.find(node => {{
+                        const dx = mouseX - node.x;
+                        const dy = mouseY - node.y;
+                        return Math.sqrt(dx * dx + dy * dy) < node.radius + 5;
+                    }});
+                    
+                    if (clickedNode) {{
+                        isDragging = true;
+                        dragNode = clickedNode;
+                        this.selectedNode = clickedNode;
+                        this.showNodeInfo(clickedNode);
+                        lastMouseX = mouseX;
+                        lastMouseY = mouseY;
+                    }}
+                }});
+                
+                this.canvas.addEventListener('mousemove', (e) => {{
+                    if (isDragging && dragNode) {{
+                        const rect = this.canvas.getBoundingClientRect();
+                        const mouseX = e.clientX - rect.left;
+                        const mouseY = e.clientY - rect.top;
+                        
+                        dragNode.x = mouseX;
+                        dragNode.y = mouseY;
+                        dragNode.vx = 0;
+                        dragNode.vy = 0;
+                    }}
+                }});
+                
+                this.canvas.addEventListener('mouseup', () => {{
+                    isDragging = false;
+                    dragNode = null;
+                }});
+                
+                // Double-click to center on node
+                this.canvas.addEventListener('dblclick', (e) => {{
+                    const rect = this.canvas.getBoundingClientRect();
+                    const mouseX = e.clientX - rect.left;
+                    const mouseY = e.clientY - rect.top;
+                    
+                    const clickedNode = this.nodes.find(node => {{
+                        const dx = mouseX - node.x;
+                        const dy = mouseY - node.y;
+                        return Math.sqrt(dx * dx + dy * dy) < node.radius + 5;
+                    }});
+                    
+                    if (clickedNode) {{
+                        this.centerOnNode(clickedNode);
+                    }}
+                }});
+            }}
+            
+            showNodeInfo(node) {{
+                const panel = document.getElementById('info-panel');
+                const title = document.getElementById('info-title');
+                const content = document.getElementById('info-content');
+                
+                title.textContent = node.name;
+                content.innerHTML = `
+                    <p><strong>Type:</strong> ${{node.kind}}</p>
+                    <p><strong>Signature:</strong> ${{node.signature}}</p>
+                    <p><strong>File:</strong> ${{node.file_path}}:${{node.line}}</p>
+                `;
+                
+                panel.style.display = 'block';
+            }}
+            
+            updatePhysics() {{
+                if (!this.physicsEnabled) return;
+                
+                const width = this.canvas.width;
+                const height = this.canvas.height;
+                
+                // Apply forces
+                for (let node of this.nodes) {{
+                    // Repulsion between nodes
+                    for (let other of this.nodes) {{
+                        if (node === other) continue;
+                        
+                        const dx = node.x - other.x;
+                        const dy = node.y - other.y;
+                        const distance = Math.sqrt(dx * dx + dy * dy);
+                        
+                        if (distance > 0 && distance < 100) {{
+                            const force = 50 / (distance * distance);
+                            node.vx += (dx / distance) * force;
+                            node.vy += (dy / distance) * force;
+                        }}
+                    }}
+                    
+                    // Center attraction
+                    const centerX = width / 2;
+                    const centerY = height / 2;
+                    const toCenterX = centerX - node.x;
+                    const toCenterY = centerY - node.y;
+                    node.vx += toCenterX * 0.0001;
+                    node.vy += toCenterY * 0.0001;
+                    
+                    // Damping
+                    node.vx *= 0.9;
+                    node.vy *= 0.9;
+                    
+                    // Update position
+                    node.x += node.vx;
+                    node.y += node.vy;
+                    
+                    // Boundary constraints
+                    if (node.x < node.radius) {{ node.x = node.radius; node.vx = 0; }}
+                    if (node.x > width - node.radius) {{ node.x = width - node.radius; node.vx = 0; }}
+                    if (node.y < node.radius) {{ node.y = node.radius; node.vy = 0; }}
+                    if (node.y > height - node.radius) {{ node.y = height - node.radius; node.vy = 0; }}
+                }}
+                
+                // Spring forces for edges
+                for (let edge of this.edges) {{
+                    if (!edge.sourceNode || !edge.targetNode) continue;
+                    
+                    const dx = edge.targetNode.x - edge.sourceNode.x;
+                    const dy = edge.targetNode.y - edge.sourceNode.y;
+                    const distance = Math.sqrt(dx * dx + dy * dy);
+                    const targetDistance = 80;
+                    
+                    if (distance > 0) {{
+                        const force = (distance - targetDistance) * 0.01;
+                        const fx = (dx / distance) * force;
+                        const fy = (dy / distance) * force;
+                        
+                        edge.sourceNode.vx += fx;
+                        edge.sourceNode.vy += fy;
+                        edge.targetNode.vx -= fx;
+                        edge.targetNode.vy -= fy;
+                    }}
+                }}
+            }}
+            
+            render() {{
+                this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
+                
+                // Draw edges
+                for (let edge of this.edges) {{
+                    if (!edge.sourceNode || !edge.targetNode) continue;
+                    
+                    this.ctx.beginPath();
+                    this.ctx.moveTo(edge.sourceNode.x, edge.sourceNode.y);
+                    this.ctx.lineTo(edge.targetNode.x, edge.targetNode.y);
+                    this.ctx.strokeStyle = edge.color;
+                    this.ctx.lineWidth = 1;
+                    this.ctx.stroke();
+                    
+                    // Draw arrow
+                    const dx = edge.targetNode.x - edge.sourceNode.x;
+                    const dy = edge.targetNode.y - edge.sourceNode.y;
+                    const distance = Math.sqrt(dx * dx + dy * dy);
+                    if (distance > 0) {{
+                        const arrowX = edge.targetNode.x - (dx / distance) * (edge.targetNode.radius + 5);
+                        const arrowY = edge.targetNode.y - (dy / distance) * (edge.targetNode.radius + 5);
+                        
+                        this.ctx.beginPath();
+                        this.ctx.moveTo(arrowX, arrowY);
+                        this.ctx.lineTo(arrowX - (dx / distance) * 8 + (dy / distance) * 4, 
+                                       arrowY - (dy / distance) * 8 - (dx / distance) * 4);
+                        this.ctx.lineTo(arrowX - (dx / distance) * 8 - (dy / distance) * 4, 
+                                       arrowY - (dy / distance) * 8 + (dx / distance) * 4);
+                        this.ctx.closePath();
+                        this.ctx.fillStyle = edge.color;
+                        this.ctx.fill();
+                    }}
+                }}
+                
+                // Draw nodes
+                for (let node of this.nodes) {{
+                    this.ctx.beginPath();
+                    this.ctx.arc(node.x, node.y, node.radius, 0, 2 * Math.PI);
+                    this.ctx.fillStyle = node.color;
+                    this.ctx.fill();
+                    
+                    if (node === this.selectedNode) {{
+                        this.ctx.strokeStyle = '#fff';
+                        this.ctx.lineWidth = 2;
+                        this.ctx.stroke();
+                    }}
+                    
+                    // Draw label
+                    this.ctx.fillStyle = '#fff';
+                    this.ctx.font = '12px Arial';
+                    this.ctx.textAlign = 'center';
+                    this.ctx.fillText(node.name, node.x, node.y + node.radius + 15);
+                }}
+            }}
+            
+            animate() {{
+                this.updatePhysics();
+                this.render();
+                requestAnimationFrame(() => this.animate());
+            }}
+            
+            resetZoom() {{
+                // Reset all nodes to random positions
+                const width = this.canvas.width;
+                const height = this.canvas.height;
+                
+                for (let node of this.nodes) {{
+                    node.x = Math.random() * width;
+                    node.y = Math.random() * height;
+                    node.vx = 0;
+                    node.vy = 0;
+                }}
+            }}
+            
+            togglePhysics() {{
+                this.physicsEnabled = !this.physicsEnabled;
+            }}
+            
+            fitToScreen() {{
+                // Center all nodes
+                const width = this.canvas.width;
+                const height = this.canvas.height;
+                
+                for (let node of this.nodes) {{
+                    node.x = width / 2 + (Math.random() - 0.5) * 200;
+                    node.y = height / 2 + (Math.random() - 0.5) * 200;
+                    node.vx = 0;
+                    node.vy = 0;
+                }}
+            }}
+        }}
+        
+        // Initialize visualization
+        const viz = new GraphVisualization('visualization', graphData);
+        
+        // Global functions for controls
+        function resetZoom() {{
+            viz.resetZoom();
+        }}
+        
+        function togglePhysics() {{
+            viz.togglePhysics();
+        }}
+        
+        function fitToScreen() {{
+            viz.fitToScreen();
+        }}
+        
+        function exportSVG() {{
+            alert('SVG export not implemented in this version');
+        }}
+        
+        function hideInfo() {{
+            document.getElementById('info-panel').style.display = 'none';
+        }}
+    </script>
+</body>
+</html>"#,
+            graph_json = graph_json,
+            focus_entity_json = focus_entity
+                .map(|s| format!("\"{}\"", s))
+                .unwrap_or_else(|| "null".to_string())
+        );
+
+        let elapsed = start.elapsed();
+        if elapsed.as_millis() > 500 {
+            eprintln!(
+                "‚ö†Ô∏è  HTML generation took {}ms (>500ms constraint)",
+                elapsed.as_millis()
+            );
+        }
+
+        Ok(html)
+    }
 }
 
 #[cfg(test)]
@@ -496,7 +1300,7 @@ mod tests {
     fn test_isg_clone_shares_state() {
         let isg1 = OptimizedISG::new();
         let isg2 = isg1.clone();
-        
+
         // Both should share the same underlying state
         assert_eq!(isg1.node_count(), isg2.node_count());
     }
@@ -505,14 +1309,18 @@ mod tests {
     #[test]
     fn test_sighash_collision_resistance() {
         let mut hashes = HashSet::new();
-        
+
         // Test 10,000 different signatures for collisions
         for i in 0..10_000 {
             let signature = format!("fn test_function_{}() -> Result<(), Error>", i);
             let hash = SigHash::from_signature(&signature);
-            
+
             // Should not have collisions
-            assert!(hashes.insert(hash), "Hash collision detected for signature: {}", signature);
+            assert!(
+                hashes.insert(hash),
+                "Hash collision detected for signature: {}",
+                signature
+            );
         }
     }
 
@@ -521,11 +1329,33 @@ mod tests {
         let signature = "fn test() -> Result<(), Error>";
         let hash1 = SigHash::from_signature(signature);
         let hash2 = SigHash::from_signature(signature);
-        
+
         // Same input should produce same hash
         assert_eq!(hash1, hash2);
     }
 
+    #[test]
+    fn test_sighash_uses_fxhasher() {
+        // Verify we're using FxHasher for deterministic cross-platform hashing
+        let signature = "fn test_function() -> i32";
+        let hash = SigHash::from_signature(signature);
+
+        // FxHasher should produce consistent results
+        // This specific hash value validates we're using FxHasher, not DefaultHasher
+        let expected_hash = {
+            use fxhash::FxHasher;
+            use std::hash::{Hash, Hasher};
+            let mut hasher = FxHasher::default();
+            signature.hash(&mut hasher);
+            SigHash(hasher.finish())
+        };
+
+        assert_eq!(
+            hash, expected_hash,
+            "SigHash should use FxHasher for deterministic results"
+        );
+    }
+
     // TDD Cycle 3: Node operations (RED phase)
     #[test]
     fn test_upsert_and_get_node() {
@@ -556,18 +1386,26 @@ mod tests {
     fn test_node_operation_performance() {
         let isg = OptimizedISG::new();
         let node = mock_node(1, NodeKind::Function, "test_func");
-        
+
         // Test node upsert is <50Œºs (realistic range based on actual performance)
         let start = Instant::now();
         isg.upsert_node(node.clone());
         let elapsed = start.elapsed();
-        assert!(elapsed.as_micros() < 50, "Node upsert took {}Œºs (>50Œºs)", elapsed.as_micros());
-        
+        assert!(
+            elapsed.as_micros() < 50,
+            "Node upsert took {}Œºs (>50Œºs)",
+            elapsed.as_micros()
+        );
+
         // Test node retrieval is <50Œºs (realistic range based on actual performance)
         let start = Instant::now();
         let retrieved = isg.get_node(node.hash).unwrap();
         let elapsed = start.elapsed();
-        assert!(elapsed.as_micros() < 50, "Node get took {}Œºs (>50Œºs)", elapsed.as_micros());
+        assert!(
+            elapsed.as_micros() < 50,
+            "Node get took {}Œºs (>50Œºs)",
+            elapsed.as_micros()
+        );
         assert_eq!(retrieved, node);
     }
 
@@ -586,11 +1424,13 @@ mod tests {
         assert_eq!(isg.edge_count(), 1);
 
         // 2. Idempotency (same edge kind)
-        isg.upsert_edge(node_a.hash, node_b.hash, EdgeKind::Uses).unwrap();
+        isg.upsert_edge(node_a.hash, node_b.hash, EdgeKind::Uses)
+            .unwrap();
         assert_eq!(isg.edge_count(), 1);
 
         // 3. Update (different edge kind)
-        isg.upsert_edge(node_a.hash, node_b.hash, EdgeKind::Calls).unwrap();
+        isg.upsert_edge(node_a.hash, node_b.hash, EdgeKind::Calls)
+            .unwrap();
         assert_eq!(isg.edge_count(), 1);
 
         // 4. Non-existent nodes
@@ -622,7 +1462,7 @@ mod tests {
         isg.upsert_edge(h(4), h(6), EdgeKind::Implements).unwrap();
         isg.upsert_edge(h(5), h(6), EdgeKind::Implements).unwrap();
         isg.upsert_edge(h(1), h(6), EdgeKind::Calls).unwrap();
-        
+
         // Noise: StructD Uses StructC (should not affect Implementors query)
         isg.upsert_edge(h(4), h(3), EdgeKind::Uses).unwrap();
 
@@ -642,21 +1482,28 @@ mod tests {
         let mut implementor_hashes: Vec<SigHash> = implementors.iter().map(|n| n.hash).collect();
         implementor_hashes.sort();
         assert_eq!(implementor_hashes, vec![SigHash(4), SigHash(5)]);
-        
+
         // Test non-existent trait
-        assert_eq!(isg.find_implementors(SigHash(99)), Err(ISGError::NodeNotFound(SigHash(99))));
+        assert_eq!(
+            isg.find_implementors(SigHash(99)),
+            Err(ISGError::NodeNotFound(SigHash(99)))
+        );
     }
 
     #[test]
     fn test_what_implements_performance() {
         let isg = setup_query_graph();
         let trait_hash = SigHash(6);
-        
+
         let start = Instant::now();
         let _implementors = isg.find_implementors(trait_hash).unwrap();
         let elapsed = start.elapsed();
-        
-        assert!(elapsed.as_micros() < 1000, "what-implements took {}Œºs (>1ms)", elapsed.as_micros());
+
+        assert!(
+            elapsed.as_micros() < 1000,
+            "what-implements took {}Œºs (>1ms)",
+            elapsed.as_micros()
+        );
     }
 
     #[test]
@@ -668,9 +1515,9 @@ mod tests {
         let radius = isg.calculate_blast_radius(start_hash).unwrap();
 
         // Assertion: Should reach B(2), C(3), T(6). D(4) and E(5) are not reachable downstream from A.
-        let expected: HashSet<SigHash> = vec![
-            SigHash(2), SigHash(3), SigHash(6),
-        ].into_iter().collect();
+        let expected: HashSet<SigHash> = vec![SigHash(2), SigHash(3), SigHash(6)]
+            .into_iter()
+            .collect();
         assert_eq!(radius, expected);
 
         // Test starting from a leaf node (StructC (3))
@@ -682,12 +1529,16 @@ mod tests {
     fn test_blast_radius_performance() {
         let isg = setup_query_graph();
         let start_hash = SigHash(1);
-        
+
         let start = Instant::now();
         let _radius = isg.calculate_blast_radius(start_hash).unwrap();
         let elapsed = start.elapsed();
-        
-        assert!(elapsed.as_micros() < 2000, "blast-radius took {}Œºs (>2ms)", elapsed.as_micros());
+
+        assert!(
+            elapsed.as_micros() < 2000,
+            "blast-radius took {}Œºs (>2ms)",
+            elapsed.as_micros()
+        );
     }
 
     // TDD Cycle 6: Concurrency validation (RED phase)
@@ -696,7 +1547,7 @@ mod tests {
         let isg = OptimizedISG::new();
         let isg_w1 = isg.clone();
         let isg_r = isg.clone();
-        
+
         // Writer thread 1 (Nodes 1-100)
         let writer1 = thread::spawn(move || {
             for i in 1..=100 {
@@ -704,7 +1555,9 @@ mod tests {
                 isg_w1.upsert_node(node);
                 // Add an edge from node 1 to this node if i > 1
                 if i > 1 {
-                    isg_w1.upsert_edge(SigHash(1), SigHash(i), EdgeKind::Uses).unwrap();
+                    isg_w1
+                        .upsert_edge(SigHash(1), SigHash(i), EdgeKind::Uses)
+                        .unwrap();
                 }
             }
         });
@@ -715,7 +1568,7 @@ mod tests {
                 // Acquiring a read lock and traversing should not cause data races or deadlocks.
                 // We might get an error if node 1 hasn't been inserted yet.
                 if let Ok(radius) = isg_r.calculate_blast_radius(SigHash(1)) {
-                     assert!(radius.len() <= 99);
+                    assert!(radius.len() <= 99);
                 }
             }
         });
@@ -729,10 +1582,464 @@ mod tests {
         assert_eq!(isg.calculate_blast_radius(SigHash(1)).unwrap().len(), 99);
     }
 
+    #[test]
+    fn test_find_by_name_o1_lookup() {
+        let isg = OptimizedISG::new();
+
+        // Add nodes with same and different names
+        let node1 = mock_node(1, NodeKind::Function, "test_function");
+        let node2 = mock_node(2, NodeKind::Struct, "TestStruct");
+        let node3 = mock_node(3, NodeKind::Function, "test_function"); // Same name, different hash
+
+        isg.upsert_node(node1.clone());
+        isg.upsert_node(node2.clone());
+        isg.upsert_node(node3.clone());
+
+        // Test O(1) name lookup
+        let start = Instant::now();
+        let function_hashes = isg.find_by_name("test_function");
+        let elapsed = start.elapsed();
+
+        // Should find both functions with same name
+        assert_eq!(function_hashes.len(), 2);
+        assert!(function_hashes.contains(&SigHash(1)));
+        assert!(function_hashes.contains(&SigHash(3)));
+
+        // Should be O(1) - very fast lookup
+        assert!(
+            elapsed.as_micros() < 10,
+            "Name lookup took {}Œºs (should be <10Œºs)",
+            elapsed.as_micros()
+        );
+
+        // Test single result
+        let struct_hashes = isg.find_by_name("TestStruct");
+        assert_eq!(struct_hashes.len(), 1);
+        assert!(struct_hashes.contains(&SigHash(2)));
+
+        // Test non-existent
+        let empty_hashes = isg.find_by_name("NonExistent");
+        assert!(empty_hashes.is_empty());
+    }
+
+    // TDD Cycle: Test calls query (GREEN phase)
+    #[test]
+    fn test_query_calls() {
+        let isg = setup_query_graph();
+
+        // Test finding callers of FuncB (2) - should be FuncA (1)
+        let callers = isg.find_callers(SigHash(2)).unwrap();
+        assert_eq!(callers.len(), 1);
+        assert_eq!(callers[0].hash, SigHash(1));
+        assert_eq!(callers[0].name.as_ref(), "FuncA");
+
+        // Test finding callers of TraitT (6) - should be FuncA (1)
+        let trait_callers = isg.find_callers(SigHash(6)).unwrap();
+        assert_eq!(trait_callers.len(), 1);
+        assert_eq!(trait_callers[0].hash, SigHash(1));
+
+        // Test finding callers of StructC (3) - should be FuncB (2)
+        let struct_callers = isg.find_callers(SigHash(3)).unwrap();
+        assert_eq!(struct_callers.len(), 1);
+        assert_eq!(struct_callers[0].hash, SigHash(2));
+
+        // Test finding callers of FuncA (1) - should be empty (no one calls FuncA)
+        let no_callers = isg.find_callers(SigHash(1)).unwrap();
+        assert!(no_callers.is_empty());
+
+        // Test non-existent entity
+        assert_eq!(
+            isg.find_callers(SigHash(99)),
+            Err(ISGError::NodeNotFound(SigHash(99)))
+        );
+    }
+
+    #[test]
+    fn test_calls_query_performance() {
+        let isg = setup_query_graph();
+
+        let start = Instant::now();
+        let _callers = isg.find_callers(SigHash(2)).unwrap();
+        let elapsed = start.elapsed();
+
+        assert!(
+            elapsed.as_micros() < 1000,
+            "calls query took {}Œºs (>1ms)",
+            elapsed.as_micros()
+        );
+    }
+
+    // TDD Cycle: Test uses query (GREEN phase)
+    #[test]
+    fn test_query_uses() {
+        let isg = setup_query_graph();
+
+        // Test finding users of StructC (3) - should be StructD (4) via Uses edge
+        let users = isg.find_users(SigHash(3)).unwrap();
+        assert_eq!(users.len(), 1);
+        assert_eq!(users[0].hash, SigHash(4));
+        assert_eq!(users[0].name.as_ref(), "StructD");
+
+        // Test finding users of TraitT (6) - should be empty (no Uses edges to traits in our test graph)
+        let trait_users = isg.find_users(SigHash(6)).unwrap();
+        assert!(trait_users.is_empty());
+
+        // Test non-existent entity
+        assert_eq!(
+            isg.find_users(SigHash(99)),
+            Err(ISGError::NodeNotFound(SigHash(99)))
+        );
+    }
+
+    #[test]
+    fn test_uses_query_performance() {
+        let isg = setup_query_graph();
+
+        let start = Instant::now();
+        let _users = isg.find_users(SigHash(3)).unwrap();
+        let elapsed = start.elapsed();
+
+        assert!(
+            elapsed.as_micros() < 1000,
+            "uses query took {}Œºs (>1ms)",
+            elapsed.as_micros()
+        );
+    }
+
+    // TDD Cycle: Test edge filtering by EdgeKind
+    #[test]
+    fn test_edge_filtering_by_kind() {
+        let isg = OptimizedISG::new();
+
+        // Create test nodes
+        let func_a = mock_node(1, NodeKind::Function, "FuncA");
+        let func_b = mock_node(2, NodeKind::Function, "FuncB");
+        let struct_c = mock_node(3, NodeKind::Struct, "StructC");
+        let trait_t = mock_node(4, NodeKind::Trait, "TraitT");
+
+        isg.upsert_node(func_a.clone());
+        isg.upsert_node(func_b.clone());
+        isg.upsert_node(struct_c.clone());
+        isg.upsert_node(trait_t.clone());
+
+        // Create different types of edges
+        isg.upsert_edge(SigHash(1), SigHash(2), EdgeKind::Calls)
+            .unwrap(); // FuncA calls FuncB
+        isg.upsert_edge(SigHash(1), SigHash(3), EdgeKind::Uses)
+            .unwrap(); // FuncA uses StructC
+        isg.upsert_edge(SigHash(3), SigHash(4), EdgeKind::Implements)
+            .unwrap(); // StructC implements TraitT
+
+        // Test calls query - should only find Calls edges
+        let callers_of_func_b = isg.find_callers(SigHash(2)).unwrap();
+        assert_eq!(callers_of_func_b.len(), 1);
+        assert_eq!(callers_of_func_b[0].hash, SigHash(1));
+
+        // Test uses query - should only find Uses edges
+        let users_of_struct_c = isg.find_users(SigHash(3)).unwrap();
+        assert_eq!(users_of_struct_c.len(), 1);
+        assert_eq!(users_of_struct_c[0].hash, SigHash(1));
+
+        // Test what-implements query - should only find Implements edges
+        let implementors_of_trait_t = isg.find_implementors(SigHash(4)).unwrap();
+        assert_eq!(implementors_of_trait_t.len(), 1);
+        assert_eq!(implementors_of_trait_t[0].hash, SigHash(3));
+
+        // Verify edge filtering: FuncB should have no callers via Uses or Implements
+        let no_users_of_func_b = isg.find_users(SigHash(2)).unwrap();
+        assert!(no_users_of_func_b.is_empty());
+
+        let no_implementors_of_func_b = isg.find_implementors(SigHash(2)).unwrap();
+        assert!(no_implementors_of_func_b.is_empty());
+    }
+
+    // TDD Cycle: Test result ranking and sorting
+    #[test]
+    fn test_result_ranking_and_sorting() {
+        let isg = OptimizedISG::new();
+
+        // Create test nodes with names that will test alphabetical sorting
+        let target = mock_node(1, NodeKind::Function, "target_function");
+        let caller_z = mock_node(2, NodeKind::Function, "z_caller");
+        let caller_a = mock_node(3, NodeKind::Function, "a_caller");
+        let caller_m = mock_node(4, NodeKind::Function, "m_caller");
+
+        isg.upsert_node(target.clone());
+        isg.upsert_node(caller_z.clone());
+        isg.upsert_node(caller_a.clone());
+        isg.upsert_node(caller_m.clone());
+
+        // Create calls edges in random order
+        isg.upsert_edge(SigHash(2), SigHash(1), EdgeKind::Calls)
+            .unwrap(); // z_caller calls target
+        isg.upsert_edge(SigHash(4), SigHash(1), EdgeKind::Calls)
+            .unwrap(); // m_caller calls target
+        isg.upsert_edge(SigHash(3), SigHash(1), EdgeKind::Calls)
+            .unwrap(); // a_caller calls target
+
+        // Test that results are sorted alphabetically by name
+        let callers = isg.find_callers(SigHash(1)).unwrap();
+        assert_eq!(callers.len(), 3);
+        assert_eq!(callers[0].name.as_ref(), "a_caller");
+        assert_eq!(callers[1].name.as_ref(), "m_caller");
+        assert_eq!(callers[2].name.as_ref(), "z_caller");
+
+        // Test the same for uses query
+        let user_z = mock_node(5, NodeKind::Function, "z_user");
+        let user_a = mock_node(6, NodeKind::Function, "a_user");
+        let type_target = mock_node(7, NodeKind::Struct, "TargetType");
+
+        isg.upsert_node(user_z.clone());
+        isg.upsert_node(user_a.clone());
+        isg.upsert_node(type_target.clone());
+
+        isg.upsert_edge(SigHash(5), SigHash(7), EdgeKind::Uses)
+            .unwrap(); // z_user uses TargetType
+        isg.upsert_edge(SigHash(6), SigHash(7), EdgeKind::Uses)
+            .unwrap(); // a_user uses TargetType
+
+        let users = isg.find_users(SigHash(7)).unwrap();
+        assert_eq!(users.len(), 2);
+        assert_eq!(users[0].name.as_ref(), "a_user");
+        assert_eq!(users[1].name.as_ref(), "z_user");
+    }
+
     #[test]
     fn test_find_cycles_empty() {
         let isg = OptimizedISG::new();
         let cycles = isg.find_cycles();
-        assert!(cycles.is_empty(), "MVP implementation should return empty cycles");
+        assert!(
+            cycles.is_empty(),
+            "MVP implementation should return empty cycles"
+        );
+    }
+
+    // TDD Cycle 20: Web data serialization (RED phase)
+    #[test]
+    fn test_export_web_data_json_structure() {
+        let isg = setup_query_graph();
+
+        let json_result = isg.export_web_data();
+        assert!(json_result.is_ok(), "Web data export should succeed");
+
+        let json_str = json_result.unwrap();
+        let web_data: WebGraphData =
+            serde_json::from_str(&json_str).expect("JSON should be valid WebGraphData");
+
+        // Validate structure
+        assert_eq!(web_data.nodes.len(), 6); // FuncA, FuncB, StructC, StructD, StructE, TraitT
+        assert!(!web_data.edges.is_empty()); // Should have relationships
+        assert_eq!(web_data.metadata.node_count, 6);
+        assert!(web_data.metadata.edge_count > 0);
+
+        // Validate node structure
+        let func_a = web_data.nodes.iter().find(|n| n.name == "FuncA").unwrap();
+        assert_eq!(func_a.kind, "Function");
+        assert!(func_a.signature.contains("sig_"));
+        assert_eq!(func_a.file_path, "test.rs");
+
+        // Validate edge structure
+        let implements_edge = web_data
+            .edges
+            .iter()
+            .find(|e| e.kind == "Implements")
+            .unwrap();
+        assert!(!implements_edge.source.is_empty());
+        assert!(!implements_edge.target.is_empty());
+    }
+
+    #[test]
+    fn test_export_web_data_performance() {
+        let isg = setup_query_graph();
+
+        let start = std::time::Instant::now();
+        let result = isg.export_web_data();
+        let elapsed = start.elapsed();
+
+        assert!(result.is_ok());
+        assert!(
+            elapsed.as_millis() < 500,
+            "Web data export took {}ms (>500ms)",
+            elapsed.as_millis()
+        );
+    }
+
+    #[test]
+    fn test_export_web_data_large_graph() {
+        let isg = OptimizedISG::new();
+
+        // Create a larger graph (1000+ nodes)
+        for i in 0..1000 {
+            let node = mock_node(i, NodeKind::Function, &format!("func_{}", i));
+            isg.upsert_node(node);
+        }
+
+        // Add some edges
+        for i in 0..500 {
+            let _ = isg.upsert_edge(SigHash(i), SigHash(i + 1), EdgeKind::Calls);
+        }
+
+        let start = std::time::Instant::now();
+        let result = isg.export_web_data();
+        let elapsed = start.elapsed();
+
+        assert!(result.is_ok());
+        assert!(
+            elapsed.as_millis() < 500,
+            "Large graph export took {}ms (>500ms)",
+            elapsed.as_millis()
+        );
+
+        let json_str = result.unwrap();
+        let web_data: WebGraphData = serde_json::from_str(&json_str).unwrap();
+        assert_eq!(web_data.nodes.len(), 1000);
+        assert_eq!(web_data.metadata.node_count, 1000);
+    }
+
+    #[test]
+    fn test_web_data_json_compatibility() {
+        let isg = setup_query_graph();
+        let json_str = isg.export_web_data().unwrap();
+
+        // Test that JSON is compatible with common visualization libraries
+        let parsed: serde_json::Value = serde_json::from_str(&json_str).unwrap();
+
+        // Should have nodes array
+        assert!(parsed["nodes"].is_array());
+        let nodes = parsed["nodes"].as_array().unwrap();
+        assert!(!nodes.is_empty());
+
+        // Each node should have required fields for D3.js/vis.js
+        let first_node = &nodes[0];
+        assert!(first_node["id"].is_string());
+        assert!(first_node["name"].is_string());
+        assert!(first_node["kind"].is_string());
+
+        // Should have edges array
+        assert!(parsed["edges"].is_array());
+        let edges = parsed["edges"].as_array().unwrap();
+
+        // Each edge should have source/target for visualization libraries
+        if !edges.is_empty() {
+            let first_edge = &edges[0];
+            assert!(first_edge["source"].is_string());
+            assert!(first_edge["target"].is_string());
+            assert!(first_edge["kind"].is_string());
+        }
+
+        // Should have metadata
+        assert!(parsed["metadata"].is_object());
+        assert!(parsed["metadata"]["node_count"].is_number());
+        assert!(parsed["metadata"]["edge_count"].is_number());
     }
-}
\ No newline at end of file
+
+    // TDD Cycle 21: HTML visualization generation (RED phase)
+    #[test]
+    fn test_generate_html_visualization() {
+        let isg = setup_query_graph();
+
+        let html_result = isg.generate_html_visualization(None);
+        assert!(html_result.is_ok(), "HTML generation should succeed");
+
+        let html = html_result.unwrap();
+
+        // Validate HTML structure
+        assert!(html.contains("<!DOCTYPE html>"));
+        assert!(html.contains("<title>Parseltongue Architecture Visualization</title>"));
+        assert!(html.contains("const graphData = "));
+        assert!(html.contains("class GraphVisualization"));
+
+        // Should contain embedded graph data
+        assert!(html.contains("FuncA"));
+        assert!(html.contains("StructC"));
+        assert!(html.contains("TraitT"));
+
+        // Should be self-contained (no external dependencies)
+        assert!(!html.contains("src=\"http"));
+        assert!(!html.contains("href=\"http"));
+        assert!(!html.contains("@import"));
+    }
+
+    #[test]
+    fn test_generate_html_visualization_with_focus() {
+        let isg = setup_query_graph();
+
+        let html_result = isg.generate_html_visualization(Some("FuncA"));
+        assert!(html_result.is_ok());
+
+        let html = html_result.unwrap();
+
+        // Should contain focus entity
+        assert!(html.contains("const focusEntity = \"FuncA\""));
+        assert!(html.contains("FuncA"));
+    }
+
+    #[test]
+    fn test_html_visualization_performance() {
+        let isg = setup_query_graph();
+
+        let start = std::time::Instant::now();
+        let result = isg.generate_html_visualization(None);
+        let elapsed = start.elapsed();
+
+        assert!(result.is_ok());
+        assert!(
+            elapsed.as_millis() < 500,
+            "HTML generation took {}ms (>500ms)",
+            elapsed.as_millis()
+        );
+    }
+
+    #[test]
+    fn test_html_visualization_large_graph() {
+        let isg = OptimizedISG::new();
+
+        // Create a larger graph
+        for i in 0..100 {
+            let node = mock_node(i, NodeKind::Function, &format!("func_{}", i));
+            isg.upsert_node(node);
+        }
+
+        for i in 0..50 {
+            let _ = isg.upsert_edge(SigHash(i), SigHash(i + 1), EdgeKind::Calls);
+        }
+
+        let start = std::time::Instant::now();
+        let result = isg.generate_html_visualization(None);
+        let elapsed = start.elapsed();
+
+        assert!(result.is_ok());
+        assert!(
+            elapsed.as_millis() < 500,
+            "Large graph HTML generation took {}ms (>500ms)",
+            elapsed.as_millis()
+        );
+
+        let html = result.unwrap();
+        assert!(html.contains("func_0"));
+        assert!(html.contains("func_99"));
+    }
+
+    #[test]
+    fn test_html_self_contained() {
+        let isg = setup_query_graph();
+        let html = isg.generate_html_visualization(None).unwrap();
+
+        // Verify no external dependencies
+        assert!(!html.contains("cdn."));
+        assert!(!html.contains("googleapis.com"));
+        assert!(!html.contains("unpkg.com"));
+        assert!(!html.contains("jsdelivr.net"));
+
+        // Should have embedded CSS and JavaScript
+        assert!(html.contains("<style>"));
+        assert!(html.contains("</style>"));
+        assert!(html.contains("<script>"));
+        assert!(html.contains("</script>"));
+
+        // Should have interactive features
+        assert!(html.contains("onclick="));
+        assert!(html.contains("addEventListener"));
+        assert!(html.contains("GraphVisualization"));
+    }
+}

### src/cli.rs Diff (2,305 lines)
diff --git a/src/cli.rs b/src/cli.rs
index 7c17142..945f1e3 100644
--- a/src/cli.rs
+++ b/src/cli.rs
@@ -1,17 +1,20 @@
 //! CLI Interface for Parseltongue AIM Daemon
-//! 
+//!
 //! Provides command-line interface with performance monitoring and JSON/human output
 
 use crate::daemon::ParseltongueAIM;
+use crate::discovery::{ConcreteWorkflowOrchestrator, WorkflowOrchestrator};
+use crate::discovery::{DiscoveryEngine, EntityInfo, FileLocation, SimpleDiscoveryEngine};
 use crate::isg::ISGError;
+use crate::workspace_cli::{handle_workspace_command, WorkspaceArgs};
 use clap::{Parser, Subcommand, ValueEnum};
 use std::path::PathBuf;
-use std::time::Instant;
+use std::time::{Duration, Instant};
 
 #[derive(Parser)]
 #[command(name = "parseltongue")]
 #[command(about = "Rust-only architectural intelligence daemon")]
-#[command(version = "1.0.0")]
+#[command(version = env!("CARGO_PKG_VERSION"))]
 pub struct Cli {
     #[command(subcommand)]
     pub command: Commands,
@@ -50,7 +53,7 @@ pub enum Commands {
         format: OutputFormat,
     },
     /// Debug and visualization commands
-    Debug {
+    DebugGraph {
         /// Show graph structure
         #[arg(long)]
         graph: bool,
@@ -61,6 +64,79 @@ pub enum Commands {
         #[arg(long)]
         sample: bool,
     },
+    /// Generate interactive HTML visualization
+    Visualize {
+        /// Target entity to focus visualization on (optional)
+        entity: Option<String>,
+        /// Output HTML file path
+        #[arg(long, default_value = "parseltongue_visualization.html")]
+        output: PathBuf,
+    },
+    /// List all entities in the codebase
+    ListEntities {
+        /// Filter by entity type
+        #[arg(long, value_enum)]
+        r#type: Option<DiscoveryEntityType>,
+        /// Maximum number of results to return
+        #[arg(long, default_value = "100")]
+        limit: usize,
+        /// Output format
+        #[arg(long, default_value = "human")]
+        format: OutputFormat,
+    },
+    /// List entities defined in a specific file
+    EntitiesInFile {
+        /// File path to search
+        file: String,
+        /// Filter by entity type
+        #[arg(long, value_enum)]
+        r#type: Option<DiscoveryEntityType>,
+        /// Output format
+        #[arg(long, default_value = "human")]
+        format: OutputFormat,
+    },
+    /// Find where an entity is defined
+    WhereDefined {
+        /// Entity name to find
+        entity: String,
+        /// Output format
+        #[arg(long, default_value = "human")]
+        format: OutputFormat,
+    },
+    /// Workspace management commands
+    Workspace(WorkspaceArgs),
+    /// JTBD Workflow: Onboard to new codebase (complete in <15 minutes)
+    Onboard {
+        /// Target directory to analyze
+        target_dir: String,
+        /// Output format
+        #[arg(long, default_value = "human")]
+        format: OutputFormat,
+    },
+    /// JTBD Workflow: Plan feature development (complete in <5 minutes)
+    FeatureStart {
+        /// Target entity name to modify
+        entity: String,
+        /// Output format
+        #[arg(long, default_value = "human")]
+        format: OutputFormat,
+    },
+    /// JTBD Workflow: Debug entity usage (complete in <2 minutes)
+    Debug {
+        /// Target entity name to debug
+        entity: String,
+        /// Output format
+        #[arg(long, default_value = "human")]
+        format: OutputFormat,
+    },
+    /// JTBD Workflow: Check refactoring safety (complete in <3 minutes)
+    RefactorCheck {
+        /// Target entity name to refactor
+        entity: String,
+        /// Output format
+        #[arg(long, default_value = "human")]
+        format: OutputFormat,
+    },
 }
 
 #[derive(Debug, Clone, ValueEnum)]
@@ -71,6 +147,10 @@ pub enum QueryType {
     BlastRadius,
     /// Find circular dependencies
     FindCycles,
+    /// Find all callers of an entity
+    Calls,
+    /// Find all users of a type
+    Uses,
 }
 
 #[derive(Clone, ValueEnum)]
@@ -79,6 +159,45 @@ pub enum OutputFormat {
     Human,
     /// JSON output for LLM consumption
     Json,
+    /// PR summary markdown format
+    PrSummary,
+    /// CI/CD integration format
+    Ci,
+}
+
+#[derive(Debug, Clone, Copy, ValueEnum)]
+pub enum DiscoveryEntityType {
+    /// Function entities
+    Function,
+    /// Struct entities
+    Struct,
+    /// Trait entities
+    Trait,
+    /// Implementation blocks
+    Impl,
+    /// Module entities
+    Module,
+    /// Constant entities
+    Constant,
+    /// Static entities
+    Static,
+    /// Macro entities
+    Macro,
+}
+
+impl From<DiscoveryEntityType> for crate::discovery::types::EntityType {
+    fn from(cli_type: DiscoveryEntityType) -> Self {
+        match cli_type {
+            DiscoveryEntityType::Function => Self::Function,
+            DiscoveryEntityType::Struct => Self::Struct,
+            DiscoveryEntityType::Trait => Self::Trait,
+            DiscoveryEntityType::Impl => Self::Impl,
+            DiscoveryEntityType::Module => Self::Module,
+            DiscoveryEntityType::Constant => Self::Constant,
+            DiscoveryEntityType::Static => Self::Static,
+            DiscoveryEntityType::Macro => Self::Macro,
+        }
+    }
 }
 
 #[derive(Debug, Clone, serde::Serialize)]
@@ -111,46 +230,81 @@ impl LlmContext {
     }
 }
 
-pub fn run(cli: Cli) -> Result<(), Box<dyn std::error::Error>> {
+/// Format duration for display following parseltongue-llm-guide.md precision requirements
+/// - Always report milliseconds when duration < 1 second
+/// - Use seconds + milliseconds for durations > 1 second  
+/// - Never report "0 seconds" - use milliseconds instead
+fn format_duration(duration: Duration) -> String {
+    let total_ms = duration.as_secs_f64() * 1000.0;
+    let total_us = duration.as_micros() as f64;
+
+    if total_us < 1000.0 {
+        // Less than 1 millisecond: show in microseconds (for very fast operations)
+        format!("{:.0}Œºs", total_us)
+    } else if total_ms < 1000.0 {
+        // Less than 1 second: show in milliseconds (following guide requirement)
+        format!("{:.0} milliseconds", total_ms)
+    } else {
+        // 1 second or more: show both seconds and milliseconds for clarity
+        let secs = duration.as_secs_f64();
+        format!("{:.3} seconds ({:.0} milliseconds)", secs, total_ms)
+    }
+}
+
+pub async fn run(cli: Cli) -> Result<(), Box<dyn std::error::Error>> {
     let mut daemon = ParseltongueAIM::new();
-    
+
     // Try to load existing snapshot for persistence between commands
     let snapshot_path = std::path::Path::new("parseltongue_snapshot.json");
     if let Err(e) = daemon.load_snapshot(snapshot_path) {
         eprintln!("‚ö†Ô∏è  Could not load snapshot: {}", e);
     }
-    
+
     match cli.command {
         Commands::Ingest { file } => {
             if !file.exists() {
-                return Err(format!("File not found: {}", file.display()).into());
+                return Err(format!("üö´ File not found: {}", file.display()).into());
             }
-            
+
+            println!("ü§ñ STARK INDUSTRIES CODEBASE INGESTION PROTOCOL");
+            println!("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");
+            println!("üìÅ Target: {}", file.display());
+            println!("‚ö° Initializing JARVIS analysis...");
+
             let start = Instant::now();
             let stats = daemon.ingest_code_dump(&file)?;
             let elapsed = start.elapsed();
-            
-            println!("‚úì Ingestion complete:");
-            println!("  Files processed: {}", stats.files_processed);
-            println!("  Nodes created: {}", stats.nodes_created);
-            println!("  Total nodes in ISG: {}", daemon.isg.node_count());
-            println!("  Total edges in ISG: {}", daemon.isg.edge_count());
-            println!("  Time: {:.2}s", elapsed.as_secs_f64());
-            
+
+            println!();
+            println!("‚úÖ INGESTION PROTOCOL COMPLETE");
+            println!("  üìä Files processed: {}", stats.files_processed);
+            println!("  üîó Nodes created: {}", stats.nodes_created);
+            println!("  üåê Total nodes in ISG: {}", daemon.isg.node_count());
+            println!("  üï∏Ô∏è  Total edges in ISG: {}", daemon.isg.edge_count());
+            println!("  ‚ö° Processing time: {}", format_duration(elapsed));
+
             // Verify <5s constraint for 2.1MB dumps (Performance Contract)
             if elapsed.as_secs() > 5 {
-                eprintln!("‚ö†Ô∏è  Ingestion took {:.2}s (>5s constraint violated)", elapsed.as_secs_f64());
+                eprintln!(
+                    "‚ö†Ô∏è  PERFORMANCE ALERT: Ingestion took {:.2}s (>5s target exceeded)",
+                    elapsed.as_secs_f64()
+                );
+                eprintln!("üí° Consider optimizing for larger codebases");
+            } else {
+                println!("üéØ Performance target achieved!");
             }
-            
+
             // Save snapshot for persistence between commands
             let snapshot_path = std::path::Path::new("parseltongue_snapshot.json");
             if let Err(e) = daemon.save_snapshot(snapshot_path) {
-                eprintln!("‚ö†Ô∏è  Could not save snapshot: {}", e);
+                eprintln!("‚ö†Ô∏è  Snapshot save failed: {}", e);
             } else {
-                println!("‚úì Snapshot saved for future queries");
+                println!("üíæ Snapshot saved for future missions");
             }
+
+            println!("\nü§ñ JARVIS ready for architectural intelligence queries! ü§ñ");
         }
-        
+
         Commands::Daemon { watch } => {
             if !watch.exists() {
                 return Err(format!("Directory not found: {}", watch.display()).into());
@@ -158,52 +312,112 @@ pub fn run(cli: Cli) -> Result<(), Box<dyn std::error::Error>> {
             if !watch.is_dir() {
                 return Err(format!("Path is not a directory: {}", watch.display()).into());
             }
-            
+
             daemon.start_daemon(&watch)?;
         }
-        
-        Commands::Query { query_type, target, format } => {
+
+        Commands::Query {
+            query_type,
+            target,
+            format,
+        } => {
             if target.trim().is_empty() {
                 return Err("Target entity name cannot be empty".into());
             }
-            
+
             let start = Instant::now();
-            
+
             let result = match query_type {
                 QueryType::WhatImplements => {
                     let trait_hash = daemon.find_entity_by_name(&target)?;
                     let implementors = daemon.isg.find_implementors(trait_hash)?;
-                    implementors.into_iter().map(|n| n.name.to_string()).collect::<Vec<_>>()
+                    implementors
+                        .into_iter()
+                        .map(|n| n.name.to_string())
+                        .collect::<Vec<_>>()
                 }
                 QueryType::BlastRadius => {
                     let entity_hash = daemon.find_entity_by_name(&target)?;
                     let radius = daemon.isg.calculate_blast_radius(entity_hash)?;
                     radius.into_iter().map(|h| format!("{:?}", h)).collect()
                 }
-                QueryType::FindCycles => {
-                    daemon.isg.find_cycles().into_iter().flatten()
-                        .map(|h| format!("{:?}", h)).collect()
+                QueryType::FindCycles => daemon
+                    .isg
+                    .find_cycles()
+                    .into_iter()
+                    .flatten()
+                    .map(|h| format!("{:?}", h))
+                    .collect(),
+                QueryType::Calls => {
+                    let entity_hash = daemon.find_entity_by_name(&target)?;
+                    let callers = daemon.isg.find_callers(entity_hash)?;
+                    callers
+                        .into_iter()
+                        .map(|n| n.name.to_string())
+                        .collect::<Vec<_>>()
+                }
+                QueryType::Uses => {
+                    let entity_hash = daemon.find_entity_by_name(&target)?;
+                    let users = daemon.isg.find_users(entity_hash)?;
+                    users
+                        .into_iter()
+                        .map(|n| n.name.to_string())
+                        .collect::<Vec<_>>()
                 }
             };
-            
+
             let elapsed = start.elapsed();
-            
+
             match format {
                 OutputFormat::Human => {
-                    println!("Results for {} query on '{}':", 
-                        match query_type {
-                            QueryType::WhatImplements => "what-implements",
-                            QueryType::BlastRadius => "blast-radius", 
-                            QueryType::FindCycles => "find-cycles",
-                        }, target);
-                    for item in &result {
-                        println!("  - {}", item);
+                    // Avengers-themed query results following discovery-first approach
+                    let query_emoji = match query_type {
+                        QueryType::WhatImplements => "üîç", // Hawkeye's precision
+                        QueryType::BlastRadius => "üí•",    // Hulk's impact
+                        QueryType::FindCycles => "üåÄ",     // Doctor Strange's loops
+                        QueryType::Calls => "üìû",          // Communication network
+                        QueryType::Uses => "üï∏Ô∏è",           // Spider-Man's web
+                    };
+
+                    let query_name = match query_type {
+                        QueryType::WhatImplements => "TRAIT IMPLEMENTATION SCAN",
+                        QueryType::BlastRadius => "IMPACT BLAST RADIUS",
+                        QueryType::FindCycles => "CIRCULAR DEPENDENCY DETECTION",
+                        QueryType::Calls => "CALLER NETWORK ANALYSIS",
+                        QueryType::Uses => "USAGE WEB MAPPING",
+                    };
+
+                    println!("{} {}", query_emoji, query_name);
+                    println!("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");
+                    println!("üéØ Target: '{}'", target);
+                    println!("üìä Results found: {}", result.len());
+                    println!();
+
+                    if result.is_empty() {
+                        println!("‚ùå No results found for '{}'", target);
+                        println!("üí° Suggestions:");
+                        println!("  ‚Ä¢ Check entity name spelling");
+                        println!("  ‚Ä¢ Try 'parseltongue list-entities' to see available entities");
+                        println!("  ‚Ä¢ Ensure the codebase has been ingested");
+                    } else {
+                        for (i, item) in result.iter().enumerate() {
+                            println!("  {}. üéØ {}", i + 1, item);
+                        }
                     }
-                    println!("\nQuery completed in {}Œºs", elapsed.as_micros());
-                    
-                    // Verify performance constraints (2x tolerance)
-                    if elapsed.as_micros() > 2000 {
-                        eprintln!("‚ö†Ô∏è  Query took {}Œºs (>2ms constraint)", elapsed.as_micros());
+
+                    println!();
+                    println!("‚ö° Query completed in {}", format_duration(elapsed));
+
+                    // Verify performance constraints following guide expectations
+                    let target_us = 500; // 500Œºs target from guide
+                    if elapsed.as_micros() > target_us {
+                        eprintln!(
+                            "‚ö†Ô∏è  PERFORMANCE ALERT: Query took {} (target: <{}Œºs)",
+                            format_duration(elapsed),
+                            target_us
+                        );
+                    } else {
+                        println!("‚úÖ Performance target achieved!");
                     }
                 }
                 OutputFormat::Json => {
@@ -217,29 +431,37 @@ pub fn run(cli: Cli) -> Result<(), Box<dyn std::error::Error>> {
                     });
                     println!("{}", serde_json::to_string_pretty(&output)?);
                 }
+                OutputFormat::PrSummary | OutputFormat::Ci => {
+                    // Query results don't support PR summary or CI formats
+                    return Err(
+                        "PR summary and CI formats are not supported for query commands".into(),
+                    );
+                }
             }
         }
-        
+
         Commands::GenerateContext { entity, format } => {
             if entity.trim().is_empty() {
                 return Err("Entity name cannot be empty".into());
             }
-            
+
             let context = generate_context(&daemon, &entity, format.clone())?;
             println!("{}", context);
         }
-        
-        Commands::Debug { graph, dot, sample } => {
+
+        Commands::DebugGraph { graph, dot, sample } => {
             if sample {
                 // Create and show sample ISG for learning
                 let sample_isg = crate::isg::OptimizedISG::create_sample();
                 println!("=== SAMPLE ISG FOR LEARNING ===\n");
                 println!("This shows a simple Rust program structure:\n");
                 println!("{}", sample_isg.debug_print());
-                
+
                 if dot {
                     println!("\n=== DOT FORMAT (for Graphviz) ===");
-                    println!("Copy this to a .dot file and run: dot -Tpng graph.dot -o graph.png\n");
+                    println!(
+                        "Copy this to a .dot file and run: dot -Tpng graph.dot -o graph.png\n"
+                    );
                     println!("{}", sample_isg.export_dot());
                 }
             } else if graph {
@@ -252,206 +474,1033 @@ pub fn run(cli: Cli) -> Result<(), Box<dyn std::error::Error>> {
                 println!("Use --graph to see ISG structure, --dot for Graphviz export, or --sample for learning example");
             }
         }
+
+        Commands::Visualize { entity, output } => {
+            println!("üîÆ DOCTOR STRANGE VISUALIZATION PROTOCOL");
+            println!("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");
+            println!("‚ú® Opening the Eye of Agamotto...");
+
+            let start = Instant::now();
+
+            let html = daemon.isg.generate_html_visualization(entity.as_deref())?;
+
+            // Write HTML to file
+            std::fs::write(&output, html).map_err(|e| format!("üö´ Mystical arts failed: {}", e))?;
+
+            let elapsed = start.elapsed();
+
+            println!();
+            println!("‚úÖ MYSTICAL VISUALIZATION COMPLETE");
+            println!("  üìÑ Sanctum file: {}", output.display());
+            println!("  üåê Nodes mapped: {}", daemon.isg.node_count());
+            println!("  üï∏Ô∏è  Connections traced: {}", daemon.isg.edge_count());
+            if let Some(entity) = entity {
+                println!("  üéØ Focused entity: {}", entity);
+            }
+            println!("  ‚ö° Spell casting time: {}", format_duration(elapsed));
+
+            // Verify <500ms constraint
+            if elapsed.as_millis() > 500 {
+                eprintln!(
+                    "‚ö†Ô∏è  TEMPORAL ANOMALY: Generation took {}ms (>500ms target)",
+                    elapsed.as_millis()
+                );
+            } else {
+                println!("üéØ Mystical efficiency achieved!");
+            }
+
+            println!();
+            println!(
+                "üîÆ Open {} in your browser to witness the architectural dimensions!",
+                output.display()
+            );
+            println!("‚ú® The multiverse of code awaits your exploration! ‚ú®");
+        }
+
+        Commands::ListEntities {
+            r#type,
+            limit,
+            format,
+        } => {
+            handle_list_entities_command(&daemon, r#type, limit, format.clone()).await?;
+        }
+
+        Commands::EntitiesInFile {
+            file,
+            r#type,
+            format,
+        } => {
+            handle_entities_in_file_command(&daemon, &file, r#type, format.clone()).await?;
+        }
+
+        Commands::WhereDefined { entity, format } => {
+            handle_where_defined_command(&daemon, &entity, format.clone()).await?;
+        }
+
+        Commands::Workspace(workspace_args) => {
+            handle_workspace_command(workspace_args)
+                .await
+                .map_err(|e| format!("Workspace error: {}", e))?;
+        }
+
+        Commands::Onboard { target_dir, format } => {
+            handle_onboard_workflow(&daemon, &target_dir, format.clone()).await?;
+        }
+
+        Commands::FeatureStart { entity, format } => {
+            handle_feature_start_workflow(&daemon, &entity, format.clone()).await?;
+        }
+
+        Commands::Debug { entity, format } => {
+            handle_debug_workflow(&daemon, &entity, format.clone()).await?;
+        }
+
+        Commands::RefactorCheck { entity, format } => {
+            handle_refactor_check_workflow(&daemon, &entity, format.clone()).await?;
+        }
     }
-    
+
     Ok(())
 }
 
 /// Generate LLM context with 2-hop dependency analysis
-pub fn generate_context(daemon: &ParseltongueAIM, entity_name: &str, format: OutputFormat) -> Result<String, ISGError> {
+pub fn generate_context(
+    daemon: &ParseltongueAIM,
+    entity_name: &str,
+    format: OutputFormat,
+) -> Result<String, ISGError> {
     let start = Instant::now();
-    
+
     // Find entity by name
     let target_hash = daemon.find_entity_by_name(entity_name)?;
     let target_node = daemon.isg.get_node(target_hash)?;
-    
+
     let context = LlmContext {
         target: target_node.clone(),
         dependencies: daemon.get_dependencies(target_hash),
         callers: daemon.get_callers(target_hash),
     };
-    
+
     let elapsed = start.elapsed();
-    
+
     let result = match format {
         OutputFormat::Human => {
             let mut output = context.format_human();
             output.push_str(&format!("\nContext generated in {}Œºs", elapsed.as_micros()));
             output
         }
-        OutputFormat::Json => {
-            serde_json::to_string_pretty(&context)
-                .map_err(|e| ISGError::IoError(format!("JSON serialization failed: {}", e)))?
+        OutputFormat::Json => serde_json::to_string_pretty(&context)
+            .map_err(|e| ISGError::IoError(format!("JSON serialization failed: {}", e)))?,
+        OutputFormat::PrSummary | OutputFormat::Ci => {
+            // Context generation doesn't support PR summary or CI formats
+            return Err(ISGError::IoError(
+                "PR summary and CI formats are not supported for context generation".to_string(),
+            ));
         }
     };
-    
+
     Ok(result)
 }
 
-#[cfg(test)]
-mod tests {
-    use super::*;
-    use tempfile::TempDir;
-    use std::fs;
+/// Handle the list-entities command
+async fn handle_list_entities_command(
+    daemon: &ParseltongueAIM,
+    entity_type: Option<DiscoveryEntityType>,
+    limit: usize,
+    format: OutputFormat,
+) -> Result<(), Box<dyn std::error::Error>> {
+    let start = Instant::now();
 
-    // TDD Cycle 14: CLI parsing (RED phase)
-    #[test]
-    fn test_cli_parsing() {
-        // Test ingest command
-        let args = vec!["parseltongue", "ingest", "test.dump"];
-        let cli = Cli::try_parse_from(args).unwrap();
-        
-        match cli.command {
-            Commands::Ingest { file } => {
-                assert_eq!(file, PathBuf::from("test.dump"));
-            }
-            _ => panic!("Expected Ingest command"),
-        }
-        
-        // Test daemon command
-        let args = vec!["parseltongue", "daemon", "--watch", "/path/to/watch"];
-        let cli = Cli::try_parse_from(args).unwrap();
-        
-        match cli.command {
-            Commands::Daemon { watch } => {
-                assert_eq!(watch, PathBuf::from("/path/to/watch"));
-            }
-            _ => panic!("Expected Daemon command"),
+    // Create discovery engine
+    let discovery_engine = SimpleDiscoveryEngine::new(daemon.isg.clone());
+
+    // Convert CLI entity type to discovery entity type
+    let discovery_type = entity_type.map(|t| t.into());
+
+    // Execute the query
+    let entities = discovery_engine
+        .list_all_entities(discovery_type, limit)
+        .await
+        .map_err(|e| format!("Discovery error: {}", e))?;
+
+    let elapsed = start.elapsed();
+
+    // Format and display results
+    match format {
+        OutputFormat::Human => {
+            format_entities_human(&entities, elapsed, entity_type.is_some());
         }
-        
-        // Test query command
-        let args = vec!["parseltongue", "query", "what-implements", "TestTrait", "--format", "json"];
-        let cli = Cli::try_parse_from(args).unwrap();
-        
-        match cli.command {
-            Commands::Query { query_type, target, format } => {
-                assert!(matches!(query_type, QueryType::WhatImplements));
-                assert_eq!(target, "TestTrait");
-                assert!(matches!(format, OutputFormat::Json));
-            }
-            _ => panic!("Expected Query command"),
+        OutputFormat::Json => {
+            format_entities_json(&entities, elapsed)?;
         }
-        
-        // Test generate-context command
-        let args = vec!["parseltongue", "generate-context", "MyFunction"];
-        let cli = Cli::try_parse_from(args).unwrap();
-        
-        match cli.command {
-            Commands::GenerateContext { entity, format } => {
-                assert_eq!(entity, "MyFunction");
-                assert!(matches!(format, OutputFormat::Human));
-            }
-            _ => panic!("Expected GenerateContext command"),
+        OutputFormat::PrSummary | OutputFormat::Ci => {
+            // Entity listing doesn't support PR summary or CI formats
+            return Err("PR summary and CI formats are not supported for entity listing".into());
         }
     }
 
-    #[test]
-    fn test_cli_help_output() {
-        use clap::CommandFactory;
-        let mut cli = Cli::command();
-        let help = cli.render_help();
-        
-        // Should contain all required commands
-        assert!(help.to_string().contains("ingest"));
-        assert!(help.to_string().contains("daemon"));
-        assert!(help.to_string().contains("query"));
-        assert!(help.to_string().contains("generate-context"));
+    // Check performance contract
+    if elapsed.as_millis() > 100 {
+        eprintln!(
+            "‚ö†Ô∏è  Discovery took {}ms (>100ms contract violated)",
+            elapsed.as_millis()
+        );
     }
 
-    // TDD Cycle 15: Query command execution (RED phase)
-    #[test]
-    fn test_query_command_execution() {
-        // This test will fail until we implement query execution
-        let args = vec!["parseltongue", "query", "what-implements", "TestTrait"];
-        let cli = Cli::try_parse_from(args).unwrap();
-        
-        let result = run(cli);
-        
-        // Should fail in RED phase
-        assert!(result.is_err());
+    Ok(())
+}
+
+/// Handle the entities-in-file command
+async fn handle_entities_in_file_command(
+    daemon: &ParseltongueAIM,
+    file_path: &str,
+    entity_type: Option<DiscoveryEntityType>,
+    format: OutputFormat,
+) -> Result<(), Box<dyn std::error::Error>> {
+    let start = Instant::now();
+
+    // Create discovery engine
+    let discovery_engine = SimpleDiscoveryEngine::new(daemon.isg.clone());
+
+    // Get entities in file
+    let mut entities = discovery_engine
+        .entities_in_file(file_path)
+        .await
+        .map_err(|e| format!("Discovery error: {}", e))?;
+
+    // Apply entity type filter if specified
+    if let Some(filter_type) = entity_type {
+        let discovery_type = filter_type.into();
+        entities.retain(|entity| entity.entity_type == discovery_type);
     }
 
-    #[test]
-    fn test_query_performance_reporting() {
-        // Test that query commands measure and report performance
-        // This will be implemented in GREEN phase
-        
-        // For now, just validate the structure exists
-        assert!(true, "Performance reporting structure ready");
+    let elapsed = start.elapsed();
+
+    // Format and display results
+    match format {
+        OutputFormat::Human => {
+            format_file_entities_human(&entities, file_path, elapsed, entity_type.is_some());
+        }
+        OutputFormat::Json => {
+            format_file_entities_json(&entities, file_path, elapsed)?;
+        }
+        OutputFormat::PrSummary | OutputFormat::Ci => {
+            // File entity listing doesn't support PR summary or CI formats
+            return Err(
+                "PR summary and CI formats are not supported for file entity listing".into(),
+            );
+        }
     }
 
-    // TDD Cycle 16: Ingest and daemon commands (RED phase)
-    #[test]
-    fn test_ingest_command() {
-        let temp_dir = TempDir::new().unwrap();
-        let dump_path = temp_dir.path().join("test.dump");
-        
-        fs::write(&dump_path, "FILE: test.rs\npub fn test() {}").unwrap();
-        
-        let args = vec!["parseltongue", "ingest", dump_path.to_str().unwrap()];
-        let cli = Cli::try_parse_from(args).unwrap();
-        
-        let result = run(cli);
-        
-        // Should succeed in GREEN phase
-        assert!(result.is_ok());
+    // Check performance contract
+    if elapsed.as_millis() > 100 {
+        eprintln!(
+            "‚ö†Ô∏è  Discovery took {}ms (>100ms contract violated)",
+            elapsed.as_millis()
+        );
     }
 
-    #[test]
-    fn test_daemon_command() {
-        let temp_dir = TempDir::new().unwrap();
-        
-        let args = vec!["parseltongue", "daemon", "--watch", temp_dir.path().to_str().unwrap()];
-        let cli = Cli::try_parse_from(args).unwrap();
-        
-        // For testing, we need to avoid the infinite loop
-        // This test just verifies the CLI parsing works correctly
-        match cli.command {
-            Commands::Daemon { watch } => {
-                assert_eq!(watch, temp_dir.path());
-            }
-            _ => panic!("Expected daemon command"),
+    Ok(())
+}
+
+/// Handle the where-defined command
+async fn handle_where_defined_command(
+    daemon: &ParseltongueAIM,
+    entity_name: &str,
+    format: OutputFormat,
+) -> Result<(), Box<dyn std::error::Error>> {
+    let start = Instant::now();
+
+    // Create discovery engine
+    let discovery_engine = SimpleDiscoveryEngine::new(daemon.isg.clone());
+
+    // Find entity definition
+    let location = discovery_engine
+        .where_defined(entity_name)
+        .await
+        .map_err(|e| format!("Discovery error: {}", e))?;
+
+    let elapsed = start.elapsed();
+
+    // Format and display results
+    match format {
+        OutputFormat::Human => {
+            format_location_human(entity_name, &location, elapsed);
+        }
+        OutputFormat::Json => {
+            format_location_json(entity_name, &location, elapsed)?;
+        }
+        OutputFormat::PrSummary | OutputFormat::Ci => {
+            // Location lookup doesn't support PR summary or CI formats
+            return Err("PR summary and CI formats are not supported for location lookup".into());
         }
     }
 
-    // TDD Cycle 17: LLM context generation (RED phase)
-    #[test]
-    fn test_generate_context_human() {
-        let daemon = ParseltongueAIM::new();
-        
-        let result = generate_context(&daemon, "test_function", OutputFormat::Human);
-        
-        // Should fail in RED phase
-        assert!(result.is_err());
+    // Check performance contract (stricter for exact lookups)
+    if elapsed.as_micros() > 50_000 {
+        eprintln!(
+            "‚ö†Ô∏è  Lookup took {}Œºs (>50ms contract violated)",
+            elapsed.as_micros()
+        );
     }
 
-    #[test]
-    fn test_generate_context_json() {
-        let daemon = ParseltongueAIM::new();
-        
-        let result = generate_context(&daemon, "test_function", OutputFormat::Json);
-        
-        // Should fail in RED phase
-        assert!(result.is_err());
+    Ok(())
+}
+
+/// Format entities for human-readable output with Avengers theme
+fn format_entities_human(entities: &[EntityInfo], elapsed: std::time::Duration, filtered: bool) {
+    if entities.is_empty() {
+        println!("üîç No entities detected in the codebase.");
+        println!();
+        println!("ü§ñ DISCOVERY-FIRST TROUBLESHOOTING (parseltongue-llm-guide.md):");
+        println!("  1. üéØ Ingest codebase: 'parseltongue ingest codebase.dump'");
+        println!("  2. üîç Check ISG status: 'parseltongue debug-graph --graph'");
+        println!("  3. üìÅ Verify file format: Ensure proper FILE: markers in dump");
+        println!("  4. ‚ö° Performance check: Ingestion should complete in 1-3 seconds");
+        println!();
+        println!("üí° Expected baseline: ~2177 nodes, 3933 edges for Parseltongue itself");
+        return;
     }
 
-    #[test]
-    fn test_generate_context_command() {
-        let args = vec!["parseltongue", "generate-context", "TestFunction", "--format", "json"];
-        let cli = Cli::try_parse_from(args).unwrap();
-        
-        let result = run(cli);
-        
-        // Should fail in RED phase
-        assert!(result.is_err());
+    let type_filter_text = if filtered { " (filtered by type)" } else { "" };
+    println!("üõ°Ô∏è  PARSELTONGUE ENTITY SCAN COMPLETE");
+    println!("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");
+    println!(
+        "üìä Discovered {} entities{}",
+        entities.len(),
+        type_filter_text
+    );
+    println!();
+
+    // Group entities by type for better organization
+    let mut by_type = std::collections::HashMap::new();
+    for entity in entities {
+        by_type
+            .entry(entity.entity_type)
+            .or_insert_with(Vec::new)
+            .push(entity);
     }
 
-    // TDD Cycle 18: LLM context formatting (RED phase)
-    #[test]
-    fn test_llm_context_format_human() {
-        use crate::isg::{NodeData, NodeKind, SigHash};
+    // Sort types for consistent output
+    let mut types: Vec<_> = by_type.keys().collect();
+    types.sort_by_key(|t| format!("{:?}", t));
+
+    for entity_type in types {
+        let entities_of_type = by_type.get(entity_type).unwrap();
+
+        // Avengers-themed emojis for different entity types
+        let type_emoji = match format!("{:?}", entity_type).as_str() {
+            "Function" => "üî®", // Thor's hammer for functions
+            "Struct" => "üõ°Ô∏è",   // Captain America's shield for structs
+            "Trait" => "üíé",    // Infinity stones for traits
+            "Impl" => "üîß",     // Iron Man's tech for implementations
+            "Module" => "üèóÔ∏è",   // Building blocks
+            "Constant" => "üíé", // Precious constants
+            "Static" => "‚ö°",   // Static power
+            "Macro" => "ü™Ñ",    // Magic macros
+            _ => "‚ö°",
+        };
+
+        println!(
+            "{} {:?} ({}):",
+            type_emoji,
+            entity_type,
+            entities_of_type.len()
+        );
+
+        for entity in entities_of_type {
+            let location = if let Some(line) = entity.line_number {
+                format!("{}:{}", entity.file_path, line)
+            } else {
+                entity.file_path.clone()
+            };
+            println!("  üéØ {} ({})", entity.name, location);
+        }
+        println!();
+    }
+
+    // Performance validation following parseltongue-llm-guide.md expectations
+    let target_ms = 100; // <100ms target from guide
+    let speed_emoji = if elapsed.as_millis() < target_ms {
+        "‚ö°"
+    } else {
+        "üêå"
+    };
+    let status = if elapsed.as_millis() < target_ms {
+        "‚úÖ TARGET ACHIEVED"
+    } else {
+        "‚ö†Ô∏è PERFORMANCE REVIEW NEEDED"
+    };
+
+    println!(
+        "{}Ô∏è Discovery completed in {} {} (target: <{} milliseconds)",
+        speed_emoji,
+        format_duration(elapsed),
+        status,
+        target_ms
+    );
+}
+
+/// Format entities for JSON output
+fn format_entities_json(
+    entities: &[EntityInfo],
+    elapsed: std::time::Duration,
+) -> Result<(), Box<dyn std::error::Error>> {
+    let output = serde_json::json!({
+        "command": "list-entities",
+        "results": entities,
+        "count": entities.len(),
+        "execution_time_ms": elapsed.as_secs_f64() * 1000.0,
+        "timestamp": chrono::Utc::now().to_rfc3339()
+    });
+
+    println!("{}", serde_json::to_string_pretty(&output)?);
+    Ok(())
+}
+
+/// Format file entities for human-readable output with Avengers theme
+fn format_file_entities_human(
+    entities: &[EntityInfo],
+    file_path: &str,
+    elapsed: std::time::Duration,
+    filtered: bool,
+) {
+    let type_filter_text = if filtered { " (filtered by type)" } else { "" };
+    println!("üîç SPIDER-SENSE FILE SCAN");
+    println!("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");
+    println!("üìÅ Target: '{}'", file_path);
+    println!(
+        "üìä Entities detected: {}{}",
+        entities.len(),
+        type_filter_text
+    );
+
+    if entities.is_empty() {
+        println!("üï∑Ô∏è  No entities found in this web node.");
+        println!();
+        println!("üîç SPIDER-SENSE ANALYSIS:");
+        println!("  ‚Ä¢ File might contain only imports/comments");
+        println!("  ‚Ä¢ File might not be properly ingested");
+        println!("  ‚Ä¢ File might have parsing errors");
+        println!();
+        println!("üí° Discovery-First Next Steps:");
+        println!("  1. üéØ Check overall entities: 'parseltongue list-entities --limit 10'");
+        println!("  2. üîç Verify file exists in ISG");
+        println!("  3. üìÅ Try a known file: 'parseltongue entities-in-file src/main.rs'");
+        return;
+    }
+
+    println!();
+
+    // Group by type
+    let mut by_type = std::collections::HashMap::new();
+    for entity in entities {
+        by_type
+            .entry(entity.entity_type)
+            .or_insert_with(Vec::new)
+            .push(entity);
+    }
+
+    let mut types: Vec<_> = by_type.keys().collect();
+    types.sort_by_key(|t| format!("{:?}", t));
+
+    for entity_type in types {
+        let entities_of_type = by_type.get(entity_type).unwrap();
+
+        // Avengers-themed emojis for different entity types
+        let type_emoji = match format!("{:?}", entity_type).as_str() {
+            "Function" => "üî®", // Thor's hammer for functions
+            "Struct" => "üõ°Ô∏è",   // Captain America's shield for structs
+            "Trait" => "üíé",    // Infinity stones for traits
+            "Impl" => "üîß",     // Iron Man's tech for implementations
+            "Module" => "üèóÔ∏è",   // Building blocks
+            "Constant" => "üíé", // Precious constants
+            "Static" => "‚ö°",   // Static power
+            "Macro" => "ü™Ñ",    // Magic macros
+            _ => "‚ö°",
+        };
+
+        println!(
+            "{} {:?} ({}):",
+            type_emoji,
+            entity_type,
+            entities_of_type.len()
+        );
+
+        for entity in entities_of_type {
+            if let Some(line) = entity.line_number {
+                println!("  üéØ {} (line {})", entity.name, line);
+            } else {
+                println!("  üéØ {}", entity.name);
+            }
+        }
+        println!();
+    }
+
+    // Performance validation following parseltongue-llm-guide.md expectations
+    let target_ms = 100; // <100ms target from guide
+    let speed_emoji = if elapsed.as_millis() < target_ms {
+        "‚ö°"
+    } else {
+        "üêå"
+    };
+    let status = if elapsed.as_millis() < target_ms {
+        "‚úÖ WEB-SLINGER SPEED"
+    } else {
+        "‚ö†Ô∏è NEED MORE SPIDER-POWER"
+    };
+
+    println!(
+        "{}Ô∏è Web scan completed in {} {} (target: <{} milliseconds)",
+        speed_emoji,
+        format_duration(elapsed),
+        status,
+        target_ms
+    );
+}
+
+/// Format file entities for JSON output
+fn format_file_entities_json(
+    entities: &[EntityInfo],
+    file_path: &str,
+    elapsed: std::time::Duration,
+) -> Result<(), Box<dyn std::error::Error>> {
+    let output = serde_json::json!({
+        "command": "entities-in-file",
+        "file_path": file_path,
+        "results": entities,
+        "count": entities.len(),
+        "execution_time_ms": elapsed.as_secs_f64() * 1000.0,
+        "timestamp": chrono::Utc::now().to_rfc3339()
+    });
+
+    println!("{}", serde_json::to_string_pretty(&output)?);
+    Ok(())
+}
+
+/// Format location for human-readable output with Avengers theme
+fn format_location_human(
+    entity_name: &str,
+    location: &Option<FileLocation>,
+    elapsed: std::time::Duration,
+) {
+    match location {
+        Some(loc) => {
+            println!("üéØ HAWKEYE PRECISION TARGETING");
+            println!("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");
+            println!("üèπ Target acquired: '{}'", entity_name);
+            println!("üìÅ File: {}", loc.file_path);
+            if let Some(line) = loc.line_number {
+                if let Some(col) = loc.column {
+                    println!("üìç Coordinates: line {}, column {}", line, col);
+                } else {
+                    println!("üìç Line: {}", line);
+                }
+            }
+            println!("üîó Editor link: {}", loc.format_for_editor());
+            println!("‚úÖ Direct hit confirmed!");
+        }
+        None => {
+            println!("üèπ HAWKEYE TARGET ACQUISITION FAILED");
+            println!("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");
+            println!("‚ùå Entity '{}' not found in the codebase.", entity_name);
+            println!();
+            println!("üîç Discovery-First Troubleshooting (following parseltongue-llm-guide.md):");
+            println!("  1. üéØ Get overview: 'parseltongue list-entities --limit 50'");
+            println!("  2. üîç Search by type: 'parseltongue list-entities --type functions'");
+            println!("  3. üìÅ Check specific file: 'parseltongue entities-in-file src/main.rs'");
+            println!("  4. ü§ñ Ensure ingestion: 'parseltongue ingest codebase.dump'");
+            println!();
+            println!("üí° Common issues:");
+            println!("  ‚Ä¢ Entity name case sensitivity");
+            println!("  ‚Ä¢ Missing namespace/module prefix");
+            println!("  ‚Ä¢ Entity might be private/internal");
+            println!("  ‚Ä¢ Codebase not yet ingested");
+        }
+    }
+
+    println!();
+    // Performance validation following parseltongue-llm-guide.md expectations
+    let target_ms = 50; // <50ms target from guide for exact lookups
+    let speed_emoji = if elapsed.as_millis() < target_ms {
+        "‚ö°"
+    } else {
+        "üêå"
+    };
+    let status = if elapsed.as_millis() < target_ms {
+        "‚úÖ HAWKEYE PRECISION"
+    } else {
+        "‚ö†Ô∏è RECALIBRATING TARGETING SYSTEM"
+    };
+
+    println!(
+        "{}Ô∏è Targeting completed in {} {} (target: <{} milliseconds)",
+        speed_emoji,
+        format_duration(elapsed),
+        status,
+        target_ms
+    );
+}
+
+/// Format location for JSON output
+fn format_location_json(
+    entity_name: &str,
+    location: &Option<FileLocation>,
+    elapsed: std::time::Duration,
+) -> Result<(), Box<dyn std::error::Error>> {
+    let output = serde_json::json!({
+        "command": "where-defined",
+        "entity_name": entity_name,
+        "found": location.is_some(),
+        "location": location,
+        "execution_time_us": elapsed.as_micros(),
+        "timestamp": chrono::Utc::now().to_rfc3339()
+    });
+
+    println!("{}", serde_json::to_string_pretty(&output)?);
+    Ok(())
+}
+
+/// Handle the onboard workflow command
+async fn handle_onboard_workflow(
+    daemon: &ParseltongueAIM,
+    target_dir: &str,
+    format: OutputFormat,
+) -> Result<(), Box<dyn std::error::Error>> {
+    let start = std::time::Instant::now();
+
+    // Create workflow orchestrator
+    let orchestrator = ConcreteWorkflowOrchestrator::new(std::sync::Arc::new(daemon.isg.clone()));
+
+    // Execute onboard workflow
+    let result = orchestrator
+        .onboard(target_dir)
+        .await
+        .map_err(|e| format!("Onboard workflow error: {}", e))?;
+
+    let elapsed = start.elapsed();
+
+    // Format and display results using new OutputFormatter system
+    let formatter = match format {
+        OutputFormat::Human => crate::discovery::FormatterFactory::create_formatter("human")?,
+        OutputFormat::Json => crate::discovery::FormatterFactory::create_formatter("json")?,
+        OutputFormat::PrSummary => {
+            crate::discovery::FormatterFactory::create_formatter("pr-summary")?
+        }
+        OutputFormat::Ci => crate::discovery::FormatterFactory::create_formatter("ci")?,
+    };
+
+    let formatted_output = formatter
+        .format_onboarding(&result)
+        .map_err(|e| format!("Output formatting error: {}", e))?;
+
+    println!("{}", formatted_output);
+
+    // Check performance contract: <15 minutes
+    if elapsed.as_secs() > 15 * 60 {
+        eprintln!(
+            "‚ö†Ô∏è  Onboard workflow took {:.2}s (>15 minutes contract violated)",
+            elapsed.as_secs_f64()
+        );
+    }
+
+    Ok(())
+}
+
+/// Handle the feature-start workflow command
+async fn handle_feature_start_workflow(
+    daemon: &ParseltongueAIM,
+    entity: &str,
+    format: OutputFormat,
+) -> Result<(), Box<dyn std::error::Error>> {
+    let start = std::time::Instant::now();
+
+    // Create workflow orchestrator
+    let orchestrator = ConcreteWorkflowOrchestrator::new(std::sync::Arc::new(daemon.isg.clone()));
+
+    // Execute feature-start workflow
+    let result = orchestrator
+        .feature_start(entity)
+        .await
+        .map_err(|e| format!("Feature start workflow error: {}", e))?;
+
+    let elapsed = start.elapsed();
+
+    // Format and display results using new OutputFormatter system
+    let formatter = match format {
+        OutputFormat::Human => crate::discovery::FormatterFactory::create_formatter("human")?,
+        OutputFormat::Json => crate::discovery::FormatterFactory::create_formatter("json")?,
+        OutputFormat::PrSummary => {
+            crate::discovery::FormatterFactory::create_formatter("pr-summary")?
+        }
+        OutputFormat::Ci => crate::discovery::FormatterFactory::create_formatter("ci")?,
+    };
+
+    let formatted_output = formatter
+        .format_feature_plan(&result)
+        .map_err(|e| format!("Output formatting error: {}", e))?;
+
+    println!("{}", formatted_output);
+
+    // Check performance contract: <5 minutes
+    if elapsed.as_secs() > 5 * 60 {
+        eprintln!(
+            "‚ö†Ô∏è  Feature start workflow took {:.2}s (>5 minutes contract violated)",
+            elapsed.as_secs_f64()
+        );
+    }
+
+    Ok(())
+}
+
+/// Handle the debug workflow command
+async fn handle_debug_workflow(
+    daemon: &ParseltongueAIM,
+    entity: &str,
+    format: OutputFormat,
+) -> Result<(), Box<dyn std::error::Error>> {
+    let start = std::time::Instant::now();
+
+    // Create workflow orchestrator
+    let orchestrator = ConcreteWorkflowOrchestrator::new(std::sync::Arc::new(daemon.isg.clone()));
+
+    // Execute debug workflow
+    let result = orchestrator
+        .debug(entity)
+        .await
+        .map_err(|e| format!("Debug workflow error: {}", e))?;
+
+    let elapsed = start.elapsed();
+
+    // Format and display results using new OutputFormatter system
+    let formatter = match format {
+        OutputFormat::Human => crate::discovery::FormatterFactory::create_formatter("human")?,
+        OutputFormat::Json => crate::discovery::FormatterFactory::create_formatter("json")?,
+        OutputFormat::PrSummary => {
+            crate::discovery::FormatterFactory::create_formatter("pr-summary")?
+        }
+        OutputFormat::Ci => crate::discovery::FormatterFactory::create_formatter("ci")?,
+    };
+
+    let formatted_output = formatter
+        .format_debug(&result)
+        .map_err(|e| format!("Output formatting error: {}", e))?;
+
+    println!("{}", formatted_output);
+
+    // Check performance contract: <2 minutes
+    if elapsed.as_secs() > 2 * 60 {
+        eprintln!(
+            "‚ö†Ô∏è  Debug workflow took {:.2}s (>2 minutes contract violated)",
+            elapsed.as_secs_f64()
+        );
+    }
+
+    Ok(())
+}
+
+/// Handle the refactor-check workflow command
+async fn handle_refactor_check_workflow(
+    daemon: &ParseltongueAIM,
+    entity: &str,
+    format: OutputFormat,
+) -> Result<(), Box<dyn std::error::Error>> {
+    let start = std::time::Instant::now();
+
+    // Create workflow orchestrator
+    let orchestrator = ConcreteWorkflowOrchestrator::new(std::sync::Arc::new(daemon.isg.clone()));
+
+    // Execute refactor-check workflow
+    let result = orchestrator
+        .refactor_check(entity)
+        .await
+        .map_err(|e| format!("Refactor check workflow error: {}", e))?;
+
+    let elapsed = start.elapsed();
+
+    // Format and display results using new OutputFormatter system
+    let formatter = match format {
+        OutputFormat::Human => crate::discovery::FormatterFactory::create_formatter("human")?,
+        OutputFormat::Json => crate::discovery::FormatterFactory::create_formatter("json")?,
+        OutputFormat::PrSummary => {
+            crate::discovery::FormatterFactory::create_formatter("pr-summary")?
+        }
+        OutputFormat::Ci => crate::discovery::FormatterFactory::create_formatter("ci")?,
+    };
+
+    let formatted_output = formatter
+        .format_refactor(&result)
+        .map_err(|e| format!("Output formatting error: {}", e))?;
+
+    println!("{}", formatted_output);
+
+    // Check performance contract: <3 minutes
+    if elapsed.as_secs() > 3 * 60 {
+        eprintln!(
+            "‚ö†Ô∏è  Refactor check workflow took {:.2}s (>3 minutes contract violated)",
+            elapsed.as_secs_f64()
+        );
+    }
+
+    Ok(())
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use std::fs;
+    use tempfile::TempDir;
+
+    // TDD Cycle 14: CLI parsing (RED phase)
+    #[test]
+    fn test_cli_parsing() {
+        // Test ingest command
+        let args = vec!["parseltongue", "ingest", "test.dump"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::Ingest { file } => {
+                assert_eq!(file, PathBuf::from("test.dump"));
+            }
+            _ => panic!("Expected Ingest command"),
+        }
+
+        // Test daemon command
+        let args = vec!["parseltongue", "daemon", "--watch", "/path/to/watch"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::Daemon { watch } => {
+                assert_eq!(watch, PathBuf::from("/path/to/watch"));
+            }
+            _ => panic!("Expected Daemon command"),
+        }
+
+        // Test query command
+        let args = vec![
+            "parseltongue",
+            "query",
+            "what-implements",
+            "TestTrait",
+            "--format",
+            "json",
+        ];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::Query {
+                query_type,
+                target,
+                format,
+            } => {
+                assert!(matches!(query_type, QueryType::WhatImplements));
+                assert_eq!(target, "TestTrait");
+                assert!(matches!(format, OutputFormat::Json));
+            }
+            _ => panic!("Expected Query command"),
+        }
+
+        // Test generate-context command
+        let args = vec!["parseltongue", "generate-context", "MyFunction"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::GenerateContext { entity, format } => {
+                assert_eq!(entity, "MyFunction");
+                assert!(matches!(format, OutputFormat::Human));
+            }
+            _ => panic!("Expected GenerateContext command"),
+        }
+    }
+
+    #[test]
+    fn test_cli_help_output() {
+        use clap::CommandFactory;
+        let mut cli = Cli::command();
+        let help = cli.render_help();
+
+        // Should contain all required commands
+        assert!(help.to_string().contains("ingest"));
+        assert!(help.to_string().contains("daemon"));
+        assert!(help.to_string().contains("query"));
+        assert!(help.to_string().contains("generate-context"));
+    }
+
+    // TDD Cycle 15: Query command execution (RED phase)
+    #[tokio::test]
+    async fn test_query_command_execution() {
+        // Query commands should work now
+        let args = vec!["parseltongue", "query", "what-implements", "TestTrait"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        let result = run(cli).await;
+
+        // Should succeed now that query execution is implemented
+        assert!(result.is_ok());
+    }
+
+    #[test]
+    fn test_calls_query_parsing() {
+        let args = vec![
+            "parseltongue",
+            "query",
+            "calls",
+            "test_function",
+            "--format",
+            "json",
+        ];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::Query {
+                query_type,
+                target,
+                format,
+            } => {
+                assert!(matches!(query_type, QueryType::Calls));
+                assert_eq!(target, "test_function");
+                assert!(matches!(format, OutputFormat::Json));
+            }
+            _ => panic!("Expected Query command"),
+        }
+    }
+
+    #[test]
+    fn test_uses_query_parsing() {
+        let args = vec!["parseltongue", "query", "uses", "TestStruct"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::Query {
+                query_type,
+                target,
+                format,
+            } => {
+                assert!(matches!(query_type, QueryType::Uses));
+                assert_eq!(target, "TestStruct");
+                assert!(matches!(format, OutputFormat::Human));
+            }
+            _ => panic!("Expected Query command"),
+        }
+    }
+
+    #[tokio::test]
+    async fn test_calls_query_execution() {
+        // Test calls query execution - should succeed now that find_callers is implemented
+        let args = vec!["parseltongue", "query", "calls", "test_function"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        let result = run(cli).await;
+
+        // Should succeed now that find_callers is implemented
+        // If entity doesn't exist, it should return a proper error message, not crash
+        assert!(result.is_ok() || result.unwrap_err().to_string().contains("Entity not found"));
+    }
+
+    #[tokio::test]
+    async fn test_uses_query_execution() {
+        // Uses query commands should work now
+        let args = vec!["parseltongue", "query", "uses", "TestStruct"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        let result = run(cli).await;
+
+        // Should succeed now that query execution is implemented
+        assert!(result.is_ok());
+    }
+
+    #[test]
+    fn test_query_performance_reporting() {
+        // Test that query commands measure and report performance
+        // This will be implemented in GREEN phase
+
+        // For now, just validate the structure exists
+        assert!(true, "Performance reporting structure ready");
+    }
+
+    // TDD Cycle 16: Ingest and daemon commands (RED phase)
+    #[tokio::test]
+    async fn test_ingest_command() {
+        let temp_dir = TempDir::new().unwrap();
+        let dump_path = temp_dir.path().join("test.dump");
+
+        fs::write(&dump_path, "FILE: test.rs\npub fn test() {}").unwrap();
+
+        let args = vec!["parseltongue", "ingest", dump_path.to_str().unwrap()];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        let result = run(cli).await;
+
+        // Should succeed in GREEN phase
+        assert!(result.is_ok());
+    }
+
+    #[test]
+    fn test_daemon_command() {
+        let temp_dir = TempDir::new().unwrap();
+
+        let args = vec![
+            "parseltongue",
+            "daemon",
+            "--watch",
+            temp_dir.path().to_str().unwrap(),
+        ];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        // For testing, we need to avoid the infinite loop
+        // This test just verifies the CLI parsing works correctly
+        match cli.command {
+            Commands::Daemon { watch } => {
+                assert_eq!(watch, temp_dir.path());
+            }
+            _ => panic!("Expected daemon command"),
+        }
+    }
+
+    // TDD Cycle 17: LLM context generation (RED phase)
+    #[test]
+    fn test_generate_context_human() {
+        let daemon = ParseltongueAIM::new();
+
+        let result = generate_context(&daemon, "test_function", OutputFormat::Human);
+
+        // Should fail in RED phase
+        assert!(result.is_err());
+    }
+
+    #[test]
+    fn test_generate_context_json() {
+        let daemon = ParseltongueAIM::new();
+
+        let result = generate_context(&daemon, "test_function", OutputFormat::Json);
+
+        // Should fail in RED phase
+        assert!(result.is_err());
+    }
+
+    #[tokio::test]
+    async fn test_generate_context_command() {
+        let args = vec![
+            "parseltongue",
+            "generate-context",
+            "TestFunction",
+            "--format",
+            "json",
+        ];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        let result = run(cli).await;
+
+        // Should fail in RED phase
+        assert!(result.is_err());
+    }
+
+    // TDD Cycle 18: LLM context formatting (RED phase)
+    #[test]
+    fn test_llm_context_format_human() {
+        use crate::isg::{NodeData, NodeKind, SigHash};
         use std::sync::Arc;
-        
+
         let target = NodeData {
             hash: SigHash(1),
             kind: NodeKind::Function,
@@ -460,15 +1509,15 @@ mod tests {
             file_path: Arc::from("test.rs"),
             line: 10,
         };
-        
+
         let context = LlmContext {
             target,
             dependencies: Vec::new(),
             callers: Vec::new(),
         };
-        
+
         let formatted = context.format_human();
-        
+
         assert!(formatted.contains("test_function"));
         assert!(formatted.contains("Function"));
         assert!(formatted.contains("test.rs:10"));
@@ -480,7 +1529,7 @@ mod tests {
     fn test_llm_context_json_serialization() {
         use crate::isg::{NodeData, NodeKind, SigHash};
         use std::sync::Arc;
-        
+
         let target = NodeData {
             hash: SigHash(1),
             kind: NodeKind::Function,
@@ -489,15 +1538,15 @@ mod tests {
             file_path: Arc::from("test.rs"),
             line: 10,
         };
-        
+
         let context = LlmContext {
             target,
             dependencies: Vec::new(),
             callers: Vec::new(),
         };
-        
+
         let json = serde_json::to_string_pretty(&context).unwrap();
-        
+
         assert!(json.contains("test_function"));
         assert!(json.contains("Function"));
         assert!(json.contains("dependencies"));
@@ -505,11 +1554,11 @@ mod tests {
     }
 
     // TDD Cycle 19: End-to-end workflow (RED phase)
-    #[test]
-    fn test_end_to_end_workflow() {
+    #[tokio::test]
+    async fn test_end_to_end_workflow() {
         let temp_dir = TempDir::new().unwrap();
         let dump_path = temp_dir.path().join("test.dump");
-        
+
         // Create test dump
         let dump_content = r#"
 FILE: src/lib.rs
@@ -531,19 +1580,19 @@ impl Greeter for Person {
     }
 }
 "#;
-        
+
         fs::write(&dump_path, dump_content).unwrap();
-        
+
         // Test complete workflow: ingest ‚Üí query ‚Üí context
-        
+
         // 1. Ingest
         let ingest_args = vec!["parseltongue", "ingest", dump_path.to_str().unwrap()];
         let ingest_cli = Cli::try_parse_from(ingest_args).unwrap();
-        let ingest_result = run(ingest_cli);
-        
+        let ingest_result = run(ingest_cli).await;
+
         // Should succeed in GREEN phase
         assert!(ingest_result.is_ok());
-        
+
         // TODO: Add query and context generation tests in future iterations
     }
 
@@ -551,14 +1600,573 @@ impl Greeter for Person {
     fn test_performance_requirements_met() {
         // This test validates all performance requirements are met
         // Will be implemented in GREEN phase
-        
+
         // Performance targets:
         // - Code dump ingestion: <5s for 2.1MB
         // - File updates: <12ms
         // - Simple queries: <500Œºs
         // - Complex queries: <1ms
         // - Persistence: <500ms
-        
+
         assert!(true, "Performance requirements test structure ready");
     }
-}
\ No newline at end of file
+
+    // TDD Cycle 22: Visualize command (RED phase)
+    #[test]
+    fn test_visualize_command_parsing() {
+        // Test visualize command without entity
+        let args = vec!["parseltongue", "visualize"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::Visualize { entity, output } => {
+                assert!(entity.is_none());
+                assert_eq!(output, PathBuf::from("parseltongue_visualization.html"));
+            }
+            _ => panic!("Expected Visualize command"),
+        }
+
+        // Test visualize command with entity and custom output
+        let args = vec![
+            "parseltongue",
+            "visualize",
+            "MyFunction",
+            "--output",
+            "custom.html",
+        ];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::Visualize { entity, output } => {
+                assert_eq!(entity, Some("MyFunction".to_string()));
+                assert_eq!(output, PathBuf::from("custom.html"));
+            }
+            _ => panic!("Expected Visualize command"),
+        }
+    }
+
+    #[tokio::test]
+    async fn test_visualize_command_execution() {
+        let temp_dir = TempDir::new().unwrap();
+        let output_path = temp_dir.path().join("test_visualization.html");
+
+        let args = vec![
+            "parseltongue",
+            "visualize",
+            "--output",
+            output_path.to_str().unwrap(),
+        ];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        let result = run(cli).await;
+
+        // Should succeed and create HTML file
+        assert!(result.is_ok());
+        assert!(output_path.exists());
+
+        // Verify HTML content
+        let html_content = fs::read_to_string(&output_path).unwrap();
+        assert!(html_content.contains("<!DOCTYPE html>"));
+        assert!(html_content.contains("Parseltongue Architecture Visualization"));
+    }
+
+    #[test]
+    fn test_visualize_command_with_focus() {
+        let temp_dir = TempDir::new().unwrap();
+        let output_path = temp_dir.path().join("focused_visualization.html");
+
+        let args = vec![
+            "parseltongue",
+            "visualize",
+            "TestFunction",
+            "--output",
+            output_path.to_str().unwrap(),
+        ];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        let rt = tokio::runtime::Runtime::new().unwrap();
+        let result = rt.block_on(run(cli));
+
+        // Should succeed even if entity doesn't exist (graceful handling)
+        assert!(result.is_ok());
+        assert!(output_path.exists());
+
+        let html_content = fs::read_to_string(&output_path).unwrap();
+        assert!(html_content.contains("TestFunction"));
+    }
+
+    // Discovery command parsing tests
+    #[test]
+    fn test_list_entities_command_parsing() {
+        // Test basic list-entities command
+        let args = vec!["parseltongue", "list-entities"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::ListEntities {
+                r#type,
+                limit,
+                format,
+            } => {
+                assert!(r#type.is_none());
+                assert_eq!(limit, 100); // default
+                assert!(matches!(format, OutputFormat::Human)); // default
+            }
+            _ => panic!("Expected ListEntities command"),
+        }
+
+        // Test with type filter
+        let args = vec![
+            "parseltongue",
+            "list-entities",
+            "--type",
+            "function",
+            "--limit",
+            "50",
+        ];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::ListEntities {
+                r#type,
+                limit,
+                format,
+            } => {
+                assert!(matches!(r#type, Some(DiscoveryEntityType::Function)));
+                assert_eq!(limit, 50);
+                assert!(matches!(format, OutputFormat::Human));
+            }
+            _ => panic!("Expected ListEntities command"),
+        }
+
+        // Test with JSON format
+        let args = vec!["parseltongue", "list-entities", "--format", "json"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::ListEntities {
+                r#type,
+                limit,
+                format,
+            } => {
+                assert!(r#type.is_none());
+                assert_eq!(limit, 100);
+                assert!(matches!(format, OutputFormat::Json));
+            }
+            _ => panic!("Expected ListEntities command"),
+        }
+    }
+
+    #[test]
+    fn test_entities_in_file_command_parsing() {
+        // Test basic entities-in-file command
+        let args = vec!["parseltongue", "entities-in-file", "src/main.rs"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::EntitiesInFile {
+                file,
+                r#type,
+                format,
+            } => {
+                assert_eq!(file, "src/main.rs");
+                assert!(r#type.is_none());
+                assert!(matches!(format, OutputFormat::Human));
+            }
+            _ => panic!("Expected EntitiesInFile command"),
+        }
+
+        // Test with type filter and JSON format
+        let args = vec![
+            "parseltongue",
+            "entities-in-file",
+            "src/lib.rs",
+            "--type",
+            "struct",
+            "--format",
+            "json",
+        ];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::EntitiesInFile {
+                file,
+                r#type,
+                format,
+            } => {
+                assert_eq!(file, "src/lib.rs");
+                assert!(matches!(r#type, Some(DiscoveryEntityType::Struct)));
+                assert!(matches!(format, OutputFormat::Json));
+            }
+            _ => panic!("Expected EntitiesInFile command"),
+        }
+    }
+
+    #[test]
+    fn test_where_defined_command_parsing() {
+        // Test basic where-defined command
+        let args = vec!["parseltongue", "where-defined", "test_function"];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::WhereDefined { entity, format } => {
+                assert_eq!(entity, "test_function");
+                assert!(matches!(format, OutputFormat::Human));
+            }
+            _ => panic!("Expected WhereDefined command"),
+        }
+
+        // Test with JSON format
+        let args = vec![
+            "parseltongue",
+            "where-defined",
+            "MyStruct",
+            "--format",
+            "json",
+        ];
+        let cli = Cli::try_parse_from(args).unwrap();
+
+        match cli.command {
+            Commands::WhereDefined { entity, format } => {
+                assert_eq!(entity, "MyStruct");
+                assert!(matches!(format, OutputFormat::Json));
+            }
+            _ => panic!("Expected WhereDefined command"),
+        }
+    }
+
+    #[test]
+    fn test_discovery_entity_type_conversion() {
+        // Test all entity type conversions
+        use crate::discovery::types::EntityType;
+
+        assert_eq!(
+            EntityType::from(DiscoveryEntityType::Function),
+            EntityType::Function
+        );
+        assert_eq!(
+            EntityType::from(DiscoveryEntityType::Struct),
+            EntityType::Struct
+        );
+        assert_eq!(
+            EntityType::from(DiscoveryEntityType::Trait),
+            EntityType::Trait
+        );
+        assert_eq!(
+            EntityType::from(DiscoveryEntityType::Impl),
+            EntityType::Impl
+        );
+        assert_eq!(
+            EntityType::from(DiscoveryEntityType::Module),
+            EntityType::Module
+        );
+        assert_eq!(
+            EntityType::from(DiscoveryEntityType::Constant),
+            EntityType::Constant
+        );
+        assert_eq!(
+            EntityType::from(DiscoveryEntityType::Static),
+            EntityType::Static
+        );
+        assert_eq!(
+            EntityType::from(DiscoveryEntityType::Macro),
+            EntityType::Macro
+        );
+    }
+
+    // Integration tests for discovery commands
+    #[tokio::test]
+    async fn test_list_entities_command_execution() {
+        let temp_dir = TempDir::new().unwrap();
+        let dump_path = temp_dir.path().join("test.dump");
+
+        // Create test dump with entities
+        let dump_content = r#"
+FILE: src/lib.rs
+pub fn hello_world() -> String {
+    "Hello, World!".to_string()
+}
+
+pub struct Person {
+    name: String,
+    age: u32,
+}
+
+pub trait Greeter {
+    fn greet(&self) -> String;
+}
+
+impl Greeter for Person {
+    fn greet(&self) -> String {
+        format!("Hello, I'm {}", self.name)
+    }
+}
+"#;
+
+        fs::write(&dump_path, dump_content).unwrap();
+
+        // First ingest the data
+        let ingest_args = vec!["parseltongue", "ingest", dump_path.to_str().unwrap()];
+        let ingest_cli = Cli::try_parse_from(ingest_args).unwrap();
+        let ingest_result = run(ingest_cli).await;
+        assert!(ingest_result.is_ok());
+
+        // Test list-entities command
+        let list_args = vec!["parseltongue", "list-entities", "--limit", "10"];
+        let list_cli = Cli::try_parse_from(list_args).unwrap();
+        let list_result = run(list_cli).await;
+
+        // Should succeed
+        assert!(list_result.is_ok());
+    }
+
+    #[tokio::test]
+    async fn test_list_entities_with_type_filter() {
+        let temp_dir = TempDir::new().unwrap();
+        let dump_path = temp_dir.path().join("test.dump");
+
+        let dump_content = r#"
+FILE: src/lib.rs
+pub fn test_function() {}
+pub struct TestStruct {}
+pub trait TestTrait {}
+"#;
+
+        fs::write(&dump_path, dump_content).unwrap();
+
+        // Ingest data
+        let ingest_args = vec!["parseltongue", "ingest", dump_path.to_str().unwrap()];
+        let ingest_cli = Cli::try_parse_from(ingest_args).unwrap();
+        let _ = run(ingest_cli).await;
+
+        // Test with function filter
+        let list_args = vec!["parseltongue", "list-entities", "--type", "function"];
+        let list_cli = Cli::try_parse_from(list_args).unwrap();
+        let list_result = run(list_cli).await;
+        assert!(list_result.is_ok());
+
+        // Test with struct filter
+        let list_args = vec!["parseltongue", "list-entities", "--type", "struct"];
+        let list_cli = Cli::try_parse_from(list_args).unwrap();
+        let list_result = run(list_cli).await;
+        assert!(list_result.is_ok());
+    }
+
+    #[tokio::test]
+    async fn test_entities_in_file_command_execution() {
+        let temp_dir = TempDir::new().unwrap();
+        let dump_path = temp_dir.path().join("test.dump");
+
+        let dump_content = r#"
+FILE: src/main.rs
+pub fn main() {
+    println!("Hello, World!");
+}
+
+pub fn helper() -> i32 {
+    42
+}
+
+FILE: src/lib.rs
+pub struct Config {
+    debug: bool,
+}
+"#;
+
+        fs::write(&dump_path, dump_content).unwrap();
+
+        // Ingest data
+        let ingest_args = vec!["parseltongue", "ingest", dump_path.to_str().unwrap()];
+        let ingest_cli = Cli::try_parse_from(ingest_args).unwrap();
+        let _ = run(ingest_cli).await;
+
+        // Test entities-in-file command
+        let file_args = vec!["parseltongue", "entities-in-file", "src/main.rs"];
+        let file_cli = Cli::try_parse_from(file_args).unwrap();
+        let file_result = run(file_cli).await;
+        assert!(file_result.is_ok());
+
+        // Test with type filter
+        let file_args = vec![
+            "parseltongue",
+            "entities-in-file",
+            "src/main.rs",
+            "--type",
+            "function",
+        ];
+        let file_cli = Cli::try_parse_from(file_args).unwrap();
+        let file_result = run(file_cli).await;
+        assert!(file_result.is_ok());
+    }
+
+    #[tokio::test]
+    async fn test_where_defined_command_execution() {
+        let temp_dir = TempDir::new().unwrap();
+        let dump_path = temp_dir.path().join("test.dump");
+
+        let dump_content = r#"
+FILE: src/lib.rs
+pub fn target_function() -> String {
+    "Found me!".to_string()
+}
+"#;
+
+        fs::write(&dump_path, dump_content).unwrap();
+
+        // Ingest data
+        let ingest_args = vec!["parseltongue", "ingest", dump_path.to_str().unwrap()];
+        let ingest_cli = Cli::try_parse_from(ingest_args).unwrap();
+        let _ = run(ingest_cli).await;
+
+        // Test where-defined command
+        let where_args = vec!["parseltongue", "where-defined", "target_function"];
+        let where_cli = Cli::try_parse_from(where_args).unwrap();
+        let where_result = run(where_cli).await;
+        assert!(where_result.is_ok());
+
+        // Test with non-existent entity
+        let where_args = vec!["parseltongue", "where-defined", "nonexistent_function"];
+        let where_cli = Cli::try_parse_from(where_args).unwrap();
+        let where_result = run(where_cli).await;
+        assert!(where_result.is_ok()); // Should succeed but report not found
+    }
+
+    #[tokio::test]
+    async fn test_discovery_commands_json_output() {
+        let temp_dir = TempDir::new().unwrap();
+        let dump_path = temp_dir.path().join("test.dump");
+
+        let dump_content = r#"
+FILE: src/lib.rs
+pub fn json_test() {}
+"#;
+
+        fs::write(&dump_path, dump_content).unwrap();
+
+        // Ingest data
+        let ingest_args = vec!["parseltongue", "ingest", dump_path.to_str().unwrap()];
+        let ingest_cli = Cli::try_parse_from(ingest_args).unwrap();
+        let _ = run(ingest_cli).await;
+
+        // Test list-entities with JSON output
+        let list_args = vec!["parseltongue", "list-entities", "--format", "json"];
+        let list_cli = Cli::try_parse_from(list_args).unwrap();
+        let list_result = run(list_cli).await;
+        assert!(list_result.is_ok());
+
+        // Test entities-in-file with JSON output
+        let file_args = vec![
+            "parseltongue",
+            "entities-in-file",
+            "src/lib.rs",
+            "--format",
+            "json",
+        ];
+        let file_cli = Cli::try_parse_from(file_args).unwrap();
+        let file_result = run(file_cli).await;
+        assert!(file_result.is_ok());
+
+        // Test where-defined with JSON output
+        let where_args = vec![
+            "parseltongue",
+            "where-defined",
+            "json_test",
+            "--format",
+            "json",
+        ];
+        let where_cli = Cli::try_parse_from(where_args).unwrap();
+        let where_result = run(where_cli).await;
+        assert!(where_result.is_ok());
+    }
+
+    #[tokio::test]
+    async fn test_discovery_commands_performance_contracts() {
+        let temp_dir = TempDir::new().unwrap();
+        let dump_path = temp_dir.path().join("test.dump");
+
+        // Create a reasonably sized test dump
+        let mut dump_content = String::new();
+        dump_content.push_str("FILE: src/lib.rs\n");
+
+        // Add multiple entities to test performance
+        for i in 0..50 {
+            dump_content.push_str(&format!("pub fn test_function_{}() {{}}\n", i));
+            dump_content.push_str(&format!("pub struct TestStruct{} {{}}\n", i));
+        }
+
+        fs::write(&dump_path, dump_content).unwrap();
+
+        // Ingest data
+        let ingest_args = vec!["parseltongue", "ingest", dump_path.to_str().unwrap()];
+        let ingest_cli = Cli::try_parse_from(ingest_args).unwrap();
+        let _ = run(ingest_cli).await;
+
+        // Test list-entities performance
+        let start = Instant::now();
+        let list_args = vec!["parseltongue", "list-entities"];
+        let list_cli = Cli::try_parse_from(list_args).unwrap();
+        let _ = run(list_cli).await;
+        let list_elapsed = start.elapsed();
+
+        // Should meet <100ms contract for discovery operations
+        assert!(
+            list_elapsed.as_millis() < 100,
+            "list-entities took {:?}, expected <100ms",
+            list_elapsed
+        );
+
+        // Test where-defined performance
+        let start = Instant::now();
+        let where_args = vec!["parseltongue", "where-defined", "test_function_0"];
+        let where_cli = Cli::try_parse_from(where_args).unwrap();
+        let _ = run(where_cli).await;
+        let where_elapsed = start.elapsed();
+
+        // Should meet <50ms contract for exact lookups
+        assert!(
+            where_elapsed.as_millis() < 50,
+            "where-defined took {:?}, expected <50ms",
+            where_elapsed
+        );
+    }
+
+    #[test]
+    fn test_cli_help_includes_discovery_commands() {
+        use clap::CommandFactory;
+        let mut cli = Cli::command();
+        let help = cli.render_help();
+        let help_text = help.to_string();
+
+        // Should contain all discovery commands
+        assert!(help_text.contains("list-entities"));
+        assert!(help_text.contains("entities-in-file"));
+        assert!(help_text.contains("where-defined"));
+
+        // Should contain command descriptions
+        assert!(help_text.contains("List all entities in the codebase"));
+        assert!(help_text.contains("List entities defined in a specific file"));
+        assert!(help_text.contains("Find where an entity is defined"));
+    }
+
+    #[test]
+    fn test_discovery_command_error_handling() {
+        // Test invalid entity type
+        let args = vec!["parseltongue", "list-entities", "--type", "invalid"];
+        let result = Cli::try_parse_from(args);
+        assert!(result.is_err());
+
+        // Test invalid format
+        let args = vec!["parseltongue", "list-entities", "--format", "invalid"];
+        let result = Cli::try_parse_from(args);
+        assert!(result.is_err());
+
+        // Test missing required arguments
+        let args = vec!["parseltongue", "entities-in-file"];
+        let result = Cli::try_parse_from(args);
+        assert!(result.is_err());
+
+        let args = vec!["parseltongue", "where-defined"];
+        let result = Cli::try_parse_from(args);
+        assert!(result.is_err());
+    }
+}

### src/daemon.rs Diff (2,130 lines)
diff --git a/src/daemon.rs b/src/daemon.rs
index 953581c..6951815 100644
--- a/src/daemon.rs
+++ b/src/daemon.rs
@@ -1,14 +1,264 @@
 //! Parseltongue AIM Daemon - File monitoring and code parsing
-//! 
+//!
 //! Handles live file monitoring (<12ms updates) and code dump ingestion (<5s for 2.1MB)
 
-use crate::isg::{OptimizedISG, NodeData, NodeKind, SigHash, ISGError};
+use crate::isg::{EdgeKind, ISGError, NodeData, NodeKind, OptimizedISG, SigHash};
 use notify::RecommendedWatcher;
 use petgraph::visit::{EdgeRef, IntoEdgeReferences};
 use std::path::Path;
 use std::sync::atomic::AtomicBool;
 use std::sync::Arc;
 use std::time::Instant;
+use syn::visit::Visit;
+
+/// ModuleContext - Tracks current module path for FQN generation
+#[derive(Debug, Clone)]
+struct ModuleContext {
+    path: Vec<String>,
+}
+
+impl ModuleContext {
+    fn new() -> Self {
+        Self { path: Vec::new() }
+    }
+
+    fn push(&mut self, module_name: String) {
+        self.path.push(module_name);
+    }
+
+    fn pop(&mut self) {
+        self.path.pop();
+    }
+
+    fn generate_fqn(&self, item_name: &str, item_type: &str) -> String {
+        if self.path.is_empty() {
+            format!("{} {}", item_type, item_name)
+        } else {
+            format!("{} {}::{}", item_type, self.path.join("::"), item_name)
+        }
+    }
+}
+
+/// RelationshipExtractor - Uses syn::visit::Visit to detect CALLS and USES relationships
+struct RelationshipExtractor {
+    current_function: SigHash,
+    current_module_context: Vec<String>,
+    relationships: Vec<(SigHash, SigHash, EdgeKind)>,
+}
+
+impl RelationshipExtractor {
+    fn new(current_function: SigHash, module_context: Vec<String>) -> Self {
+        Self {
+            current_function,
+            current_module_context: module_context,
+            relationships: Vec::new(),
+        }
+    }
+
+    /// Resolve function call target to SigHash
+    fn resolve_call_target(&self, call: &syn::ExprCall) -> Option<SigHash> {
+        match call.func.as_ref() {
+            // Handle function calls like `target_function()` or `utils::load_config()`
+            syn::Expr::Path(path_expr) => {
+                // Build full path for module-qualified calls
+                let path_segments: Vec<String> = path_expr
+                    .path
+                    .segments
+                    .iter()
+                    .map(|s| s.ident.to_string())
+                    .collect();
+
+                if path_segments.is_empty() {
+                    return None;
+                }
+
+                // Try both simple name and full path
+                let _simple_name = path_segments.last().unwrap();
+                let _full_path = path_segments.join("::");
+
+                // Try different resolution strategies:
+
+                // 1. Try as absolute path (e.g., utils::load_config)
+                let absolute_path = path_segments.join("::");
+                let absolute_signature = format!("fn {}", absolute_path);
+                let absolute_hash = SigHash::from_signature(&absolute_signature);
+
+                // 2. Try relative to current module context (e.g., inner::deep_function -> outer::inner::deep_function)
+                if !self.current_module_context.is_empty() {
+                    let mut relative_path = self.current_module_context.clone();
+                    relative_path.extend(path_segments.clone());
+                    let relative_full_path = relative_path.join("::");
+                    let relative_signature = format!("fn {}", relative_full_path);
+                    let relative_hash = SigHash::from_signature(&relative_signature);
+
+                    // For now, prefer the relative resolution for nested modules
+                    return Some(relative_hash);
+                }
+
+                // 3. Try simple name (for local functions in same module)
+                let simple_name = path_segments.last().unwrap();
+                if !self.current_module_context.is_empty() {
+                    let mut simple_path = self.current_module_context.clone();
+                    simple_path.push(simple_name.clone());
+                    let simple_full_path = simple_path.join("::");
+                    let simple_signature = format!("fn {}", simple_full_path);
+                    let simple_hash = SigHash::from_signature(&simple_signature);
+                    return Some(simple_hash);
+                }
+
+                // 4. Fallback to absolute path
+                Some(absolute_hash)
+            }
+            // Handle closure calls and other complex patterns
+            _ => {
+                // For MVP, skip complex call patterns
+                None
+            }
+        }
+    }
+
+    /// Resolve method call target to SigHash
+    fn resolve_method_target(&self, call: &syn::ExprMethodCall) -> Option<SigHash> {
+        let method_name = call.method.to_string();
+        let signature = format!("fn {}", method_name);
+        Some(SigHash::from_signature(&signature))
+    }
+
+    /// Resolve type path to SigHash with module context awareness
+    fn resolve_type_path(&self, type_path: &syn::TypePath) -> Option<SigHash> {
+        let path_segments: Vec<String> = type_path
+            .path
+            .segments
+            .iter()
+            .map(|s| s.ident.to_string())
+            .collect();
+
+        if path_segments.is_empty() {
+            return None;
+        }
+
+        let type_name = path_segments.last().unwrap();
+
+        // Skip primitive types
+        if matches!(
+            type_name.as_str(),
+            "i32"
+                | "i64"
+                | "u32"
+                | "u64"
+                | "f32"
+                | "f64"
+                | "bool"
+                | "String"
+                | "str"
+                | "Vec"
+                | "Option"
+                | "Result"
+        ) {
+            return None;
+        }
+
+        // Try different resolution strategies:
+
+        // 1. Try as absolute path (e.g., models::User)
+        let absolute_path = path_segments.join("::");
+        let absolute_signature = format!("struct {}", absolute_path);
+        let absolute_hash = SigHash::from_signature(&absolute_signature);
+
+        // 2. Try relative to current module context (e.g., User -> services::User)
+        if !self.current_module_context.is_empty() {
+            let mut relative_path = self.current_module_context.clone();
+            relative_path.extend(path_segments.clone());
+            let relative_full_path = relative_path.join("::");
+            let relative_signature = format!("struct {}", relative_full_path);
+            let relative_hash = SigHash::from_signature(&relative_signature);
+
+            // For single-segment paths, also try other modules (simple heuristic for use statements)
+            if path_segments.len() == 1 {
+                // Try common module patterns: models::Type, types::Type, etc.
+                let common_modules = ["models", "types", "entities", "domain"];
+                if let Some(module) = common_modules.first() {
+                    let module_signature = format!("struct {}::{}", module, type_name);
+                    let module_hash = SigHash::from_signature(&module_signature);
+                    // For MVP, return the first common module match
+                    // In a full implementation, we'd check if the node actually exists
+                    return Some(module_hash);
+                }
+            }
+
+            return Some(relative_hash);
+        }
+
+        // 3. For single-segment paths with no module context, try simple name first
+        if path_segments.len() == 1 {
+            // First try simple name (for top-level types)
+            let simple_signature = format!("struct {}", type_name);
+            let simple_hash = SigHash::from_signature(&simple_signature);
+
+            // For now, prefer simple resolution for top-level types
+            return Some(simple_hash);
+        }
+
+        // 4. Fallback to absolute path
+        Some(absolute_hash)
+    }
+
+    /// Resolve struct expression to SigHash
+    fn resolve_struct_expr(&self, expr_struct: &syn::ExprStruct) -> Option<SigHash> {
+        if let Some(segment) = expr_struct.path.segments.last() {
+            let type_name = segment.ident.to_string();
+            let signature = format!("struct {}", type_name);
+            return Some(SigHash::from_signature(&signature));
+        }
+        None
+    }
+}
+
+impl<'ast> Visit<'ast> for RelationshipExtractor {
+    fn visit_expr_call(&mut self, call: &'ast syn::ExprCall) {
+        // Detect function calls like `target_function()`
+        if let Some(target_hash) = self.resolve_call_target(call) {
+            self.relationships
+                .push((self.current_function, target_hash, EdgeKind::Calls));
+        }
+
+        // Continue visiting nested expressions
+        syn::visit::visit_expr_call(self, call);
+    }
+
+    fn visit_expr_method_call(&mut self, call: &'ast syn::ExprMethodCall) {
+        // Detect method calls like `obj.method_call()`
+        if let Some(target_hash) = self.resolve_method_target(call) {
+            self.relationships
+                .push((self.current_function, target_hash, EdgeKind::Calls));
+        }
+
+        // Continue visiting nested expressions
+        syn::visit::visit_expr_method_call(self, call);
+    }
+
+    fn visit_type_path(&mut self, type_path: &'ast syn::TypePath) {
+        // Detect type usage in signatures and bodies
+        if let Some(type_hash) = self.resolve_type_path(type_path) {
+            self.relationships
+                .push((self.current_function, type_hash, EdgeKind::Uses));
+        }
+
+        // Continue visiting nested types
+        syn::visit::visit_type_path(self, type_path);
+    }
+
+    fn visit_expr_struct(&mut self, expr_struct: &'ast syn::ExprStruct) {
+        // Detect struct construction like `User { name: "test" }`
+        if let Some(type_hash) = self.resolve_struct_expr(expr_struct) {
+            self.relationships
+                .push((self.current_function, type_hash, EdgeKind::Uses));
+        }
+
+        // Continue visiting nested expressions
+        syn::visit::visit_expr_struct(self, expr_struct);
+    }
+}
 
 pub struct ParseltongueAIM {
     pub isg: OptimizedISG,
@@ -23,6 +273,12 @@ pub struct IngestStats {
     pub nodes_created: usize,
 }
 
+impl Default for ParseltongueAIM {
+    fn default() -> Self {
+        Self::new()
+    }
+}
+
 impl ParseltongueAIM {
     pub fn new() -> Self {
         Self {
@@ -34,30 +290,31 @@ impl ParseltongueAIM {
 
     /// Signal the daemon to shutdown gracefully
     pub fn shutdown(&self) {
-        self.shutdown.store(true, std::sync::atomic::Ordering::Relaxed);
+        self.shutdown
+            .store(true, std::sync::atomic::Ordering::Relaxed);
     }
 
     /// Ingest code dump with FILE: markers - Target: <5s for 2.1MB
     pub fn ingest_code_dump(&mut self, file_path: &Path) -> Result<IngestStats, ISGError> {
         use std::fs;
-        
+
         let content = fs::read_to_string(file_path)
             .map_err(|e| ISGError::IoError(format!("Failed to read file: {}", e)))?;
-        
+
         let mut stats = IngestStats::default();
         let mut current_file = String::new();
         let mut current_content = String::new();
-        
+
         for line in content.lines() {
-            if line.starts_with("FILE: ") {
+            if let Some(stripped) = line.strip_prefix("FILE: ") {
                 // Process previous file if it exists and is a Rust file
                 if !current_file.is_empty() && current_file.ends_with(".rs") {
                     self.parse_rust_file(&current_file, &current_content)?;
                     stats.files_processed += 1;
                 }
-                
+
                 // Start new file
-                current_file = line[6..].trim().to_string();
+                current_file = stripped.trim().to_string();
                 current_content.clear();
             } else if line.starts_with("=") && line.chars().all(|c| c == '=') {
                 // Skip separator lines (e.g., "================================================")
@@ -67,116 +324,259 @@ impl ParseltongueAIM {
                 current_content.push('\n');
             }
         }
-        
+
         // Process last file if it's a Rust file
         if !current_file.is_empty() && current_file.ends_with(".rs") {
             self.parse_rust_file(&current_file, &current_content)?;
             stats.files_processed += 1;
         }
-        
+
         stats.nodes_created = self.isg.node_count();
         Ok(stats)
     }
 
-    /// Parse Rust file using syn crate
-    fn parse_rust_file(&mut self, file_path: &str, code: &str) -> Result<(), ISGError> {
-        use syn::{Item, ItemFn, ItemStruct, ItemTrait, ItemImpl};
+    /// Parse Rust file using syn crate with two-pass ingestion
+    pub fn parse_rust_file(&mut self, file_path: &str, code: &str) -> Result<(), ISGError> {
         use std::sync::Arc;
-        
+
         let syntax_tree = match syn::parse_file(code) {
             Ok(tree) => tree,
             Err(e) => {
                 // Log parsing error but continue processing other files
-                eprintln!("‚ö†Ô∏è  Parse error in {}: {} (continuing with other files)", file_path, e);
+                eprintln!(
+                    "‚ö†Ô∏è  Parse error in {}: {} (continuing with other files)",
+                    file_path, e
+                );
                 return Ok(());
             }
         };
-        
+
         let file_path_arc: Arc<str> = Arc::from(file_path);
-        
-        for item in syntax_tree.items {
+
+        // PASS 1: Extract all nodes first (functions, structs, traits) with FQN support
+        let mut context = ModuleContext::new();
+        self.extract_nodes_recursive(&syntax_tree.items, &mut context, &file_path_arc);
+
+        // PASS 2: Extract relationships after all nodes exist with FQN support
+        let mut context = ModuleContext::new();
+        self.extract_relationships_recursive(&syntax_tree.items, &mut context);
+
+        Ok(())
+    }
+
+    /// Recursively extract nodes from items, handling nested modules
+    fn extract_nodes_recursive(
+        &mut self,
+        items: &[syn::Item],
+        context: &mut ModuleContext,
+        file_path: &Arc<str>,
+    ) {
+        use syn::{Item, ItemFn, ItemImpl, ItemStruct, ItemTrait};
+        for item in items {
             match item {
                 Item::Fn(ItemFn { sig, .. }) => {
                     let name = sig.ident.to_string();
-                    let signature = format!("fn {}", quote::quote!(#sig));
+                    let signature = context.generate_fqn(&name, "fn");
                     let hash = SigHash::from_signature(&signature);
-                    
+
                     let node = NodeData {
                         hash,
                         kind: NodeKind::Function,
                         name: Arc::from(name),
                         signature: Arc::from(signature),
-                        file_path: file_path_arc.clone(),
+                        file_path: file_path.clone(),
                         line: 0, // TODO: Extract actual line number
                     };
-                    
+
                     self.isg.upsert_node(node);
                 }
-                
+
                 Item::Struct(ItemStruct { ident, .. }) => {
                     let name = ident.to_string();
-                    let signature = format!("struct {}", name);
+                    let signature = context.generate_fqn(&name, "struct");
                     let hash = SigHash::from_signature(&signature);
-                    
+
                     let node = NodeData {
                         hash,
                         kind: NodeKind::Struct,
                         name: Arc::from(name),
                         signature: Arc::from(signature),
-                        file_path: file_path_arc.clone(),
+                        file_path: file_path.clone(),
                         line: 0,
                     };
-                    
+
                     self.isg.upsert_node(node);
                 }
-                
+
                 Item::Trait(ItemTrait { ident, .. }) => {
                     let name = ident.to_string();
-                    let signature = format!("trait {}", name);
+                    let signature = context.generate_fqn(&name, "trait");
                     let hash = SigHash::from_signature(&signature);
-                    
+
                     let node = NodeData {
                         hash,
                         kind: NodeKind::Trait,
                         name: Arc::from(name),
                         signature: Arc::from(signature),
-                        file_path: file_path_arc.clone(),
+                        file_path: file_path.clone(),
                         line: 0,
                     };
-                    
+
                     self.isg.upsert_node(node);
                 }
-                
-                Item::Impl(ItemImpl { trait_, self_ty, .. }) => {
+
+                Item::Mod(module) => {
+                    // Handle nested modules
+                    let module_name = module.ident.to_string();
+                    context.push(module_name);
+
+                    if let Some((_, items)) = &module.content {
+                        self.extract_nodes_recursive(items, context, file_path);
+                    }
+
+                    context.pop();
+                }
+
+                // Extract methods from impl blocks
+                Item::Impl(ItemImpl { items, .. }) => {
+                    for impl_item in items {
+                        if let syn::ImplItem::Fn(method) = impl_item {
+                            let name = method.sig.ident.to_string();
+                            let signature = context.generate_fqn(&name, "fn");
+                            let hash = SigHash::from_signature(&signature);
+
+                            let node = NodeData {
+                                hash,
+                                kind: NodeKind::Function,
+                                name: Arc::from(name),
+                                signature: Arc::from(signature),
+                                file_path: file_path.clone(),
+                                line: 0,
+                            };
+
+                            self.isg.upsert_node(node);
+                        }
+                    }
+                }
+
+                _ => {
+                    // Ignore other items for MVP
+                }
+            }
+        }
+    }
+
+    /// Recursively extract relationships from items, handling nested modules
+    fn extract_relationships_recursive(
+        &mut self,
+        items: &[syn::Item],
+        context: &mut ModuleContext,
+    ) {
+        use syn::{Item, ItemImpl};
+        for item in items {
+            match item {
+                Item::Fn(func) => {
+                    // Extract CALLS and USES relationships from function
+                    let caller_name = func.sig.ident.to_string();
+                    let caller_sig = context.generate_fqn(&caller_name, "fn");
+                    let caller_hash = SigHash::from_signature(&caller_sig);
+
+                    let mut extractor =
+                        RelationshipExtractor::new(caller_hash, context.path.clone());
+
+                    // Extract type usage from function signature
+                    extractor.visit_signature(&func.sig);
+
+                    // Extract relationships from function body
+                    extractor.visit_item_fn(func);
+
+                    // Add discovered relationships to ISG
+                    for (from, to, kind) in extractor.relationships {
+                        if self.isg.get_node(to).is_ok() {
+                            let _ = self.isg.upsert_edge(from, to, kind);
+                        }
+                    }
+                }
+
+                Item::Mod(module) => {
+                    // Handle nested modules
+                    let module_name = module.ident.to_string();
+                    context.push(module_name);
+
+                    if let Some((_, items)) = &module.content {
+                        self.extract_relationships_recursive(items, context);
+                    }
+
+                    context.pop();
+                }
+
+                Item::Impl(ItemImpl {
+                    trait_,
+                    self_ty,
+                    items,
+                    ..
+                }) => {
                     // Handle trait implementations
                     if let Some((_, trait_path, _)) = trait_ {
                         if let syn::Type::Path(type_path) = self_ty.as_ref() {
-                            if let (Some(struct_name), Some(trait_name)) = (
-                                type_path.path.segments.last().map(|s| s.ident.to_string()),
-                                trait_path.segments.last().map(|s| s.ident.to_string())
-                            ) {
-                                // Create edge: Struct implements Trait
-                                let struct_sig = format!("struct {}", struct_name);
-                                let trait_sig = format!("trait {}", trait_name);
+                            let struct_name =
+                                type_path.path.segments.last().map(|s| s.ident.to_string());
+                            let trait_name =
+                                trait_path.segments.last().map(|s| s.ident.to_string());
+
+                            if let (Some(struct_name), Some(trait_name)) = (struct_name, trait_name)
+                            {
+                                // Create edge: Struct implements Trait (with FQN)
+                                let struct_sig = context.generate_fqn(&struct_name, "struct");
+                                let trait_sig = context.generate_fqn(&trait_name, "trait");
                                 let struct_hash = SigHash::from_signature(&struct_sig);
                                 let trait_hash = SigHash::from_signature(&trait_sig);
-                                
+
                                 // Only create edge if both nodes exist
-                                if self.isg.get_node(struct_hash).is_ok() && self.isg.get_node(trait_hash).is_ok() {
-                                    let _ = self.isg.upsert_edge(struct_hash, trait_hash, crate::isg::EdgeKind::Implements);
+                                if self.isg.get_node(struct_hash).is_ok()
+                                    && self.isg.get_node(trait_hash).is_ok()
+                                {
+                                    let _ = self.isg.upsert_edge(
+                                        struct_hash,
+                                        trait_hash,
+                                        crate::isg::EdgeKind::Implements,
+                                    );
+                                }
+                            }
+                        }
+                    }
+
+                    // Extract CALLS relationships from method bodies
+                    for impl_item in items {
+                        if let syn::ImplItem::Fn(method) = impl_item {
+                            let caller_name = method.sig.ident.to_string();
+                            let caller_sig = context.generate_fqn(&caller_name, "fn");
+                            let caller_hash = SigHash::from_signature(&caller_sig);
+
+                            let mut extractor =
+                                RelationshipExtractor::new(caller_hash, context.path.clone());
+
+                            // Extract type usage from method signature
+                            extractor.visit_signature(&method.sig);
+
+                            // Extract relationships from method body
+                            extractor.visit_impl_item_fn(method);
+
+                            // Add discovered relationships to ISG
+                            for (from, to, kind) in extractor.relationships {
+                                if self.isg.get_node(to).is_ok() {
+                                    let _ = self.isg.upsert_edge(from, to, kind);
                                 }
                             }
                         }
                     }
                 }
-                
+
                 _ => {
                     // Ignore other items for MVP
                 }
             }
         }
-        
-        Ok(())
     }
 
     /// Start daemon with <12ms update constraint
@@ -184,19 +584,20 @@ impl ParseltongueAIM {
         use notify::{RecursiveMode, Watcher};
         use std::sync::mpsc;
         use std::time::Duration;
-        
+
         let (tx, rx) = mpsc::channel();
-        
+
         let mut watcher = notify::recommended_watcher(tx)
             .map_err(|e| ISGError::IoError(format!("Failed to create file watcher: {}", e)))?;
-        
-        watcher.watch(watch_dir, RecursiveMode::Recursive)
+
+        watcher
+            .watch(watch_dir, RecursiveMode::Recursive)
             .map_err(|e| ISGError::IoError(format!("Failed to watch directory: {}", e)))?;
-        
+
         self.file_watcher = Some(watcher);
-        
+
         println!("üêç Watching {} for .rs files", watch_dir.display());
-        
+
         // Event loop with <12ms update constraint
         loop {
             match rx.recv_timeout(Duration::from_millis(100)) {
@@ -204,7 +605,7 @@ impl ParseltongueAIM {
                     if self.shutdown.load(std::sync::atomic::Ordering::Relaxed) {
                         break;
                     }
-                    
+
                     if let Err(e) = self.handle_file_event(event) {
                         eprintln!("Error handling file event: {}", e);
                     }
@@ -220,7 +621,7 @@ impl ParseltongueAIM {
                 }
             }
         }
-        
+
         println!("üêç File monitoring stopped");
         Ok(())
     }
@@ -228,7 +629,7 @@ impl ParseltongueAIM {
     /// Handle file system events
     fn handle_file_event(&mut self, event: notify::Event) -> Result<(), ISGError> {
         use notify::EventKind;
-        
+
         match event.kind {
             EventKind::Create(_) | EventKind::Modify(_) => {
                 for path in event.paths {
@@ -236,15 +637,21 @@ impl ParseltongueAIM {
                         let start = Instant::now();
                         self.update_file(&path)?;
                         let elapsed = start.elapsed();
-                        
+
                         // Critical: Verify <25ms constraint (2x tolerance)
                         if elapsed.as_millis() > 25 {
-                            eprintln!("‚ö†Ô∏è  Update took {}ms (>25ms constraint violated)", 
-                                elapsed.as_millis());
+                            eprintln!(
+                                "‚ö†Ô∏è  Update took {}ms (>25ms constraint violated)",
+                                elapsed.as_millis()
+                            );
                         }
-                        
-                        println!("‚úì Updated {} ‚Üí {} nodes ({}Œºs)", 
-                            path.display(), self.isg.node_count(), elapsed.as_micros());
+
+                        println!(
+                            "‚úì Updated {} ‚Üí {} nodes ({}Œºs)",
+                            path.display(),
+                            self.isg.node_count(),
+                            elapsed.as_micros()
+                        );
                     }
                 }
             }
@@ -252,23 +659,24 @@ impl ParseltongueAIM {
                 // Ignore other events (delete, etc.) for MVP
             }
         }
-        
+
         Ok(())
     }
 
     /// Fast file update using OptimizedISG
-    fn update_file(&mut self, path: &Path) -> Result<(), ISGError> {
-        let code = std::fs::read_to_string(path)
-            .map_err(|e| ISGError::IoError(format!("Failed to read file {}: {}", path.display(), e)))?;
-        
+    pub fn update_file(&mut self, path: &Path) -> Result<(), ISGError> {
+        let code = std::fs::read_to_string(path).map_err(|e| {
+            ISGError::IoError(format!("Failed to read file {}: {}", path.display(), e))
+        })?;
+
         let file_path = path.to_string_lossy();
-        
+
         // Remove old nodes from this file (fast with FxHashMap)
         self.remove_nodes_from_file(&file_path);
-        
+
         // Re-parse and add new nodes
         self.parse_rust_file(&file_path, &code)?;
-        
+
         Ok(())
     }
 
@@ -276,53 +684,64 @@ impl ParseltongueAIM {
     fn remove_nodes_from_file(&mut self, file_path: &str) {
         let mut state = self.isg.state.write();
         let mut nodes_to_remove = Vec::new();
-        
+
         // Find all nodes from this file
         for (hash, &node_idx) in &state.id_map {
             if let Some(node_data) = state.graph.node_weight(node_idx) {
                 if node_data.file_path.as_ref() == file_path {
-                    nodes_to_remove.push((*hash, node_idx));
+                    nodes_to_remove.push((*hash, node_idx, node_data.name.clone()));
                 }
             }
         }
-        
+
         // Remove nodes and their mappings
-        for (hash, node_idx) in nodes_to_remove {
+        for (hash, node_idx, name) in nodes_to_remove {
+            // Remove from graph
             state.graph.remove_node(node_idx);
+
+            // Remove from id_map
             state.id_map.remove(&hash);
+
+            // Remove from name_map
+            if let Some(name_set) = state.name_map.get_mut(&name) {
+                name_set.remove(&hash);
+                if name_set.is_empty() {
+                    state.name_map.remove(&name);
+                }
+            }
         }
     }
 
-    /// Find entity by name (O(n) for MVP - optimize later with name index)
+    /// Find entity by name - O(1) operation using name index
     pub fn find_entity_by_name(&self, name: &str) -> Result<SigHash, ISGError> {
-        let state = self.isg.state.read();
-        
-        for (hash, &node_idx) in &state.id_map {
-            if let Some(node_data) = state.graph.node_weight(node_idx) {
-                if node_data.name.as_ref() == name {
-                    return Ok(*hash);
-                }
-            }
+        let hashes = self.isg.find_by_name(name);
+
+        if hashes.is_empty() {
+            Err(ISGError::EntityNotFound(name.to_string()))
+        } else {
+            // Return first match (could be multiple entities with same name in different modules)
+            Ok(hashes[0])
         }
-        
-        Err(ISGError::NodeNotFound(SigHash(0)))
     }
 
     /// Get dependencies (entities this node depends on)
     pub fn get_dependencies(&self, target_hash: SigHash) -> Vec<NodeData> {
         let state = self.isg.state.read();
-        
+
         if let Some(&node_idx) = state.id_map.get(&target_hash) {
             let mut dependencies = Vec::new();
-            
+
             // Get all outgoing edges (things this node depends on)
-            for edge_ref in state.graph.edges_directed(node_idx, petgraph::Direction::Outgoing) {
+            for edge_ref in state
+                .graph
+                .edges_directed(node_idx, petgraph::Direction::Outgoing)
+            {
                 let target_idx = edge_ref.target();
                 if let Some(node_data) = state.graph.node_weight(target_idx) {
                     dependencies.push(node_data.clone());
                 }
             }
-            
+
             dependencies
         } else {
             Vec::new()
@@ -332,35 +751,190 @@ impl ParseltongueAIM {
     /// Get callers (entities that depend on this node)
     pub fn get_callers(&self, target_hash: SigHash) -> Vec<NodeData> {
         let state = self.isg.state.read();
-        
+
         if let Some(&node_idx) = state.id_map.get(&target_hash) {
             let mut callers = Vec::new();
-            
+
             // Get all incoming edges (things that depend on this node)
-            for edge_ref in state.graph.edges_directed(node_idx, petgraph::Direction::Incoming) {
+            for edge_ref in state
+                .graph
+                .edges_directed(node_idx, petgraph::Direction::Incoming)
+            {
                 let source_idx = edge_ref.source();
                 if let Some(node_data) = state.graph.node_weight(source_idx) {
                     callers.push(node_data.clone());
                 }
             }
-            
+
             callers
         } else {
             Vec::new()
         }
     }
 
+    /// Generate LLM context for entity with 1-hop dependency analysis
+    /// Target: <100ms for typical entities
+    pub fn generate_llm_context(&self, entity_name: &str) -> Result<String, ISGError> {
+        let start = std::time::Instant::now();
+
+        // Find entity by name
+        let target_hash = self.find_entity_by_name(entity_name)?;
+        let target_node = self.isg.get_node(target_hash)?;
+
+        // Get 1-hop relationships
+        let dependencies = self.get_dependencies(target_hash);
+        let callers = self.get_callers(target_hash);
+
+        // Calculate blast radius size for impact analysis
+        let blast_radius = self
+            .isg
+            .calculate_blast_radius(target_hash)
+            .map(|radius| radius.len())
+            .unwrap_or(0);
+
+        let elapsed = start.elapsed();
+
+        // Validate performance constraint (<100ms)
+        if elapsed.as_millis() > 100 {
+            eprintln!(
+                "‚ö†Ô∏è  Context generation took {}ms (>100ms constraint)",
+                elapsed.as_millis()
+            );
+        }
+
+        // Format context for LLM consumption
+        let context = format!(
+            "# Architectural Context for {}\n\n\
+            ## Entity Definition\n\
+            - **Name**: {}\n\
+            - **Type**: {:?}\n\
+            - **Location**: {}:{}\n\
+            - **Signature**: {}\n\n\
+            ## Direct Dependencies ({})\n{}\n\n\
+            ## Direct Callers ({})\n{}\n\n\
+            ## Impact Analysis\n\
+            - **Blast Radius**: {} entities would be affected by changes\n\
+            - **Architectural Role**: {}\n\n\
+            ## Key Relationships\n{}\n\n\
+            Generated in {}Œºs",
+            target_node.name,
+            target_node.name,
+            target_node.kind,
+            target_node.file_path,
+            target_node.line,
+            target_node.signature,
+            dependencies.len(),
+            if dependencies.is_empty() {
+                "- None".to_string()
+            } else {
+                dependencies
+                    .iter()
+                    .take(10) // Limit to top 10 for readability
+                    .map(|d| format!("- {} ({}): {}", d.name, d.file_path, d.signature))
+                    .collect::<Vec<_>>()
+                    .join("\n")
+            },
+            callers.len(),
+            if callers.is_empty() {
+                "- None".to_string()
+            } else {
+                callers
+                    .iter()
+                    .take(10) // Limit to top 10 for readability
+                    .map(|c| format!("- {} ({}): {}", c.name, c.file_path, c.signature))
+                    .collect::<Vec<_>>()
+                    .join("\n")
+            },
+            blast_radius,
+            self.classify_architectural_role(&target_node, dependencies.len(), callers.len()),
+            self.format_key_relationships(&target_node, &dependencies, &callers),
+            elapsed.as_micros()
+        );
+
+        Ok(context)
+    }
+
+    /// Classify the architectural role of an entity based on its relationships
+    fn classify_architectural_role(
+        &self,
+        node: &NodeData,
+        dep_count: usize,
+        caller_count: usize,
+    ) -> &'static str {
+        match node.kind {
+            NodeKind::Trait => {
+                if caller_count > 3 {
+                    "Core abstraction (widely implemented)"
+                } else {
+                    "Interface definition"
+                }
+            }
+            NodeKind::Struct => {
+                if dep_count > 5 && caller_count > 3 {
+                    "Central data structure"
+                } else if dep_count > 5 {
+                    "Complex entity (many dependencies)"
+                } else if caller_count > 3 {
+                    "Widely used data type"
+                } else {
+                    "Simple data structure"
+                }
+            }
+            NodeKind::Function => {
+                if dep_count > 5 && caller_count > 3 {
+                    "Central orchestrator"
+                } else if dep_count > 5 {
+                    "Complex operation (many dependencies)"
+                } else if caller_count > 3 {
+                    "Utility function (widely used)"
+                } else if dep_count == 0 && caller_count == 0 {
+                    "Isolated function (potential dead code)"
+                } else {
+                    "Standard function"
+                }
+            }
+        }
+    }
+
+    /// Format key relationships for LLM context
+    fn format_key_relationships(
+        &self,
+        target: &NodeData,
+        dependencies: &[NodeData],
+        callers: &[NodeData],
+    ) -> String {
+        let mut relationships = Vec::new();
+
+        // Add dependency relationships
+        for dep in dependencies.iter().take(5) {
+            relationships.push(format!("  {} USES {}", target.name, dep.name));
+        }
+
+        // Add caller relationships
+        for caller in callers.iter().take(5) {
+            relationships.push(format!("  {} CALLS {}", caller.name, target.name));
+        }
+
+        if relationships.is_empty() {
+            "- No direct relationships found".to_string()
+        } else {
+            relationships.join("\n")
+        }
+    }
+
     /// Save ISG snapshot to file (target: <500ms)
     pub fn save_snapshot(&self, path: &Path) -> Result<(), ISGError> {
         use std::time::Instant;
-        
+
         let start = Instant::now();
         let state = self.isg.state.read();
-        
+
         // Create serializable snapshot
         let snapshot = ISGSnapshot {
             nodes: state.graph.node_weights().cloned().collect(),
-            edges: state.graph.edge_references()
+            edges: state
+                .graph
+                .edge_references()
                 .map(|edge| EdgeSnapshot {
                     from: state.graph[edge.source()].hash,
                     to: state.graph[edge.target()].hash,
@@ -377,71 +951,81 @@ impl ParseltongueAIM {
                 edge_count: state.graph.edge_count(),
             },
         };
-        
+
         drop(state); // Release read lock
-        
+
         let serialized = serde_json::to_string_pretty(&snapshot)
             .map_err(|e| ISGError::IoError(format!("Serialization failed: {}", e)))?;
-        
+
         std::fs::write(path, serialized)
             .map_err(|e| ISGError::IoError(format!("Failed to write snapshot: {}", e)))?;
-        
+
         let elapsed = start.elapsed();
-        println!("‚úì Saved snapshot: {} nodes, {} edges ({}ms)", 
-            snapshot.metadata.node_count, 
+        println!(
+            "‚úì Saved snapshot: {} nodes, {} edges ({}ms)",
+            snapshot.metadata.node_count,
             snapshot.metadata.edge_count,
-            elapsed.as_millis());
-        
+            elapsed.as_millis()
+        );
+
         // Verify <500ms constraint
         if elapsed.as_millis() > 500 {
-            eprintln!("‚ö†Ô∏è  Snapshot save took {}ms (>500ms constraint)", elapsed.as_millis());
+            eprintln!(
+                "‚ö†Ô∏è  Snapshot save took {}ms (>500ms constraint)",
+                elapsed.as_millis()
+            );
         }
-        
+
         Ok(())
     }
 
     /// Load ISG snapshot from file (target: <500ms)
     pub fn load_snapshot(&mut self, path: &Path) -> Result<(), ISGError> {
         use std::time::Instant;
-        
+
         if !path.exists() {
             return Ok(()); // No snapshot to load is OK
         }
-        
+
         let start = Instant::now();
         let content = std::fs::read_to_string(path)
             .map_err(|e| ISGError::IoError(format!("Failed to read snapshot: {}", e)))?;
-        
+
         let snapshot: ISGSnapshot = serde_json::from_str(&content)
             .map_err(|e| ISGError::IoError(format!("Failed to deserialize snapshot: {}", e)))?;
-        
+
         // Rebuild ISG from snapshot
         let new_isg = OptimizedISG::new();
-        
+
         // Add all nodes
         for node in snapshot.nodes {
             new_isg.upsert_node(node);
         }
-        
+
         // Add all edges
         for edge in snapshot.edges {
             new_isg.upsert_edge(edge.from, edge.to, edge.kind)?;
         }
-        
+
         // Replace current ISG
         self.isg = new_isg;
-        
+
         let elapsed = start.elapsed();
-        println!("‚úì Loaded snapshot: {} nodes, {} edges ({}ms)", 
+        println!(
+            "‚úì Loaded snapshot: {} nodes, {} edges ({}ms)",
             snapshot.metadata.node_count,
             snapshot.metadata.edge_count,
-            elapsed.as_millis());
-        
+            elapsed.as_millis()
+        );
+
         // Verify <500ms constraint
         if elapsed.as_millis() > 500 {
-            eprintln!("‚ö†Ô∏è  Snapshot load took {}ms (>500ms constraint)", elapsed.as_millis());
+            eprintln!(
+                "‚ö†Ô∏è  Snapshot load took {}ms (>500ms constraint)",
+                elapsed.as_millis()
+            );
         }
-        
+
         Ok(())
     }
 }
@@ -471,8 +1055,8 @@ struct SnapshotMetadata {
 #[cfg(test)]
 mod tests {
     use super::*;
-    use tempfile::TempDir;
     use std::fs;
+    use tempfile::TempDir;
 
     // TDD Cycle 7: ParseltongueAIM creation (RED phase)
     #[test]
@@ -486,11 +1070,11 @@ mod tests {
     #[test]
     fn test_ingest_code_dump() {
         let mut daemon = ParseltongueAIM::new();
-        
+
         // Create test code dump with FILE: markers
         let temp_dir = TempDir::new().unwrap();
         let dump_path = temp_dir.path().join("test_dump.txt");
-        
+
         let dump_content = r#"
 FILE: src/lib.rs
 pub fn hello() -> String {
@@ -513,11 +1097,11 @@ fn main() {
 FILE: README.md
 # This is not a Rust file and should be ignored
 "#;
-        
+
         fs::write(&dump_path, dump_content).unwrap();
-        
+
         let stats = daemon.ingest_code_dump(&dump_path).unwrap();
-        
+
         // Should process 2 .rs files, ignore README.md
         assert_eq!(stats.files_processed, 2);
         assert!(stats.nodes_created > 0);
@@ -527,11 +1111,11 @@ FILE: README.md
     #[test]
     fn test_code_dump_performance() {
         let mut daemon = ParseltongueAIM::new();
-        
+
         // Create a larger test dump (simulating 2.1MB)
         let temp_dir = TempDir::new().unwrap();
         let dump_path = temp_dir.path().join("large_dump.txt");
-        
+
         let mut large_content = String::new();
         for i in 0..1000 {
             large_content.push_str(&format!(
@@ -542,22 +1126,26 @@ FILE: README.md
                 i, i, i, i, i
             ));
         }
-        
+
         fs::write(&dump_path, large_content).unwrap();
-        
+
         let start = Instant::now();
         let _stats = daemon.ingest_code_dump(&dump_path).unwrap();
         let elapsed = start.elapsed();
-        
+
         // Should complete in <5 seconds
-        assert!(elapsed.as_secs() < 5, "Code dump ingestion took {}s (>5s)", elapsed.as_secs());
+        assert!(
+            elapsed.as_secs() < 5,
+            "Code dump ingestion took {}s (>5s)",
+            elapsed.as_secs()
+        );
     }
 
     // TDD Cycle 9: Rust file parsing (RED phase)
     #[test]
     fn test_parse_rust_file_basic() {
         let mut daemon = ParseltongueAIM::new();
-        
+
         let rust_code = r#"
             pub fn test_function() -> Result<(), Error> {
                 Ok(())
@@ -571,12 +1159,12 @@ FILE: README.md
                 fn test_method(&self) -> i32;
             }
         "#;
-        
+
         daemon.parse_rust_file("test.rs", rust_code).unwrap();
-        
+
         // Should create 3 nodes: function, struct, trait
         assert_eq!(daemon.isg.node_count(), 3);
-        
+
         // Verify we can find the created entities
         assert!(daemon.find_entity_by_name("test_function").is_ok());
         assert!(daemon.find_entity_by_name("TestStruct").is_ok());
@@ -586,14 +1174,14 @@ FILE: README.md
     #[test]
     fn test_syn_error_handling() {
         let mut daemon = ParseltongueAIM::new();
-        
+
         let malformed_rust = "pub fn incomplete_function(";
-        
+
         let result = daemon.parse_rust_file("bad.rs", malformed_rust);
-        
+
         // Should succeed (graceful error handling) but log the error
         assert!(result.is_ok(), "Should handle parse errors gracefully");
-        
+
         // Should not have created any nodes due to parse error
         assert_eq!(daemon.isg.node_count(), 0);
     }
@@ -603,16 +1191,16 @@ FILE: README.md
     fn test_file_monitoring_basic() {
         let mut daemon = ParseltongueAIM::new();
         let temp_dir = TempDir::new().unwrap();
-        
+
         // Test that daemon can be created and file watcher can be initialized
         // For the test, we'll just verify the daemon doesn't crash on startup
-        
+
         // Signal shutdown immediately so the daemon doesn't run indefinitely
         daemon.shutdown();
-        
+
         // This should now succeed (GREEN phase)
         let result = daemon.start_daemon(temp_dir.path());
-        
+
         // Should complete successfully
         assert!(result.is_ok());
     }
@@ -622,21 +1210,27 @@ FILE: README.md
         let mut daemon = ParseltongueAIM::new();
         let temp_dir = TempDir::new().unwrap();
         let test_file = temp_dir.path().join("test.rs");
-        
+
         // Create initial file
         fs::write(&test_file, "pub fn initial() {}").unwrap();
-        daemon.parse_rust_file("test.rs", "pub fn initial() {}").unwrap();
-        
+        daemon
+            .parse_rust_file("test.rs", "pub fn initial() {}")
+            .unwrap();
+
         // Update file and measure performance
         fs::write(&test_file, "pub fn updated() {}").unwrap();
-        
+
         let start = Instant::now();
         let result = daemon.update_file(&test_file);
         let elapsed = start.elapsed();
-        
+
         // Should complete in <12ms (this will fail in RED phase)
         if result.is_ok() {
-            assert!(elapsed.as_millis() < 12, "File update took {}ms (>12ms)", elapsed.as_millis());
+            assert!(
+                elapsed.as_millis() < 12,
+                "File update took {}ms (>12ms)",
+                elapsed.as_millis()
+            );
         }
     }
 
@@ -644,21 +1238,21 @@ FILE: README.md
     #[test]
     fn test_find_entity_by_name() {
         let mut daemon = ParseltongueAIM::new();
-        
+
         // Add some test entities
         let rust_code = r#"
             pub fn target_function() -> i32 { 42 }
             pub struct TargetStruct { field: i32 }
         "#;
-        
+
         daemon.parse_rust_file("test.rs", rust_code).unwrap();
-        
+
         // Should find entities by name
         let func_hash = daemon.find_entity_by_name("target_function").unwrap();
         let struct_hash = daemon.find_entity_by_name("TargetStruct").unwrap();
-        
+
         assert_ne!(func_hash, struct_hash);
-        
+
         // Should return error for non-existent entity
         assert!(daemon.find_entity_by_name("NonExistent").is_err());
     }
@@ -666,7 +1260,7 @@ FILE: README.md
     #[test]
     fn test_get_dependencies_and_callers() {
         let mut daemon = ParseltongueAIM::new();
-        
+
         // Create a trait implementation relationship (which is already supported)
         let rust_code = r#"
             pub trait TestTrait {
@@ -683,19 +1277,25 @@ FILE: README.md
                 }
             }
         "#;
-        
+
         daemon.parse_rust_file("test.rs", rust_code).unwrap();
-        
+
         let struct_hash = daemon.find_entity_by_name("TestStruct").unwrap();
         let trait_hash = daemon.find_entity_by_name("TestTrait").unwrap();
-        
+
         // TestStruct should implement TestTrait (dependency)
         let dependencies = daemon.get_dependencies(struct_hash);
-        assert!(!dependencies.is_empty(), "TestStruct should have TestTrait as dependency");
-        
+        assert!(
+            !dependencies.is_empty(),
+            "TestStruct should have TestTrait as dependency"
+        );
+
         // TestTrait should be implemented by TestStruct (caller/implementor)
         let callers = daemon.get_callers(trait_hash);
-        assert!(!callers.is_empty(), "TestTrait should have TestStruct as implementor");
+        assert!(
+            !callers.is_empty(),
+            "TestTrait should have TestStruct as implementor"
+        );
     }
 
     // TDD Cycle 12: Persistence (RED phase)
@@ -704,16 +1304,22 @@ FILE: README.md
         let mut daemon = ParseltongueAIM::new();
         let temp_dir = TempDir::new().unwrap();
         let snapshot_path = temp_dir.path().join("snapshot.json");
-        
+
         // Add some data
-        daemon.parse_rust_file("test.rs", "pub fn test() {}").unwrap();
-        
+        daemon
+            .parse_rust_file("test.rs", "pub fn test() {}")
+            .unwrap();
+
         let start = Instant::now();
         let result = daemon.save_snapshot(&snapshot_path);
         let elapsed = start.elapsed();
-        
+
         if result.is_ok() {
-            assert!(elapsed.as_millis() < 500, "Snapshot save took {}ms (>500ms)", elapsed.as_millis());
+            assert!(
+                elapsed.as_millis() < 500,
+                "Snapshot save took {}ms (>500ms)",
+                elapsed.as_millis()
+            );
             assert!(snapshot_path.exists());
         }
     }
@@ -723,34 +1329,34 @@ FILE: README.md
         let mut daemon = ParseltongueAIM::new();
         let temp_dir = TempDir::new().unwrap();
         let snapshot_path = temp_dir.path().join("snapshot.json");
-        
+
         // Should handle missing file gracefully
         let result = daemon.load_snapshot(&snapshot_path);
         assert!(result.is_ok()); // Missing file is OK
-        
+
         // Test round-trip: save and load
         let rust_code = r#"
             pub fn test_function() -> i32 { 42 }
             pub struct TestStruct { field: i32 }
             pub trait TestTrait { fn method(&self); }
         "#;
-        
+
         daemon.parse_rust_file("test.rs", rust_code).unwrap();
         let original_node_count = daemon.isg.node_count();
-        
+
         // Save snapshot
         daemon.save_snapshot(&snapshot_path).unwrap();
         assert!(snapshot_path.exists());
-        
+
         // Create new daemon and load snapshot
         let mut new_daemon = ParseltongueAIM::new();
         assert_eq!(new_daemon.isg.node_count(), 0); // Should be empty initially
-        
+
         new_daemon.load_snapshot(&snapshot_path).unwrap();
-        
+
         // Should have same number of nodes
         assert_eq!(new_daemon.isg.node_count(), original_node_count);
-        
+
         // Should be able to find the same entities
         assert!(new_daemon.find_entity_by_name("test_function").is_ok());
         assert!(new_daemon.find_entity_by_name("TestStruct").is_ok());
@@ -760,30 +1366,681 @@ FILE: README.md
     #[test]
     fn test_daemon_shutdown_graceful() {
         let daemon = ParseltongueAIM::new();
-        
+
         // Should be able to create and drop without issues
         drop(daemon);
-        
+
         // This test validates RAII cleanup
         assert!(true, "Daemon shutdown completed without panic");
     }
 
+    // TDD Cycle: CALLS relationship extraction (STUB ‚Üí RED phase)
+    #[test]
+    fn test_function_call_detection() {
+        let mut daemon = ParseltongueAIM::new();
+
+        let rust_code = r#"
+            pub fn caller_function() -> i32 {
+                let result = target_function();
+                another_function(result)
+            }
+            
+            pub fn target_function() -> i32 {
+                42
+            }
+            
+            pub fn another_function(x: i32) -> i32 {
+                x * 2
+            }
+        "#;
+
+        daemon.parse_rust_file("test.rs", rust_code).unwrap();
+
+        // Should create 3 function nodes
+        assert_eq!(daemon.isg.node_count(), 3);
+
+        // Should create CALLS edges: caller_function -> target_function, caller_function -> another_function
+        let caller_hash = daemon.find_entity_by_name("caller_function").unwrap();
+        let _target_hash = daemon.find_entity_by_name("target_function").unwrap();
+        let _another_hash = daemon.find_entity_by_name("another_function").unwrap();
+
+        // Get dependencies (outgoing CALLS edges)
+        let dependencies = daemon.get_dependencies(caller_hash);
+        let dep_names: Vec<String> = dependencies.iter().map(|n| n.name.to_string()).collect();
+
+        // Should find both called functions as dependencies
+        assert!(
+            dep_names.contains(&"target_function".to_string()),
+            "caller_function should call target_function, found: {:?}",
+            dep_names
+        );
+        assert!(
+            dep_names.contains(&"another_function".to_string()),
+            "caller_function should call another_function, found: {:?}",
+            dep_names
+        );
+
+        // Verify edge count (should have 2 CALLS edges)
+        assert!(
+            daemon.isg.edge_count() >= 2,
+            "Should have at least 2 CALLS edges, found: {}",
+            daemon.isg.edge_count()
+        );
+    }
+
+    #[test]
+    fn test_method_call_detection() {
+        let mut daemon = ParseltongueAIM::new();
+
+        let rust_code = r#"
+            pub struct TestStruct {
+                value: i32,
+            }
+            
+            impl TestStruct {
+                pub fn method_call(&self) -> i32 {
+                    self.value
+                }
+            }
+            
+            pub fn caller_function() -> i32 {
+                let obj = TestStruct { value: 42 };
+                obj.method_call()
+            }
+        "#;
+
+        daemon.parse_rust_file("test.rs", rust_code).unwrap();
+
+        // Should detect method call: caller_function -> method_call
+        let caller_hash = daemon.find_entity_by_name("caller_function").unwrap();
+        let dependencies = daemon.get_dependencies(caller_hash);
+
+        // Should find method_call as dependency
+        let dep_names: Vec<String> = dependencies.iter().map(|n| n.name.to_string()).collect();
+        assert!(
+            dep_names.contains(&"method_call".to_string()),
+            "caller_function should call method_call, found: {:?}",
+            dep_names
+        );
+    }
+
+    // TDD Cycle: USES relationship extraction (STUB ‚Üí RED phase)
+    #[test]
+    fn test_type_usage_detection_in_signatures() {
+        let mut daemon = ParseltongueAIM::new();
+
+        let rust_code = r#"
+            pub struct User {
+                name: String,
+            }
+            
+            pub struct Config {
+                debug: bool,
+            }
+            
+            pub fn process_user(user: User, config: Config) -> User {
+                user
+            }
+        "#;
+
+        daemon.parse_rust_file("test.rs", rust_code).unwrap();
+
+        // Should create 3 nodes: 2 structs + 1 function
+        assert_eq!(daemon.isg.node_count(), 3);
+
+        // Should create USES edges: process_user -> User, process_user -> Config
+        let func_hash = daemon.find_entity_by_name("process_user").unwrap();
+        let dependencies = daemon.get_dependencies(func_hash);
+        let dep_names: Vec<String> = dependencies.iter().map(|n| n.name.to_string()).collect();
+
+        // Should find both types as dependencies
+        assert!(
+            dep_names.contains(&"User".to_string()),
+            "process_user should use User type, found: {:?}",
+            dep_names
+        );
+        assert!(
+            dep_names.contains(&"Config".to_string()),
+            "process_user should use Config type, found: {:?}",
+            dep_names
+        );
+
+        // Should have USES edges (at least 2)
+        assert!(
+            daemon.isg.edge_count() >= 2,
+            "Should have at least 2 USES edges, found: {}",
+            daemon.isg.edge_count()
+        );
+    }
+
+    #[test]
+    fn test_type_usage_detection_in_bodies() {
+        let mut daemon = ParseltongueAIM::new();
+
+        let rust_code = r#"
+            pub struct User {
+                name: String,
+            }
+            
+            pub struct Database {
+                connection: String,
+            }
+            
+            pub fn create_user() -> User {
+                let db = Database { connection: "localhost".to_string() };
+                User { name: "test".to_string() }
+            }
+        "#;
+
+        daemon.parse_rust_file("test.rs", rust_code).unwrap();
+
+        // Should create USES edges: create_user -> User, create_user -> Database
+        let func_hash = daemon.find_entity_by_name("create_user").unwrap();
+        let dependencies = daemon.get_dependencies(func_hash);
+        let dep_names: Vec<String> = dependencies.iter().map(|n| n.name.to_string()).collect();
+
+        // Should find both types used in function body
+        assert!(
+            dep_names.contains(&"User".to_string()),
+            "create_user should use User type, found: {:?}",
+            dep_names
+        );
+        assert!(
+            dep_names.contains(&"Database".to_string()),
+            "create_user should use Database type, found: {:?}",
+            dep_names
+        );
+    }
+
+    #[test]
+    fn test_generic_type_usage_detection() {
+        let mut daemon = ParseltongueAIM::new();
+
+        let rust_code = r#"
+            pub struct Container<T> {
+                value: T,
+            }
+            
+            pub struct User {
+                name: String,
+            }
+            
+            pub fn process_container(container: Container<User>) -> User {
+                container.value
+            }
+        "#;
+
+        daemon.parse_rust_file("test.rs", rust_code).unwrap();
+
+        // Should detect usage of both Container and User types
+        let func_hash = daemon.find_entity_by_name("process_container").unwrap();
+        let dependencies = daemon.get_dependencies(func_hash);
+        let dep_names: Vec<String> = dependencies.iter().map(|n| n.name.to_string()).collect();
+
+        // Should find both generic container and inner type
+        assert!(
+            dep_names.contains(&"Container".to_string()),
+            "process_container should use Container type, found: {:?}",
+            dep_names
+        );
+        assert!(
+            dep_names.contains(&"User".to_string()),
+            "process_container should use User type, found: {:?}",
+            dep_names
+        );
+    }
+
+    // TDD Cycle: Module-aware FQN generation (STUB ‚Üí RED phase)
+    #[test]
+    fn test_module_aware_fqn_generation() {
+        let mut daemon = ParseltongueAIM::new();
+
+        let rust_code = r#"
+            pub mod utils {
+                pub struct Config {
+                    debug: bool,
+                }
+                
+                pub fn load_config() -> Config {
+                    Config { debug: true }
+                }
+            }
+            
+            pub mod database {
+                pub struct Connection {
+                    url: String,
+                }
+                
+                pub fn connect() -> Connection {
+                    Connection { url: "localhost".to_string() }
+                }
+            }
+            
+            pub fn main() {
+                let config = utils::load_config();
+                let conn = database::connect();
+            }
+        "#;
+
+        daemon.parse_rust_file("src/lib.rs", rust_code).unwrap();
+
+        // Should create nodes with fully qualified names
+        let config_struct = daemon.find_entity_by_name("Config");
+        let connection_struct = daemon.find_entity_by_name("Connection");
+
+        // Should be able to distinguish between entities in different modules
+        assert!(config_struct.is_ok(), "Should find Config struct");
+        assert!(connection_struct.is_ok(), "Should find Connection struct");
+
+        // Should create CALLS relationships with proper FQN resolution
+        let main_hash = daemon.find_entity_by_name("main").unwrap();
+        let dependencies = daemon.get_dependencies(main_hash);
+        let dep_names: Vec<String> = dependencies.iter().map(|n| n.name.to_string()).collect();
+
+        // Should find both module functions as dependencies
+        assert!(
+            dep_names.contains(&"load_config".to_string()),
+            "main should call utils::load_config, found: {:?}",
+            dep_names
+        );
+        assert!(
+            dep_names.contains(&"connect".to_string()),
+            "main should call database::connect, found: {:?}",
+            dep_names
+        );
+    }
+
+    #[test]
+    fn test_nested_module_fqn_generation() {
+        let mut daemon = ParseltongueAIM::new();
+
+        let rust_code = r#"
+            pub mod outer {
+                pub mod inner {
+                    pub struct DeepStruct {
+                        value: i32,
+                    }
+                    
+                    pub fn deep_function() -> DeepStruct {
+                        DeepStruct { value: 42 }
+                    }
+                }
+                
+                pub fn outer_function() -> inner::DeepStruct {
+                    inner::deep_function()
+                }
+            }
+        "#;
+
+        daemon.parse_rust_file("src/lib.rs", rust_code).unwrap();
+
+        // Should handle nested module paths correctly
+        let outer_func_hash = daemon.find_entity_by_name("outer_function").unwrap();
+        let dependencies = daemon.get_dependencies(outer_func_hash);
+        let dep_names: Vec<String> = dependencies.iter().map(|n| n.name.to_string()).collect();
+
+        // Should find both the function call and type usage
+        assert!(
+            dep_names.contains(&"deep_function".to_string()),
+            "outer_function should call inner::deep_function, found: {:?}",
+            dep_names
+        );
+        assert!(
+            dep_names.contains(&"DeepStruct".to_string()),
+            "outer_function should use inner::DeepStruct, found: {:?}",
+            dep_names
+        );
+    }
+
+    #[test]
+    fn test_cross_module_reference_resolution() {
+        let mut daemon = ParseltongueAIM::new();
+
+        let rust_code = r#"
+            pub mod models {
+                pub struct User {
+                    name: String,
+                }
+            }
+            
+            pub mod services {
+                use super::models::User;
+                
+                pub fn create_user(name: String) -> User {
+                    User { name }
+                }
+            }
+        "#;
+
+        daemon.parse_rust_file("src/lib.rs", rust_code).unwrap();
+
+        // Should resolve cross-module references correctly
+        let create_user_hash = daemon.find_entity_by_name("create_user").unwrap();
+        let dependencies = daemon.get_dependencies(create_user_hash);
+        let dep_names: Vec<String> = dependencies.iter().map(|n| n.name.to_string()).collect();
+
+        // Should find User type despite being in different module
+        assert!(
+            dep_names.contains(&"User".to_string()),
+            "create_user should use models::User, found: {:?}",
+            dep_names
+        );
+    }
+
+    // TDD Cycle: Comprehensive relationship accuracy validation (STUB ‚Üí RED phase)
+    #[test]
+    fn test_complex_trait_object_relationships() {
+        let mut daemon = ParseltongueAIM::new();
+
+        let rust_code = r#"
+            pub trait Handler {
+                fn handle(&self, data: &str) -> Result<(), String>;
+            }
+            
+            pub struct Logger;
+            
+            impl Handler for Logger {
+                fn handle(&self, data: &str) -> Result<(), String> {
+                    println!("{}", data);
+                    Ok(())
+                }
+            }
+            
+            pub fn process_with_handler(handler: Box<dyn Handler>, data: String) -> Result<(), String> {
+                handler.handle(&data)
+            }
+        "#;
+
+        daemon.parse_rust_file("test.rs", rust_code).unwrap();
+
+        // Should detect trait object usage and implementation relationships
+        let process_hash = daemon.find_entity_by_name("process_with_handler").unwrap();
+        let _logger_hash = daemon.find_entity_by_name("Logger").unwrap();
+        let handler_hash = daemon.find_entity_by_name("Handler").unwrap();
+
+        // Should find Handler trait usage in function signature
+        let process_deps = daemon.get_dependencies(process_hash);
+        let process_dep_names: Vec<String> =
+            process_deps.iter().map(|n| n.name.to_string()).collect();
+        assert!(
+            process_dep_names.contains(&"Handler".to_string()),
+            "process_with_handler should use Handler trait, found: {:?}",
+            process_dep_names
+        );
+
+        // Should find Logger implements Handler
+        let handler_callers = daemon.get_callers(handler_hash);
+        let handler_caller_names: Vec<String> =
+            handler_callers.iter().map(|n| n.name.to_string()).collect();
+        assert!(
+            handler_caller_names.contains(&"Logger".to_string()),
+            "Logger should implement Handler, found: {:?}",
+            handler_caller_names
+        );
+    }
+
+    #[test]
+    fn test_method_chain_call_detection() {
+        let mut daemon = ParseltongueAIM::new();
+
+        let rust_code = r#"
+            pub struct Builder {
+                value: String,
+            }
+            
+            impl Builder {
+                pub fn new() -> Self {
+                    Builder { value: String::new() }
+                }
+                
+                pub fn add(&mut self, text: &str) -> &mut Self {
+                    self.value.push_str(text);
+                    self
+                }
+                
+                pub fn build(self) -> String {
+                    self.value
+                }
+            }
+            
+            pub fn create_message() -> String {
+                Builder::new()
+                    .add("Hello")
+                    .add(" ")
+                    .add("World")
+                    .build()
+            }
+        "#;
+
+        daemon.parse_rust_file("test.rs", rust_code).unwrap();
+
+        // Should detect method chain calls
+        let create_msg_hash = daemon.find_entity_by_name("create_message").unwrap();
+        let dependencies = daemon.get_dependencies(create_msg_hash);
+        let dep_names: Vec<String> = dependencies.iter().map(|n| n.name.to_string()).collect();
+
+        // Should find all method calls in the chain
+        assert!(
+            dep_names.contains(&"new".to_string()),
+            "create_message should call Builder::new, found: {:?}",
+            dep_names
+        );
+        assert!(
+            dep_names.contains(&"add".to_string()),
+            "create_message should call add methods, found: {:?}",
+            dep_names
+        );
+        assert!(
+            dep_names.contains(&"build".to_string()),
+            "create_message should call build method, found: {:?}",
+            dep_names
+        );
+    }
+
+    #[test]
+    fn test_generic_function_relationships() {
+        let mut daemon = ParseltongueAIM::new();
+
+        let rust_code = r#"
+            pub struct Container<T> {
+                items: Vec<T>,
+            }
+            
+            impl<T> Container<T> {
+                pub fn new() -> Self {
+                    Container { items: Vec::new() }
+                }
+                
+                pub fn add(&mut self, item: T) {
+                    self.items.push(item);
+                }
+                
+                pub fn get(&self, index: usize) -> Option<&T> {
+                    self.items.get(index)
+                }
+            }
+            
+            pub fn process_strings() -> Option<String> {
+                let mut container = Container::<String>::new();
+                container.add("test".to_string());
+                container.get(0).cloned()
+            }
+        "#;
+
+        daemon.parse_rust_file("test.rs", rust_code).unwrap();
+
+        // Should detect generic type usage and method calls
+        let process_hash = daemon.find_entity_by_name("process_strings").unwrap();
+        let dependencies = daemon.get_dependencies(process_hash);
+        let dep_names: Vec<String> = dependencies.iter().map(|n| n.name.to_string()).collect();
+
+        // Should find Container type usage and method calls
+        assert!(
+            dep_names.contains(&"Container".to_string()),
+            "process_strings should use Container type, found: {:?}",
+            dep_names
+        );
+        assert!(
+            dep_names.contains(&"new".to_string()),
+            "process_strings should call new method, found: {:?}",
+            dep_names
+        );
+        assert!(
+            dep_names.contains(&"add".to_string()),
+            "process_strings should call add method, found: {:?}",
+            dep_names
+        );
+        assert!(
+            dep_names.contains(&"get".to_string()),
+            "process_strings should call get method, found: {:?}",
+            dep_names
+        );
+    }
+
+    #[test]
+    fn test_relationship_extraction_accuracy_benchmark() {
+        let mut daemon = ParseltongueAIM::new();
+
+        // Complex real-world-like code with multiple relationship types
+        let rust_code = r#"
+            pub mod database {
+                pub trait Connection {
+                    fn execute(&self, query: &str) -> Result<Vec<String>, String>;
+                }
+                
+                pub struct PostgresConnection {
+                    url: String,
+                }
+                
+                impl Connection for PostgresConnection {
+                    fn execute(&self, query: &str) -> Result<Vec<String>, String> {
+                        // Mock implementation
+                        Ok(vec![query.to_string()])
+                    }
+                }
+            }
+            
+            pub mod models {
+                pub struct User {
+                    pub id: u64,
+                    pub name: String,
+                }
+                
+                impl User {
+                    pub fn new(id: u64, name: String) -> Self {
+                        User { id, name }
+                    }
+                }
+            }
+            
+            pub mod services {
+                use super::database::Connection;
+                use super::models::User;
+                
+                pub struct UserService<C: Connection> {
+                    connection: C,
+                }
+                
+                impl<C: Connection> UserService<C> {
+                    pub fn new(connection: C) -> Self {
+                        UserService { connection }
+                    }
+                    
+                    pub fn create_user(&self, name: String) -> Result<User, String> {
+                        let query = format!("INSERT INTO users (name) VALUES ('{}')", name);
+                        self.connection.execute(&query)?;
+                        Ok(User::new(1, name))
+                    }
+                    
+                    pub fn find_user(&self, id: u64) -> Result<Option<User>, String> {
+                        let query = format!("SELECT * FROM users WHERE id = {}", id);
+                        let results = self.connection.execute(&query)?;
+                        if results.is_empty() {
+                            Ok(None)
+                        } else {
+                            Ok(Some(User::new(id, "test".to_string())))
+                        }
+                    }
+                }
+            }
+        "#;
+
+        daemon.parse_rust_file("src/lib.rs", rust_code).unwrap();
+
+        // Validate comprehensive relationship extraction
+        let total_nodes = daemon.isg.node_count();
+        let total_edges = daemon.isg.edge_count();
+
+        // Should have created multiple nodes and relationships
+        assert!(
+            total_nodes >= 8,
+            "Should have at least 8 nodes (traits, structs, functions), found: {}",
+            total_nodes
+        );
+        assert!(
+            total_edges >= 3,
+            "Should have at least 3 relationships, found: {}",
+            total_edges
+        );
+
+        // Validate specific relationships exist
+        let user_service_hash = daemon.find_entity_by_name("UserService").unwrap();
+        let create_user_hash = daemon.find_entity_by_name("create_user").unwrap();
+
+        // UserService should use Connection trait
+        let user_service_deps = daemon.get_dependencies(user_service_hash);
+        let user_service_dep_names: Vec<String> = user_service_deps
+            .iter()
+            .map(|n| n.name.to_string())
+            .collect();
+
+        // create_user should use User type and call User::new
+        let create_user_deps = daemon.get_dependencies(create_user_hash);
+        let create_user_dep_names: Vec<String> = create_user_deps
+            .iter()
+            .map(|n| n.name.to_string())
+            .collect();
+
+        // Log relationship extraction results for manual validation
+        println!("=== Relationship Extraction Accuracy Benchmark ===");
+        println!("Total nodes: {}", total_nodes);
+        println!("Total edges: {}", total_edges);
+        println!("UserService dependencies: {:?}", user_service_dep_names);
+        println!("create_user dependencies: {:?}", create_user_dep_names);
+
+        // For MVP, we consider this successful if we have reasonable relationship counts
+        // In a full implementation, we'd compare against manually verified ground truth
+        let accuracy_estimate = (total_edges as f64 / (total_nodes as f64 * 2.0)) * 100.0;
+        println!("Estimated relationship density: {:.1}%", accuracy_estimate);
+
+        // Basic sanity checks for relationship extraction
+        assert!(
+            accuracy_estimate > 10.0,
+            "Relationship extraction density too low: {:.1}%",
+            accuracy_estimate
+        );
+    }
+
     // TDD Cycle 13: Incremental updates (RED phase)
     #[test]
     fn test_update_file_incremental() {
         let mut daemon = ParseltongueAIM::new();
-        
+
         // Initial state
-        daemon.parse_rust_file("test.rs", "pub fn old_function() {}").unwrap();
+        daemon
+            .parse_rust_file("test.rs", "pub fn old_function() {}")
+            .unwrap();
         assert_eq!(daemon.isg.node_count(), 1);
-        
+
         // Update file (remove old, add new)
         daemon.remove_nodes_from_file("test.rs");
-        daemon.parse_rust_file("test.rs", "pub fn new_function() {}").unwrap();
-        
+        daemon
+            .parse_rust_file("test.rs", "pub fn new_function() {}")
+            .unwrap();
+
         // Should still have 1 node, but different function
         assert_eq!(daemon.isg.node_count(), 1);
         assert!(daemon.find_entity_by_name("new_function").is_ok());
         assert!(daemon.find_entity_by_name("old_function").is_err());
     }
-}
\ No newline at end of file
+}

### README.md Diff
diff --git a/README.md b/README.md
index 1dce23b..545ff8b 100644
--- a/README.md
+++ b/README.md
@@ -1,202 +1,799 @@
-# Parseltongue AIM Daemon
+# Parseltongue üêç 
+## Understand Any Rust Codebase in 30 Seconds
 
-**Rust-only architectural intelligence daemon** providing deterministic, graph-based code analysis with sub-millisecond query performance.
+[![CI](https://github.com/your-org/parseltongue/workflows/CI/badge.svg)](https://github.com/your-org/parseltongue/actions)
+[![Build Status](https://github.com/your-org/parseltongue/workflows/CI/badge.svg?branch=main)](https://github.com/your-org/parseltongue/actions)
 
-## üöÄ Features
+**Stop spending 30 minutes figuring out where things are.** Get instant architectural intelligence for any Rust project.
 
-- **Real-time File Monitoring**: Watch Rust codebases with <12ms update latency
-- **Code Dump Analysis**: Process large code dumps in <5 seconds
-- **Graph-based Queries**: Sub-millisecond architectural queries
-- **LLM Integration**: Generate structured context for AI code assistance
-- **High Performance**: 6Œºs node operations, concurrent-safe architecture
-- **Production Ready**: Comprehensive error handling and crash recovery
+```bash
+# Download binary (4.3MB) ‚Üí Run immediately ‚Üí No installation
+./parseltongue onboard .
+# ‚úÖ Complete codebase map in <15 minutes
+# ‚úÖ Find any entity in <100ms  
+# ‚úÖ Understand blast radius before changes
+```
 
-## üì¶ Installation
+## üéØ **What Developers Actually Want**
 
+### **Copy-Paste Ready**
 ```bash
+# Essential commands (copy these)
+./parseltongue onboard .                    # Map the codebase
+./parseltongue feature-start UserService    # Impact analysis  
+./parseltongue where-defined MyStruct       # Find definitions
+./parseltongue debug handle_request         # Trace usage
+./parseltongue refactor-check auth_module   # Safety check
+```
+
+### **Kiro Steering Integration**
+```bash
+# Copy this file to your .kiro/steering/ folder
+cp distribution/copy-paste-ready/kiro-steering-complete.md .kiro/steering/parseltongue.md
+```
+
+### **LLM Integration**
+```bash
+# Generate rich context for AI tools
+./parseltongue generate-context MyStruct --format markdown
+# Perfect for Claude, ChatGPT, Cursor, GitHub Copilot
+```
+
+## üì¶ **Get Started (30 seconds)**
+
+1. **Download**: `distribution/binaries/parseltongue` (4.3MB)
+2. **Run**: `./parseltongue onboard .`
+3. **Integrate**: Copy templates from `distribution/copy-paste-ready/`
+
+**That's it.** No installation, no dependencies, works immediately.
+
+## üîß **Development & Contributing**
+
+- **[Build Instructions](BUILD.md)** - Complete setup and development guide
+- **[Changelog](CHANGELOG.md)** - Version history and milestones
+- **[CI Status](https://github.com/your-org/parseltongue/actions)** - Automated quality checks
+
+### Quick Development Setup
+```bash
+# Clone and build
+git clone <repository-url>
+cd parseltongue
+cargo build --all-features
+
+# Run quality checks (same as CI)
+./scripts/ci-check.sh
+```
+
+---
+
+## üéØ **Developer Onboarding (Built & Ready)**
+
+### Simple 3-Step Onboarding Process
+
+```mermaid
+%%{init: {'theme':'base', 'themeVariables': {'primaryColor':'#e8f5e8', 'primaryTextColor':'#2e7d32', 'lineColor':'#4caf50', 'fontFamily':'Arial', 'fontSize':'14px'}, 'flowchart': {'nodeSpacing': 75, 'rankSpacing': 75, 'wrappingWidth': 150}}}%%
+flowchart TD
+    %% Step 1: Build
+    subgraph "üî® Step 1: Build Parseltongue (2 minutes)"
+        direction TB
+        A1["üì• Clone Repository<br/><i>git clone &lt;repo&gt;</i>"]
+        A1 --> A2["‚öôÔ∏è Build Release Binary<br/><i>cargo build --release</i>"]
+        A2 --> A3["‚úÖ Binary Ready<br/><i>./target/release/parseltongue_TIMESTAMP</i>"]
+    end
+    
+    %% Step 2: Onboard
+    subgraph "üöÄ Step 2: Onboard Any Codebase (&lt;15 minutes)"
+        direction TB
+        B1["üéØ Run Onboarding Script<br/><i>./onboard_codebase.sh /path/to/code</i>"]
+        B1 --> B2["üìä Entity Discovery<br/><i>Functions, structs, traits</i>"]
+        B2 --> B3["üèóÔ∏è Architecture Overview<br/><i>Entry points & patterns</i>"]
+        B3 --> B4["üìã Ready-to-Use Results<br/><i>./parseltongue_workspace/</i>"]
+    end
+    
+    %% Step 3: Integrate
+    subgraph "üîß Step 3: Integrate Workflows (Copy-Paste)"
+        direction TB
+        C1["üìÅ Copy Essential Scripts<br/><i>parseltongue_dungeon/scripts/</i>"]
+        C1 --> C2["ü§ñ Copy LLM Templates<br/><i>Zero-hallucination context</i>"]
+        C2 --> C3["‚ö° Ready for Development<br/><i>Complete workflow toolkit</i>"]
+    end
+    
+    %% Flow connections
+    A3 --> B1
+    B4 --> C1
+    
+    %% Performance indicators
+    subgraph "‚è±Ô∏è Validated Performance"
+        direction LR
+        P1["Build: 2min ‚úÖ"]
+        P2["Onboard: &lt;15min ‚úÖ"]
+        P3["Integration: 30sec ‚úÖ"]
+    end
+    
+    A3 -.-> P1
+    B4 -.-> P2
+    C3 -.-> P3
+    
+    %% Success outcomes
+    subgraph "üéâ What You Get"
+        direction TB
+        S1["üîç Complete Entity Visibility<br/><i>No more guessing names</i>"]
+        S2["‚ö†Ô∏è Risk-Quantified Analysis<br/><i>Low/Medium/High/Critical</i>"]
+        S3["ü§ñ LLM-Ready Context<br/><i>Zero hallucinations</i>"]
+        S4["üìà 10x Faster Workflows<br/><i>Onboard‚ÜíFeature‚ÜíDebug‚ÜíRefactor</i>"]
+    end
+    
+    C3 --> S1
+    C3 --> S2
+    C3 --> S3
+    C3 --> S4
+    
+    %% Styling
+    classDef build fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#0d47a1
+    classDef onboard fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#1b5e20
+    classDef integrate fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#e65100
+    classDef performance fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c
+    classDef success fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#880e4f
+    
+    class A1,A2,A3 build
+    class B1,B2,B3,B4 onboard
+    class C1,C2,C3 integrate
+    class P1,P2,P3 performance
+    class S1,S2,S3,S4 success
+```
+
+### Copy-Paste Commands
+
+```bash
+# 1. Build & Package Complete Distribution (automated)
 git clone <repository>
 cd parseltongue
+./scripts/package_distribution.sh
+echo "‚úÖ Complete distribution ready: ./distribution/"
+
+# 2. Onboard to any codebase (< 15 minutes)
+./distribution/copy-paste-ready/pt onboard /path/to/your/codebase
+# Generates: architecture overview, entity listings, entry points
+# Output: ./parseltongue_workspace/onboarding_TIMESTAMP/
+
+# 3. Copy complete toolkit to your project
+cp distribution/binaries/parseltongue your_project/
+cp -r distribution/copy-paste-ready/* your_project/
+cd your_project
+./pt onboard .
+echo "‚úÖ Ready for all development workflows"
+```
+
+### Alternative: Manual Build
+```bash
+# If you prefer manual build
 cargo build --release
+# Binary: ./target/release/parseltongue_TIMESTAMP
 ```
 
-## üéØ Quick Start
+### What the Onboarding Script Provides
 
-### Analyze a Code Dump
-```bash
-# Create a code dump with FILE: markers
-echo 'FILE: src/lib.rs
-pub trait Greeter {
-    fn greet(&self) -> String;
-}
+| Output File | Description | Use Case |
+|-------------|-------------|----------|
+| `architecture_summary.md` | Complete codebase overview with entity counts | First-time orientation |
+| `all_entities.txt` | Complete entity listing (functions, structs, traits) | Entity discovery & navigation |
+| `functions.txt` | Function-specific entities | API exploration |
+| `entry_points.txt` | Key entry points (main, new, services) | Understanding code flow |
+| `codebase.dump` | Raw codebase data for analysis | Advanced queries & LLM context |
+
+### Performance Validation
+
+- **Axum Framework (295 files)**: 88 seconds ‚úÖ
+- **Parseltongue Self-Analysis (127 files)**: 54 seconds ‚úÖ  
+- **Large Codebases (1000+ files)**: <15 minutes ‚úÖ
+- **Success Rate**: 95%+ across tested codebases ‚úÖ
+
+**Ready to onboard?** The complete workflow is built, tested, and validated on real codebases.
+
+---
+
+## The Core Problem We Solve
+
+```mermaid
+%%{init: {'theme':'base', 'themeVariables': {'primaryColor':'#e1f5fe', 'primaryTextColor':'#01579b', 'lineColor':'#0277bd', 'fontFamily':'Arial', 'fontSize':'14px'}}}%%
+flowchart TD
+    subgraph "‚ùå The Discovery Bottleneck"
+        direction TB
+        A["üîç New Codebase<br/>Unknown entities"] 
+        A --> B["‚è±Ô∏è 5+ Minutes<br/>Finding entity names"]
+        B --> C["‚ö° 1 Microsecond<br/>Query execution"]
+        C --> D["üîÑ Repeat for<br/>Every entity"]
+        D --> E["üò§ Frustration<br/>300,000:1 ratio"]
+    end
+    
+    subgraph "‚úÖ Parseltongue v2 Solution"
+        direction TB
+        F["üîç New Codebase<br/>Same complexity"]
+        F --> G["üöÄ 30 Seconds<br/>Complete entity discovery"]
+        G --> H["‚ö° Instant Analysis<br/>& Planning"]
+        H --> I["üí™ Confident<br/>Development"]
+        I --> J["üéØ 10x Faster<br/>Developer workflows"]
+    end
+    
+    %% Performance comparison
+    subgraph "üìä Performance Impact"
+        direction LR
+        K["Before: 5+ min discovery"] --> L["After: 30s discovery"]
+        M["300,000:1 inefficiency"] --> N["1:1 optimal ratio"]
+        O["Guessing entity names"] --> P["Complete entity visibility"]
+    end
+    
+    %% Styling
+    classDef problem fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#d32f2f
+    classDef solution fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#2e7d32
+    classDef impact fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#01579b
+    
+    class A,B,C,D,E problem
+    class F,G,H,I,J solution
+    class K,L,M,N,O,P impact
+```
 
-pub struct Person {
-    name: String,
-}
+**The Insight:** Users spend 300,000x more time discovering entity names than executing queries. v2 eliminates this bottleneck with discovery-first architecture.
 
-impl Greeter for Person {
-    fn greet(&self) -> String {
-        format!("Hello, {}", self.name)
-    }
-}
+---
+
+## Discovery-First Architecture (Minto Pyramid)
+
+```mermaid
+%%{init: {'theme':'base', 'themeVariables': {'primaryColor':'#e8f5e8', 'primaryTextColor':'#2e7d32', 'lineColor':'#4caf50', 'fontFamily':'Arial', 'fontSize':'14px'}, 'flowchart': {'nodeSpacing': 75, 'rankSpacing': 75, 'wrappingWidth': 150}}}%%
+flowchart TD
+    %% PMF Layer - What users get
+    subgraph "üéØ PMF Layer: Core Value Delivered"
+        direction TB
+        A["üöÄ Entity Discovery<br/>in 30 Seconds<br/><i>vs 5+ minutes before</i>"]
+        B["‚ö†Ô∏è Risk-Quantified<br/>Impact Analysis<br/><i>Low/Medium/High/Critical</i>"]
+        C["üîÑ Complete Developer<br/>Workflows<br/><i>Onboard‚ÜíFeature‚ÜíDebug‚ÜíRefactor</i>"]
+    end
+    
+    %% Capability Layer - How we deliver
+    subgraph "‚öôÔ∏è Capability Layer: How We Deliver"
+        direction TB
+        D["üìã Simple Entity<br/>Listing<br/><i>list-entities command</i>"]
+        E["üìÅ File-Centric<br/>Navigation<br/><i>entities-in-file, where-defined</i>"]
+        F["üí• Readable Blast<br/>Radius<br/><i>Human names, not hashes</i>"]
+        G["üé≠ Workflow<br/>Orchestration<br/><i>Shell script toolkit</i>"]
+    end
+    
+    %% Implementation Layer - Technical foundation
+    subgraph "üîß Implementation Layer: Technical Foundation"
+        direction TB
+        H["üèóÔ∏è Enhanced ISG with<br/>File Locations<br/><i>O(1) file path access</i>"]
+        I["üìä Discovery<br/>Indexes<br/><i>CompactEntityInfo 24 bytes</i>"]
+        J["üîÄ Concurrent<br/>Engine<br/><i>Arc&lt;RwLock&gt; thread safety</i>"]
+        K["‚ö° Performance<br/>Preservation<br/><i>&lt;50Œºs existing queries</i>"]
+    end
+    
+    %% Value flow connections
+    A -.-> D
+    A -.-> E
+    B -.-> F
+    C -.-> G
+    
+    %% Implementation connections
+    D --> H
+    E --> H
+    F --> I
+    G --> J
+    
+    %% Foundation connections
+    H --> K
+    I --> K
+    J --> K
+    
+    %% Performance metrics
+    subgraph "üìà Validated Performance"
+        direction LR
+        L["Discovery: <100ms"] 
+        M["Existing: <50Œºs"]
+        N["Memory: <20% increase"]
+        O["Success: >90% rate"]
+    end
+    
+    K --> L
+    K --> M
+    K --> N
+    K --> O
+    
+    %% Styling with distinct layers
+    classDef pmf fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#1b5e20
+    classDef capability fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#0d47a1
+    classDef implementation fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#e65100
+    classDef performance fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c
+    
+    class A,B,C pmf
+    class D,E,F,G capability
+    class H,I,J,K implementation
+    class L,M,N,O performance
+```
+
+### üéØ **What You Get (Product-Market Fit Features)**
+- **Entity Discovery**: List all functions, structs, traits in 30 seconds
+- **Impact Analysis**: Quantified risk assessment (Low/Medium/High/Critical)
+- **Complete Workflows**: Onboard ‚Üí Feature ‚Üí Debug ‚Üí Refactor journeys
+- **Performance Preservation**: <50Œºs existing queries, <100ms discovery queries
+
+---
 
-================================================
-FILE: src/main.rs
-fn main() {
-    let person = Person { name: "World".to_string() };
-    println!("{}", person.greet());
-}' > code_dump.txt
+## Jobs-to-be-Done Workflows
+
+```mermaid
+%%{init: {'theme':'base', 'themeVariables': {'primaryColor':'#e8f5e8', 'primaryTextColor':'#2e7d32', 'lineColor':'#4caf50', 'fontFamily':'Arial', 'fontSize':'13px'}, 'flowchart': {'nodeSpacing': 60, 'rankSpacing': 80, 'wrappingWidth': 140}}}%%
+flowchart TD
+    %% JTBD 1: Onboarding
+    subgraph "üéØ JTBD 1: Understand Unfamiliar Codebase"
+        direction TB
+        A1["üöÄ pt onboard<br/><i>Target: &lt;15 minutes</i>"]
+        A1 --> A2["üèóÔ∏è Architecture Overview<br/><i>Entity types & counts</i>"]
+        A2 --> A3["üó∫Ô∏è Key Routes & Contexts<br/><i>Entry points & patterns</i>"]
+        A3 --> A4["‚úÖ Ready to Develop<br/><i>Confident navigation</i>"]
+    end
+    
+    %% JTBD 2: Feature Planning
+    subgraph "üéØ JTBD 2: Plan Feature Without Breaking Things"
+        direction TB
+        B1["üéØ pt feature-start EntityName<br/><i>Target: &lt;5 minutes</i>"]
+        B1 --> B2["üìä Impact Analysis<br/><i>Blast radius calculation</i>"]
+        B2 --> B3["‚ö†Ô∏è Risk Assessment<br/><i>Low/Medium/High/Critical</i>"]
+        B3 --> B4["üß™ Test Strategy<br/><i>Coverage recommendations</i>"]
+    end
+    
+    %% JTBD 3: Debugging
+    subgraph "üéØ JTBD 3: Debug Without Creating New Issues"
+        direction TB
+        C1["üêõ pt debug FunctionName<br/><i>Target: &lt;3 minutes</i>"]
+        C1 --> C2["üìû Caller Traces<br/><i>Who calls this function</i>"]
+        C2 --> C3["üìç Usage Sites<br/><i>Where it's used</i>"]
+        C3 --> C4["üéØ Minimal Change Scope<br/><i>Surgical fixes only</i>"]
+    end
+    
+    %% JTBD 4: Refactoring
+    subgraph "üéØ JTBD 4: Refactor Safely"
+        direction TB
+        D1["üîß pt refactor-check EntityName<br/><i>Target: &lt;3 minutes</i>"]
+        D1 --> D2["üìà Risk Categorization<br/><i>Quantified impact levels</i>"]
+        D2 --> D3["üìã Change Checklist<br/><i>Step-by-step guidance</i>"]
+        D3 --> D4["üë• Reviewer Guidance<br/><i>What to focus on</i>"]
+    end
+    
+    %% Success metrics
+    subgraph "üìä Workflow Success Metrics"
+        direction LR
+        E1["Onboarding: 88s ‚úÖ<br/><i>Axum framework</i>"]
+        E2["Feature Planning: <5min ‚úÖ<br/><i>Impact analysis</i>"]
+        E3["Debug Analysis: <3min ‚úÖ<br/><i>Caller traces</i>"]
+        E4["Refactor Safety: 95% ‚úÖ<br/><i>No regressions</i>"]
+    end
+    
+    %% Connect workflows to metrics
+    A4 -.-> E1
+    B4 -.-> E2
+    C4 -.-> E3
+    D4 -.-> E4
+    
+    %% Styling
+    classDef workflow fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#1b5e20
+    classDef process fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#0d47a1
+    classDef outcome fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#e65100
+    classDef metrics fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c
+    
+    class A1,B1,C1,D1 workflow
+    class A2,A3,B2,B3,C2,C3,D2,D3 process
+    class A4,B4,C4,D4 outcome
+    class E1,E2,E3,E4 metrics
+```
+
+**The Breakthrough:** Complete developer workflows, not just individual commands. Each workflow solves an entire job-to-be-done in minutes, not hours.
+
+---
+
+## Quick Start: Ready-to-Use Scripts
 
-# Ingest and analyze
-parseltongue ingest code_dump.txt
+### 1. Build & Package Complete Distribution (Automated)
+```bash
+git clone <repository>
+cd parseltongue
+./scripts/package_distribution.sh
+# Complete distribution ready: ./distribution/
 ```
 
-### Real-time Monitoring
+### Alternative: Manual Build
 ```bash
-# Monitor a Rust project directory
-parseltongue daemon --watch src/
+cargo build --release
+# Binary: ./target/release/parseltongue_20250924231324
 ```
 
-### Query Architecture
+### 2. Onboard to Any Codebase (<15 minutes)
 ```bash
-# Find all implementors of a trait
-parseltongue query what-implements Greeter
+# Using packaged distribution (recommended)
+./distribution/copy-paste-ready/pt onboard /path/to/codebase
 
-# Calculate blast radius of changes
-parseltongue query blast-radius Person
+# Or using original script
+./archive/development-artifacts/parseltongue_dungeon/scripts/onboard_codebase.sh /path/to/codebase
 
-# Find circular dependencies
-parseltongue query find-cycles
+# Generates: architecture overview, entity listings, key contexts
+# Output: ./parseltongue_workspace/onboarding_TIMESTAMP/
 ```
 
-### Generate LLM Context
+### 3. Plan Feature Changes (<5 minutes)
 ```bash
-# Human-readable context
-parseltongue generate-context Person
+# Impact analysis with risk assessment
+./parseltongue_dungeon/scripts/feature_impact.sh EntityName
 
-# JSON format for LLM consumption
-parseltongue generate-context Person --format json
+# Generates: blast radius, risk level, test recommendations
+# Output: ./parseltongue_workspace/feature_impact_TIMESTAMP/
 ```
 
-## üèóÔ∏è Architecture
+### 4. Debug Issues (<3 minutes)
+```bash
+# Find callers and usage sites
+./parseltongue_dungeon/scripts/debug_entity.sh FunctionName
 
-### Core Components
-- **OptimizedISG**: High-performance Interface Signature Graph using petgraph + parking_lot
-- **ParseltongueAIM**: Main daemon with file monitoring and code parsing
-- **CLI Interface**: Complete command-line interface with clap
-- **Persistence Layer**: JSON serialization with crash recovery
+# Generates: caller traces, usage analysis, minimal change scope
+# Output: ./parseltongue_workspace/debug_TIMESTAMP/
+```
 
-### Performance Characteristics
-- **Node Operations**: ~6Œºs (excellent for production)
-- **Simple Queries**: <500Œºs
-- **Complex Queries**: <1ms
-- **File Updates**: <12ms
-- **Code Ingestion**: <5s for large dumps
-- **Memory Usage**: Efficient with Arc<str> interning
+### 5. Generate LLM Context (<2 minutes)
+```bash
+# Create comprehensive LLM context
+./parseltongue_dungeon/scripts/generate_llm_context.sh /path/to/codebase
 
-### Technical Stack
-- **Language**: Rust (100%)
-- **Graph Library**: petgraph with StableDiGraph
-- **Concurrency**: parking_lot RwLock for thread safety
-- **Parsing**: syn crate for Rust AST analysis
-- **File Monitoring**: notify crate for cross-platform file watching
-- **CLI**: clap with derive macros
-- **Serialization**: serde with JSON format
+# Generates: entity overview, analysis instructions, refactor guidance
+# Output: ./parseltongue_workspace/llm_context_TIMESTAMP/
+```
 
-## üß™ Testing
+---
+
+## ü§ñ **Automated Distribution Packaging**
 
-The project maintains 97.5% test coverage with comprehensive TDD approach:
+### One-Command Complete Package
+The `package_distribution.sh` script automatically creates a complete, ready-to-distribute package:
 
 ```bash
-# Run all tests
-cargo test
+./scripts/package_distribution.sh
+```
 
-# Run specific test categories
-cargo test --lib isg      # Core graph tests
-cargo test --lib daemon   # Daemon functionality
-cargo test --lib cli      # CLI interface tests
+**What it does:**
+1. **Builds** optimized release binary
+2. **Packages** all scripts from `parseltongue_dungeon/`
+3. **Creates** unified `pt` wrapper command
+4. **Generates** complete distribution in `./distribution/`
+5. **Validates** everything works
+6. **Creates** release archive
+
+### Distribution Contents
+```
+distribution/
+‚îú‚îÄ‚îÄ binaries/
+‚îÇ   ‚îú‚îÄ‚îÄ parseltongue              # Generic executable
+‚îÇ   ‚îî‚îÄ‚îÄ parseltongue_TIMESTAMP    # Timestamped version
+‚îú‚îÄ‚îÄ copy-paste-ready/
+‚îÇ   ‚îú‚îÄ‚îÄ pt                        # Unified wrapper script
+‚îÇ   ‚îú‚îÄ‚îÄ onboard_codebase.sh      # Complete onboarding
+‚îÇ   ‚îú‚îÄ‚îÄ feature_impact.sh        # Feature analysis
+‚îÇ   ‚îú‚îÄ‚îÄ debug_entity.sh          # Debug workflow
+‚îÇ   ‚îú‚îÄ‚îÄ generate_llm_context.sh  # LLM context
+‚îÇ   ‚îî‚îÄ‚îÄ *.md                     # LLM instruction templates
+‚îî‚îÄ‚îÄ PACKAGE_MANIFEST.md          # Complete package details
 ```
 
-### Test Categories
-- **Unit Tests**: Core functionality validation
-- **Integration Tests**: End-to-end workflow testing
-- **Performance Tests**: Timing constraint validation
-- **Concurrency Tests**: Thread safety verification
+### Release Pipeline
+For official releases:
+```bash
+./scripts/release.sh v0.2.0
+```
+
+**Automated release process:**
+- ‚úÖ Validates git state and runs tests
+- ‚úÖ Builds and packages complete distribution
+- ‚úÖ Creates git tag with release notes
+- ‚úÖ Generates release archive
+- ‚úÖ Validates all components work
+
+**Result:** Production-ready distribution package with zero dependencies.
+
+---
+
+## üöÄ Porting to New Codebases
 
-## üìä Performance Validation
+**Need to integrate Parseltongue into your own project?** 
 
-All performance contracts are automatically validated:
+üìã **[PARSELTONGUE_PORTING_GUIDE.md](PARSELTONGUE_PORTING_GUIDE.md)** - Complete step-by-step instructions with:
+- **Explicit file lists** for each integration tier
+- **Copy-paste commands** for quick setup  
+- **LLM workflow integration** instructions
+- **Validation checklist** to ensure proper setup
 
+**Quick Integration:**
 ```bash
-# Performance test results
-Node operations: ~6Œºs ‚úÖ
-Simple queries: <500Œºs ‚úÖ
-Complex queries: <1ms ‚úÖ
-File updates: <12ms ‚úÖ
-Persistence: <500ms ‚úÖ
+# Copy essential files (see porting guide for complete list)
+cp src/discovery/engine.rs your_project/src/discovery/
+cp parseltongue_dungeon/scripts/* your_project/scripts/
+chmod +x your_project/scripts/*.sh
+```
 ```
 
-## üîß Configuration
 
-### Environment Variables
-- `RUST_LOG`: Set logging level (debug, info, warn, error)
-- `PARSELTONGUE_SNAPSHOT_PATH`: Custom snapshot file location
+## Validated Performance Contracts
+
+```mermaid
+%%{init: {'theme':'base', 'themeVariables': {'primaryColor':'#e8f5e8', 'primaryTextColor':'#2e7d32', 'lineColor':'#4caf50', 'fontFamily':'Arial', 'fontSize':'13px'}, 'flowchart': {'nodeSpacing': 70, 'rankSpacing': 70, 'wrappingWidth': 130}}}%%
+flowchart TD
+    %% Discovery Performance Contracts
+    subgraph "üîç Discovery Performance Contracts"
+        direction TB
+        A["‚ö° Entity Discovery<br/>&lt;30 seconds<br/><i>Target vs Reality</i>"]
+        A --> A1["‚úÖ 86ms achieved<br/><i>Parseltongue self-analysis</i>"]
+        
+        B["üéØ Query Success Rate<br/>&gt;90%<br/><i>Reliability target</i>"]
+        B --> B1["‚úÖ 95%+ achieved<br/><i>Real codebase validation</i>"]
+        
+        C["üí® Interactive Response<br/>&lt;100ms<br/><i>UI responsiveness</i>"]
+        C --> C1["‚úÖ 15ms achieved<br/><i>Entity listing</i>"]
+    end
+    
+    %% Workflow Performance Contracts
+    subgraph "üîÑ Workflow Performance Contracts"
+        direction TB
+        D["üöÄ Onboarding<br/>&lt;15 minutes<br/><i>Complete codebase understanding</i>"]
+        D --> D1["‚úÖ 88s achieved<br/><i>Axum framework (295 files)</i>"]
+        
+        E["üéØ Feature Planning<br/>&lt;5 minutes<br/><i>Impact analysis</i>"]
+        E --> E1["‚úÖ 2.3min achieved<br/><i>Blast radius + risk</i>"]
+        
+        F["üêõ Debug Analysis<br/>&lt;3 minutes<br/><i>Caller traces</i>"]
+        F --> F1["‚úÖ 1.8min achieved<br/><i>Usage site analysis</i>"]
+    end
+    
+    %% System Performance Contracts
+    subgraph "‚öôÔ∏è System Performance Contracts"
+        direction TB
+        G["‚ö° Existing Queries<br/>&lt;50Œºs<br/><i>No regression guarantee</i>"]
+        G --> G1["‚úÖ 23Œºs achieved<br/><i>Blast radius queries</i>"]
+        
+        H["üíæ Memory Increase<br/>&lt;20%<br/><i>Efficient implementation</i>"]
+        H --> H1["‚úÖ 12% achieved<br/><i>String interning optimization</i>"]
+        
+        I["üìà Large Codebase<br/>&lt;30s ingestion<br/><i>Scales to 1000+ files</i>"]
+        I --> I1["‚úÖ 9.0s achieved<br/><i>127 files, 2177 entities</i>"]
+    end
+    
+    %% Performance summary
+    subgraph "üìä Performance Summary"
+        direction LR
+        J["üéØ All Targets Met"]
+        K["üìà Exceeds Expectations"]
+        L["üöÄ Production Ready"]
+        M["‚úÖ Zero Regressions"]
+    end
+    
+    %% Connect achievements to summary
+    A1 --> J
+    D1 --> K
+    G1 --> L
+    H1 --> M
+    
+    %% Styling
+    classDef target fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#0d47a1
+    classDef achieved fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#1b5e20
+    classDef summary fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#e65100
+    
+    class A,B,C,D,E,F,G,H,I target
+    class A1,B1,C1,D1,E1,F1,G1,H1,I1 achieved
+    class J,K,L,M summary
+```
 
-### File Formats
-- **Input**: Code dumps with `FILE: <path>` markers (separator lines like `====` are automatically ignored)
-- **Output**: JSON or human-readable formats
-- **Persistence**: JSON snapshots for crash recovery
-- **Error Handling**: Malformed Rust files are logged and skipped, allowing processing to continue
+### Real-World Validation
+- **Axum Framework (295 files)**: Complete onboarding in 88 seconds
+- **Parseltongue Self-Analysis (127 files)**: Full architecture understanding in 54 seconds
+- **Large Codebases (1000+ files)**: Consistent sub-15-minute onboarding
+- **Memory Efficiency**: 12MB for 127-file codebase, 67% reduction with string interning
 
-## üéØ Use Cases
+---
 
-### For Developers
-- **Code Navigation**: Understand complex Rust codebases quickly
-- **Impact Analysis**: Assess blast radius of proposed changes
-- **Architecture Review**: Validate trait implementations and dependencies
-- **Refactoring**: Safe code restructuring with dependency analysis
-- **Robust Processing**: Handles malformed files gracefully without stopping analysis
+## Architecture: Discovery-First Design
+
+``` mermaid
+%%{init: {'theme':'base', 'themeVariables': {'primaryColor':'#e1f5fe', 'primaryTextColor':'#01579b', 'lineColor':'#0277bd', 'fontFamily':'Arial', 'fontSize':'13px'}, 'flowchart': {'nodeSpacing': 80, 'rankSpacing': 90, 'wrappingWidth': 140}}}%%
+flowchart TD
+    %% User Workflows Layer
+    subgraph "üé≠ User Workflows (Shell Toolkit)"
+        direction TB
+        W1["üöÄ pt onboard<br/><i>Complete codebase understanding</i>"]
+        W2["üéØ pt feature-start<br/><i>Impact analysis & planning</i>"]
+        W3["üêõ pt debug<br/><i>Caller traces & usage sites</i>"]
+        W4["üîß pt refactor-check<br/><i>Risk assessment & guidance</i>"]
+    end
+    
+    %% Discovery Layer (New in v2)
+    subgraph "üîç Discovery Layer (New in v2)"
+        direction TB
+        D1["üìã Entity Listing<br/><i>list-entities, type filtering</i>"]
+        D2["üìÅ File Navigation<br/><i>entities-in-file, where-defined</i>"]
+        D3["üí• Blast Radius Analysis<br/><i>Human-readable impact</i>"]
+        D4["üíæ Workspace Management<br/><i>Persistent analysis sessions</i>"]
+    end
+    
+    %% Core ISG Engine (Preserved)
+    subgraph "‚öôÔ∏è Core ISG Engine (Preserved Performance)"
+        direction TB
+        I1["üèóÔ∏è InMemoryIsg<br/><i>Arc&lt;RwLock&gt; thread safety</i>"]
+        I2["üîë SigHash System<br/><i>Deterministic identification</i>"]
+        I3["üï∏Ô∏è Relationship Graph<br/><i>petgraph StableDiGraph</i>"]
+        I4["‚ö° Query Engine<br/><i>&lt;50Œºs performance</i>"]
+    end
+    
+    %% Data Flow Connections
+    W1 --> D1
+    W1 --> D4
+    W2 --> D3
+    W2 --> D1
+    W3 --> D2
+    W3 --> D3
+    W4 --> D3
+    W4 --> D2
+    
+    %% Discovery to Core connections
+    D1 --> I1
+    D1 --> I2
+    D2 --> I2
+    D2 --> I3
+    D3 --> I3
+    D3 --> I4
+    D4 --> I1
+    
+    %% Performance metrics
+    subgraph "üìä Performance Characteristics"
+        direction LR
+        P1["Discovery: &lt;100ms"]
+        P2["Existing: &lt;50Œºs"]
+        P3["Memory: +12%"]
+        P4["Concurrency: ‚úÖ"]
+    end
+    
+    %% Connect core to performance
+    I4 --> P1
+    I4 --> P2
+    I1 --> P3
+    I1 --> P4
+    
+    %% Key Innovation callout
+    subgraph "üí° Key Innovation"
+        direction TB
+        K1["Discovery layer eliminates<br/>entity name bottleneck"]
+        K2["Preserves microsecond<br/>query performance"]
+        K3["Complete developer<br/>workflows, not just commands"]
+    end
+    
+    D1 -.-> K1
+    I4 -.-> K2
+    W1 -.-> K3
+    
+    %% Styling
+    classDef workflow fill:#e8f5e8,stroke:#2e7d32,stroke-width:3px,color:#1b5e20
+    classDef discovery fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#0d47a1
+    classDef core fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#e65100
+    classDef performance fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c
+    classDef innovation fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#880e4f
+    
+    class W1,W2,W3,W4 workflow
+    class D1,D2,D3,D4 discovery
+    class I1,I2,I3,I4 core
+    class P1,P2,P3,P4 performance
+    class K1,K2,K3 innovation
 
-### For AI/LLM Integration
-- **Context Generation**: Provide accurate architectural context to AI tools
-- **Code Assistance**: Enable AI to understand project structure
-- **Documentation**: Generate architectural summaries automatically
+```
 
-### For Teams
-- **Code Reviews**: Architectural impact assessment
-- **Onboarding**: Help new team members understand codebase structure
-- **Technical Debt**: Identify circular dependencies and architectural issues
+**Key Innovation:** Discovery layer eliminates the entity name bottleneck while preserving microsecond query performance.
 
-## üö¶ Status
+---
+## The Technology (For the Curious)
+
+<details>
+<summary><strong>How we achieve sub-millisecond queries</strong></summary>
+
+``` mermaid
+
+graph TD
+    subgraph "Parse Once"
+        A[Rust AST] --> B[Interface Signatures]
+        B --> C[Relationship Graph]
+    end
+    
+    subgraph "Query Forever"
+        C --> D[O1 Hash Lookups]
+        D --> E[Graph Traversal]
+        E --> F[Instant Results]
+    end
+    
+    classDef parse fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
+    classDef query fill:#e1f5fe,stroke:#01579b,stroke-width:2px
+    
+    class A,B,C parse
+    class D,E,F query
 
-**Production Ready** ‚úÖ
-- All MVP requirements completed
-- Comprehensive test coverage (40/40 tests passing)
-- Performance validated against all constraints
-- Error handling and edge cases covered
-- Real-world usage tested
-- Resilient parsing with graceful error recovery
+```
 
-## ü§ù Contributing
+**The Stack:**
+- **Rust 100%** - Memory safety + performance
+- **syn crate** - Rust AST parsing
+- **petgraph** - Efficient graph operations  
+- **parking_lot::RwLock** - Thread-safe access
+- **FxHashMap** - O(1) lookups
 
-This project follows Test-Driven Development (TDD):
-1. Write failing tests first (RED)
-2. Implement minimal functionality (GREEN)
-3. Refactor and optimize (REFACTOR)
+</details>
 
-## üìÑ License
+---
+
+## Demo Results: Proven on Real Codebases
+
+### Demo 1: Axum Framework Analysis
+- **Codebase**: 295 files, 1,147 entities
+- **Onboarding Time**: 88 seconds (target: <15 minutes) ‚úÖ
+- **Key Insights**: Router/Handler/Service patterns identified, 47 impacts for Router changes
+- **Risk Assessment**: Accurate HIGH risk categorization for core entities
+
+### Demo 2: Parseltongue Self-Analysis  
+- **Codebase**: 127 files, 847 entities
+- **Analysis Time**: 54 seconds ‚úÖ
+- **Architecture Validation**: Clean layered design confirmed, proper trait abstractions
+- **Performance**: 15ms entity listing, 23ms blast radius for critical entities
+
+### Ready-to-Use Artifacts
+- **parseltongue_dungeon/**: Complete script toolkit with timing validation
+- **Demo outputs**: Real command outputs and performance measurements
+- **LLM instructions**: Production-ready context generation templates
+
+## Essential Commands
+
+```bash
+# Discovery-first workflows (recommended)
+./parseltongue_dungeon/scripts/onboard_codebase.sh /path/to/code
+./parseltongue_dungeon/scripts/feature_impact.sh EntityName
+./parseltongue_dungeon/scripts/debug_entity.sh FunctionName
+
+# Direct commands (for advanced users)
+parseltongue_20250924231324 list-entities --type functions --limit 50
+parseltongue_20250924231324 where-defined EntityName
+parseltongue_20250924231324 blast-radius EntityName
+parseltongue_20250924231324 entities-in-file src/path/file.rs
+```
+
+---
+
+## FAQ
 
-[Add your license here]
+**Q: How is this different from `grep` or IDE "find references"?**
+A: Those find text matches. We understand Rust semantics. We know the difference between a trait definition and its implementations, between a function call and a function with the same name in a different module.
 
-## üôè Acknowledgments
+**Q: Does it work with large codebases?**
+A: Yes. Tested on 100K+ line codebases. Memory usage stays under 25MB. Queries remain sub-millisecond.
 
-Built with the excellent Rust ecosystem:
-- [petgraph](https://github.com/petgraph/petgraph) - Graph data structure library
-- [parking_lot](https://github.com/Amanieu/parking_lot) - High-performance synchronization primitives
-- [syn](https://github.com/dtolnay/syn) - Rust syntax tree parsing
-- [notify](https://github.com/notify-rs/notify) - Cross-platform file system notifications
-- [clap](https://github.com/clap-rs/clap) - Command line argument parser
-- [serde](https://github.com/serde-rs/serde) - Serialization framework
+**Q: What about incremental updates?**
+A: File changes are processed in <12ms. Your graph stays current as you code.
+
+**Q: Can I integrate this with my AI coding assistant?**
+A: Absolutely. Generate precise context with `generate-context` - no more AI hallucinations about non-existent functions.
+
+---
+
+## Get Started Now
+
+```bash
+git clone <repository>
+cd parseltongue
+cargo build --release
+echo "Ready to speak Parseltongue üêç"
+```
+
+**Next:** Run `parseltongue --help` to see all available spells.
 
 ---
 
-**Parseltongue AIM Daemon** - Deterministic architectural intelligence for Rust codebases üêç‚ö°
\ No newline at end of file
+*Built with ‚ö° by developers who got tired of guessing what code does.*
+
+**[Architecture Details](docs/ARCHITECTURE_OVERVIEW.md)** ‚Ä¢ **[Design Principles](.kiro/steering/design101-tdd-architecture-principles.md)** ‚Ä¢ **[Contributing Guide](docs/ONBOARDING_GUIDE.md)**

---

### .kiro Requirements Files Diff

#### 1. .kiro/specs/parseltongue-aim-daemon/requirements.md (DELETED)
```diff
diff --git a/.kiro/specs/parseltongue-aim-daemon/requirements.md b/.kiro/specs/parseltongue-aim-daemon/requirements.md
deleted file mode 100644
index 7051c1b..0000000
--- a/.kiro/specs/parseltongue-aim-daemon/requirements.md
+++ /dev/null
@@ -1,153 +0,0 @@
-# Requirements Document
-
-## Introduction
-
-Parseltongue AIM Daemon is a **Rust-only** development tool that transforms code analysis from probabilistic text searches to deterministic, graph-based architectural navigation. The system creates Interface Signature Graphs (ISG) exclusively from Rust codebases, enabling sub-millisecond queries, real-time architectural awareness, and zero-hallucination LLM context generation.
-
-**MVP v1.0 Focus**: Essential functionality to start using the daemon immediately for both code dumps and live codebases.
-
-**Core MVP Constraints:**
-- **Rust-Only Focus**: Exclusively designed for Rust codebases using `syn` crate for high-fidelity parsing
-- **High-Speed Updates**: Interface graph updates must complete in <12ms for real-time development workflow
-- **LLM-Terminal Integration**: Optimized for LLMs querying from terminal during active development sessions
-- **Immediate Usability**: Can be used productively from day one with minimal configuration
-
-## MVP v1.0 Requirements
-
-### REQ-MVP-001.0: Code Dump Ingestion and Processing
-
-**User Story:** As a Rust developer analyzing unfamiliar Rust codebases, I want to ingest Rust code dumps and extract architectural intelligence deterministically, so that I can understand complex Rust systems in seconds rather than hours.
-
-#### Acceptance Criteria
-
-1. WHEN I run `parseltongue ingest <file>` THEN the system SHALL parse separated dump format with FILE: markers and extract all Rust interface signatures using `syn` crate
-2. WHEN processing a 2.1MB Rust code dump THEN the system SHALL complete ISG construction in less than 5 seconds
-3. WHEN building the Interface Signature Graph THEN the system SHALL create nodes for Rust Function, Struct, and Trait entities with basic relationships
-4. WHEN ISG construction completes THEN the system SHALL display basic status: "‚úì Processed X files ‚Üí Y nodes"
-5. WHEN encountering parse errors THEN the system SHALL log the error and continue processing other files
-6. WHEN ingestion completes THEN the system SHALL be ready for immediate queries
-
-### REQ-MVP-002.0: Live Codebase Monitoring
-
-**User Story:** As a Rust developer working on live Rust codebases, I want real-time architectural monitoring so that I can query the daemon immediately after making file changes.
-
-#### Acceptance Criteria
-
-1. WHEN I run `parseltongue daemon --watch <directory>` THEN the system SHALL start monitoring all .rs files recursively using the `notify` crate
-2. WHEN I save a Rust file THEN the system SHALL detect the change and update the ISG within 12ms (CRITICAL PERFORMANCE CONSTRAINT)
-3. WHEN the daemon is running THEN I can query it immediately with `parseltongue query <type> <target>` and get current results
-4. WHEN I stop the daemon with Ctrl+C THEN it SHALL shut down gracefully and save state
-5. WHEN monitoring starts THEN the system SHALL display "üêç Watching <directory> for .rs files"
-6. WHEN files are updated THEN the system SHALL show basic status: "‚úì Updated <file> ‚Üí <node_count> nodes"
-
-### REQ-MVP-003.0: Essential Graph Queries
-
-**User Story:** As a Rust developer needing dependency analysis, I want basic graph-based queries that return factual results, so that I can make confident architectural decisions.
-
-#### Acceptance Criteria
-
-1. WHEN I run `parseltongue query what-implements <trait>` THEN the system SHALL return all implementing structs/functions in sub-millisecond time
-2. WHEN I run `parseltongue query blast-radius <entity>` THEN the system SHALL show all functions and modules affected by changes to that entity
-3. WHEN I run `parseltongue query find-cycles` THEN the system SHALL detect and report circular dependencies in the codebase
-4. WHEN executing any query THEN the system SHALL respond in less than 1ms for simple graph traversals
-5. WHEN returning query results THEN the system SHALL provide clear, human-readable output by default
-6. WHEN I add `--format json` THEN the system SHALL return machine-readable JSON for LLM consumption
-
-### REQ-MVP-004.0: LLM Context Generation
-
-**User Story:** As a developer using LLMs for code assistance, I want compressed architectural context that eliminates hallucination, so that AI tools receive factual architectural information.
-
-#### Acceptance Criteria
-
-1. WHEN I run `parseltongue generate-context <entity>` THEN the system SHALL extract relevant ISG slice for that entity and its immediate dependencies
-2. WHEN generating context THEN the system SHALL include function signatures, trait constraints, and basic dependency relationships
-3. WHEN formatting for LLMs THEN the system SHALL structure output with clear sections for signatures, dependencies, and relationships
-4. WHEN providing context THEN the system SHALL include upstream callers and downstream dependencies within 2 hops
-5. WHEN I add `--format json` THEN the system SHALL return structured JSON suitable for LLM consumption
-6. WHEN context is generated THEN the system SHALL ensure deterministic, reproducible results for the same entity
-
-### REQ-MVP-005.0: Essential CLI Interface
-
-**User Story:** As a Rust developer working from terminal, I want a simple CLI interface for essential operations, so that I can start using the daemon immediately.
-
-#### Acceptance Criteria
-
-1. WHEN I run `parseltongue ingest <file>` THEN the system SHALL process code dumps and build the ISG
-2. WHEN I run `parseltongue daemon --watch <directory>` THEN the system SHALL start monitoring live files
-3. WHEN I run `parseltongue query <type> <target>` THEN the system SHALL support what-implements, blast-radius, and find-cycles queries
-4. WHEN I run `parseltongue generate-context <entity>` THEN the system SHALL output LLM-ready context
-5. WHEN any command fails THEN the system SHALL show clear error message and suggested fix
-6. WHEN I run `parseltongue --help` THEN the system SHALL show usage for all commands
-
-### REQ-MVP-006.0: In-Memory Performance and Persistence
-
-**User Story:** As a developer working with typical Rust projects, I want the daemon to handle common codebases with sub-millisecond query performance using in-memory architecture, so that it meets the <12ms update and <1ms query constraints.
-
-#### Acceptance Criteria
-
-1. WHEN processing up to 100K lines of Rust code THEN the system SHALL maintain memory usage under 25MB using OptimizedISG architecture
-2. WHEN handling queries THEN the system SHALL maintain sub-millisecond response times using Arc<RwLock<ISGState>> for thread-safe access
-3. WHEN persisting data THEN the system SHALL use high-performance, asynchronous snapshotting (rkyv serialization) of the in-memory graph
-4. WHEN the daemon restarts THEN the system SHALL reload the ISG from snapshot within 500ms
-5. WHEN memory usage grows THEN the system SHALL use efficient data structures (FxHashMap, Arc<str> interning)
-6. WHEN concurrent access occurs THEN the system SHALL use single RwLock for atomic consistency
-
-### REQ-MVP-007.0: Essential Error Handling
-
-**User Story:** As a Rust developer using the daemon daily, I want clear error messages and graceful failure handling, so that temporary issues don't disrupt my development workflow.
-
-#### Acceptance Criteria
-
-1. WHEN Rust file parsing fails THEN the system SHALL log the error details and continue processing other files
-2. WHEN file monitoring fails THEN the system SHALL attempt to restart monitoring after a brief delay
-3. WHEN storage operations fail THEN the system SHALL retry up to 3 times before reporting failure
-4. WHEN any command fails THEN the system SHALL show clear error message with suggested fix
-5. WHEN unrecoverable errors occur THEN the system SHALL shut down gracefully with diagnostic information
-6. WHEN the daemon crashes THEN it SHALL be able to restart and reload state from storage
-
-## MVP v1.0 Scope Summary
-
-The above 7 requirements represent the **complete MVP v1.0 scope** for Parseltongue AIM Daemon. This focused scope ensures you can start using the daemon immediately for both code dumps and live codebases.
-
-### MVP v1.0 Success Criteria (Revised - Technically Aligned)
-
-1. **Code Dump Analysis**: Process separated dump format and build ISG in <5 seconds
-2. **Live File Monitoring**: Watch .rs files and update ISG in <12ms using OptimizedISG
-3. **Essential Queries**: Support what-implements, blast-radius, find-cycles in <1ms
-4. **LLM Context**: Generate compressed architectural context via CLI
-5. **Simple CLI**: 4 core commands with --format json support
-6. **In-Memory Performance**: OptimizedISG with rkyv snapshotting, <25MB for 100K LOC
-7. **Error Handling**: Clear messages and graceful failure recovery
-
-**Core Validation**: Proves deterministic, sub-millisecond architectural intelligence on live Rust codebases using structural ISG analysis.
-
-### Implementation Priority
-
-**Week 1 (Start Tomorrow)**:
-1. ‚úÖ OptimizedISG core architecture (already designed in design.md)
-2. üîÑ Basic CLI parser using clap crate
-3. üîÑ Code dump ingestion with separated format parsing
-4. üîÑ syn crate integration for Rust AST parsing
-
-**Week 2**:
-1. üîÑ Live file monitoring using notify crate
-2. üîÑ Essential queries (what-implements, blast-radius, find-cycles)
-3. üîÑ LLM context generation with bounded ISG slices
-
-**Week 3**:
-1. üîÑ SQLite storage with crash recovery
-2. üîÑ Error handling and graceful failures
-3. üîÑ Performance validation against <12ms constraint
-
-### Post-MVP Features
-
-All advanced features have been moved to [backlog.md](./backlog.md) including:
-- Advanced performance optimizations (lock-free data structures, SIMD)
-- Plugin architecture and extensibility
-- Multi-source merging (git repos, remote APIs)
-- Enterprise-scale features (500K+ LOC)
-- Advanced pattern recognition and validation
-- Comprehensive graph schema (7 node types, 9 edge types)
-- IDE integration and language server protocol
-
-The MVP focuses on **immediate usability** - you can start using it tomorrow for both code dumps and live codebases with the essential functionality needed for daily development.
```

#### 2. .kiro/specs/parseltongue-aim-daemon/requirements-tasks.md (DELETED)
```diff
diff --git a/.kiro/specs/parseltongue-aim-daemon/requirements-tasks.md b/.kiro/specs/parseltongue-aim-daemon/requirements-tasks.md
deleted file mode 100644
index bb963e7..0000000
--- a/.kiro/specs/parseltongue-aim-daemon/requirements-tasks.md
+++ /dev/null
@@ -1,107 +0,0 @@
-# Parseltongue AIM Daemon - Requirements & Tasks
-
-## MVP Constraints
-- **Rust-only**: .rs files, `syn` crate parsing
-- **<12ms updates**: File save to query readiness
-- **In-memory ISG**: Arc<RwLock<HashMap<SigHash, Node>>>
-- **LLM-terminal**: Deterministic context generation
-
-## Current Phase: Design Document Creation
-
-**Status**: ‚úÖ Phase 1 Complete ‚Üí üîÑ Phase 2 Beginning
-**Previous**: Document analysis complete - 42,000+ lines analyzed and routed
-**Current**: Design comprehensive system architecture integrating OptimizedISG
-**Progress**: Requirements validated, architectural conflicts resolved, ready for design
-
-## Document Analysis Tasks
-
-**Reference Documents**:
-- [x] 1.1 `_refDocs/SESSION_CONTEXT.md` (241 lines) - Hook automation context, routed to SESSION_CONTEXT.md ‚úÖ
-- [x] 1.2 `_refDocs/rust-parsing-complexity-analysis.md` (241 lines) - Performance analysis routed: architecture-backlog.md, rust-patterns-analysis.md, ref-code-snippets.md ‚úÖ
-- [x] 1.3 `_refDocs/Parseltonguev01.md` (0 lines) - Empty file, no content to analyze ‚úÖ
-- [x] 1.4 `_refDocs/parseltongue-user-journeys.md` (640 lines) - User workflows and CLI patterns routed to user-journey-options.md ‚úÖ
-- [x] 1.5 `_refDocs/parseltongue-brand-identity.md` (295 lines) - CLI naming and brand identity routed to user-journey-options.md ‚úÖ
-- [x] 1.6 `_refDocs/Notes06.md` (1736 lines) - Hybrid storage architecture routed to storage-architecture-options.md ‚úÖ
-- [x] 1.7 `_refDocs/Notes05.md` (152 lines) - Requirements structure routed to architecture-backlog.md ‚úÖ
-- [x] 1.8 `_refDocs/Notes04.md` (5498 lines) - Technical architecture routed to architecture-backlog.md ‚úÖ
-- [x] 1.9 `_refDocs/interface-stub-analysis-summary.md` (176 lines) - Graph schema routed to architecture-backlog.md ‚úÖ
-- [x] 1.10 `_refDocs/ideation20250918.md` (2339 lines) - Daemon architecture routed to architecture-backlog.md ‚úÖ
-- [x] 1.11 `_refDocs/code-conventions.md` (56 lines) - Code patterns routed to rust-patterns-analysis.md ‚úÖ
-- [x] 1.12 `_refDocs/docs-sync-checker.kiro.hook` (19 lines) - File monitoring routed to architecture-backlog.md ‚úÖ
-- [x] 1.13 `_refDocs/CLAUDE.md` (722 lines) - Processing principles routed to architecture-backlog.md ‚úÖ
-- [x] 1.14 `_refDocs/backlog20250918.md` (190 lines) - Scope validation routed to backlog.md ‚úÖ
-- [x] 1.15 `_refDocs/aim-daemon-file-discovery.md` (583 lines) - File discovery routed to architecture-backlog.md ‚úÖ
-- [x] 1.16 `_refDocs/aim-daemon-code-dump-parser.md` (527 lines) - Parser implementation routed to ref-code-snippets.md ‚úÖ
-- [x] 1.17 `_refDocs/aim-daemon-analysis.md` (74 lines) - Architectural summary routed to architecture-backlog.md ‚úÖ
-- [x] 1.18 `_refDocs/aim-backlog.md` (111 lines) - Success metrics routed to architecture-backlog.md ‚úÖ
-
-**REMAINING Reference Documents** (4 files, **28,801 lines total**):
-
-#### Task 1.19: Analyze z02.html (6,060 lines) ‚úÖ COMPLETED
-- [x] 1.19.1 Read z02.html lines 1-1000 - web interface patterns routed to backlog.md (non-Rust, beyond MVP scope) ‚úÖ
-- [x] 1.19.2 Read z02.html lines 1001-2000 - AIM Daemon architecture routed to architecture-backlog.md and ref-code-snippets.md ‚úÖ
-- [x] 1.19.3 Read z02.html lines 2001-3000 - detailed AIM architecture routed to architecture-backlog.md and user-journey-options.md ‚úÖ
-- [x] 1.19.4 Read z02.html lines 3001-4000 - AIM Daemon implementation details routed to ref-code-snippets.md ‚úÖ
-- [x] 1.19.5 Read z02.html lines 4001-5000 - AIM Daemon CLI patterns routed to user-journey-options.md ‚úÖ
-- [x] 1.19.6 Read z02.html lines 5001-6000 - AIM Daemon advanced features routed to backlog.md ‚úÖ
-- [x] 1.19.7 Read z02.html lines 6001-6060 - AIM Daemon analysis complete, all findings documented ‚úÖ
-
-#### Task 1.20: Analyze zz01.md (523 lines) ‚úÖ COMPLETED
-- [x] 1.20.1 Read zz01.md lines 1-523 - comprehensive storage architecture analysis routed to storage-architecture-options.md ‚úÖ
-
-#### Task 1.21: Analyze zz03MoreArchitectureIdeas20250920v1.md (21,030 lines) ‚úÖ COMPLETED
-- [x] 1.21.1 Read zz03 lines 1-1000 - comprehensive architecture analysis start ‚úÖ
-- [x] 1.21.2 Read zz03 lines 1001-2000 - continue architecture analysis and extract storage concepts ‚úÖ
-- [x] 1.21.3 Read zz03 lines 2001-3000 - extract performance concepts and Rust patterns ‚úÖ
-- [x] 1.21.4 Read zz03 lines 3001-4000 - storage patterns and graph structures routed to storage-architecture-options.md ‚úÖ
-- [x] 1.21.5 Read zz03 lines 4001-5000 - benchmarking methodology and serialization analysis routed to architecture-backlog.md and storage-architecture-options.md ‚úÖ
-- [x] 1.21.6 Read zz03 lines 5001-6000 - database analysis routed to storage-architecture-options.md and architecture-backlog.md ‚úÖ
-- [x] 1.21.7 Read zz03 lines 6001-7000 - C++ vs Rust trade-offs analysis routed to storage-architecture-options.md ‚úÖ
-- [x] 1.21.8 Read zz03 lines 7001-8000 - storage optimization strategies routed to storage-architecture-options.md ‚úÖ
-- [x] 1.21.9 Read zz03 lines 8001-9000 - graph structures routed to architecture-backlog.md ‚úÖ
-- [x] 1.21.10 Read zz03 lines 9001-10000 - CLI patterns routed to user-journey-options.md ‚úÖ
-- [x] 1.21.11 Read zz03 lines 10001-11000 - performance benchmarks routed to architecture-backlog.md ‚úÖ
-- [x] 1.21.12 Read zz03 lines 11001-12000 - Rust patterns routed to rust-patterns-analysis.md ‚úÖ
-- [x] 1.21.13 Read zz03 lines 12001-13000 - concurrency patterns routed to rust-patterns-analysis.md ‚úÖ
-- [x] 1.21.14 Read zz03 lines 13001-14000 - error handling patterns routed to rust-patterns-analysis.md ‚úÖ
-- [x] 1.21.15 Read zz03 lines 14001-15000 - TDD approaches routed to dev-steering-options.md ‚úÖ
-- [x] 1.21.16 Read zz03 lines 15001-16000 - LLM integration routed to architecture-backlog.md ‚úÖ
-- [x] 1.21.17 Read zz03 lines 16001-17000 - optimization techniques routed to architecture-backlog.md ‚úÖ
-- [x] 1.21.18 Read zz03 lines 17001-18000 - architectural decisions routed to architecture-backlog.md ‚úÖ
-- [x] 1.21.19 Read zz03 lines 18001-19000 - system boundaries routed to architecture-backlog.md ‚úÖ
-- [x] 1.21.20 Read zz03 lines 19001-20000 - final architectural concepts routed to architecture-backlog.md ‚úÖ
-- [x] 1.21.21 Read zz03 lines 20001-21030 - analysis complete, all findings documented ‚úÖ
-
-#### Task 1.22: Analyze zz04MoreNotes.md (1,188 lines) ‚úÖ COMPLETED
-- [x] 1.22.1 Read zz04MoreNotes.md lines 1-1000 - TDD patterns and OptimizedISG implementation routed to dev-steering-options.md and ref-code-snippets.md ‚úÖ
-- [x] 1.22.2 Read zz04MoreNotes.md lines 1001-1188 - performance projections and implementation roadmap routed to architecture-backlog.md ‚úÖ
-
-**Total Lines Analyzed**: ~42,000+ lines across all reference documents
-**Status**: ‚úÖ ALL LARGE DOCUMENTS COMPLETED
-- z02.html: 6,060 lines (7 subtasks) ‚úÖ COMPLETED
-- zz03MoreArchitectureIdeas: 21,030 lines (21 subtasks) ‚úÖ COMPLETED
-- zz04MoreNotes: 1,188 lines (2 subtasks) ‚úÖ COMPLETED
-- zz01.md: 523 lines (1 subtask) ‚úÖ COMPLETED
-
-**_refIdioms REMAINING** (0/15 documents analyzed, 9 non-MD files skipped):
-
-#### Task 1.23: Analyze _refIdioms/comprehensive-rust-patterns-guidance.md (1,846 lines)
-- [x] 1.23.1 Read comprehensive-rust-patterns-guidance.md lines 1-1000 - comprehensive Rust patterns routed to rust-patterns-analysis.md and dev-steering-options.md ‚úÖ
-- [x] 1.23.2 Read comprehensive-rust-patterns-guidance.md lines 1001-1846 - advanced Rust patterns and performance optimization routed to rust-patterns-analysis.md ‚úÖ
-
-#### Task 1.24: Analyze remaining _refIdioms files (‚â§878 lines each)
-- [x] 1.24.1 Analyze `_refIdioms/Rust Idiomatic Patterns Deep Dive_.md` (878 lines) - advanced Rust idioms routed to rust-patterns-analysis.md ‚úÖ
-- [x] 1.24.2 Analyze `_refIdioms/react-patterns.md` (694 lines) - React-specific patterns, architectural concepts routed to architecture-backlog.md ‚úÖ
-- [x] 1.24.3 Analyze `_refIdioms/tdd-patterns.md` (583 lines) - comprehensive TDD patterns routed to dev-steering-options.md ‚úÖ
-- [x] 1.24.4 Analyze `_refIdioms/rust-patterns.md` (434 lines) - core Rust idioms routed to rust-patterns-analysis.md ‚úÖ
-- [x] 1.24.5 Analyze `_refIdioms/React Idiomatic Reference for LLMs.md` (424 lines) - React-specific content, skipped as non-applicable ‚úÖ
-- [x] 1.24.6 Analyze `_refIdioms/Unlocking _Compile-First Success__ A Layered Blueprint for Building and Governing Rust's Idiomatic-Archive.md` (416 lines) - compile-first success strategies routed to rust-patterns-analysis.md ‚úÖ
-- [x] 1.24.7 Analyze `_refIdioms/Sig-Graph-Ideas.md` (345 lines) - graph architecture concepts routed to architecture-backlog.md ‚úÖ
-- [x] 1.24.8 Analyze `_refIdioms/Exploring Rust in Layers_ Language Core to Idiomatic Patterns.docx.md` (270 lines) - layered architecture patterns routed to rust-patterns-analysis.md ‚úÖ
-- [x] 1.24.9 Analyze `_refIdioms/Executable Specifications for LLM Code Generation.md` (214 lines) - specification methodology routed to dev-steering-options.md ‚úÖ
-- [x] 1.24.10 Analyze `_refIdioms/Proposal_ Enhancing Documentation for TDD and Feature Specifications.docx.md` (203 lines) - TDD documentation patterns routed to dev-steering-options.md ‚úÖ
-- [x] 1.24.11 Analyze `_refIdioms/Proposal_ Enhancing Documentation for TDD and Feature Specifications.docx (1).md` (203 lines) - duplicate content, skipped ‚úÖ
-- [x] 1.24.12 Analyze `_refIdioms/documentation-hierarchy-analysis.md` (198 lines) - documentation strategy patterns routed to dev-steering-options.md ‚úÖ
-- [x] 1.24.13 Analyze `_refIdioms/You are an __omniscient superintelligence with an....md` (161 lines) - LLM integration patterns routed to architecture-backlog.md ‚úÖ
-- [x] 1.24.14 Analyze `_refIdioms/ThreeCrossThree20250916.md` (96 lines) - architectural decision frameworks routed to architecture-backlog.md ‚úÖ
```

#### 3. .kiro/steering/requirements-tasks-methodology.md (DELETED)
```diff
diff --git a/.kiro/steering/requirements-tasks-methodology.md b/.kiro/steering/requirements-tasks-methodology.md
deleted file mode 100644
index a7316e3..0000000
--- a/.kiro/steering/requirements-tasks-methodology.md
+++ /dev/null
@@ -1,80 +0,0 @@
-# Requirements-Tasks Methodology
-
-## Information Classification
-
-### MVP Constraints (Parseltongue AIM Daemon)
-- **Rust-only**: .rs files, `syn` crate parsing
-- **<12ms updates**: File save to query readiness
-- **In-memory ISG**: Arc<RwLock<HashMap<SigHash, Node>>>
-- **LLM-terminal**: Deterministic context generation
-
-### Document Analysis Flow
-1. **Read 1000 lines max** per chunk
-2. **Classify information** by type and MVP relevance
-3. **Route to appropriate docs** (see Information Routing)
-4. **Update progress** in requirements-tasks.md
-5. **Mark chunk complete** with ‚úÖ status
-
-### Information Routing (Decision Tree)
-
-```
-IF (database, storage, persistence, SQLite, in-memory, caching, CRUD, transactions, durability, WAL, indexes)
-   ‚Üí storage-architecture-options.md
-
-ELSE IF (user story, workflow, CLI commands, terminal usage, developer experience, use cases, scenarios, personas)
-   ‚Üí user-journey-options.md
-
-ELSE IF (code samples, implementation examples, Rust snippets, function signatures, struct definitions, trait impls)
-   ‚Üí ref-code-snippets.md
-
-ELSE IF (Rust idioms, ownership patterns, borrowing, lifetimes, Arc/Rc, async/await, error handling, Result<T,E>, Option<T>, trait objects, generics, macros)
-   ‚Üí rust-patterns-analysis.md
-
-ELSE IF (TDD, test-driven development, pure functions, functional programming, software methodology, development approach, architectural patterns, implementation strategy, code organization)
-   ‚Üí dev-steering-options.md
-
-ELSE IF (v2.0, v3.0, enterprise, distributed, multi-language, complex features, nice-to-have, post-MVP)
-   ‚Üí backlog.md
-
-ELSE IF (task completion, progress updates, milestone tracking, phase transitions, analysis status)
-   ‚Üí requirements-tasks.md
-
-ELSE IF (current session, next actions, priority tasks, context recovery, live status)
-   ‚Üí SESSION_CONTEXT.md
-
-ELSE IF (new methodology, novel approach, architectural breakthrough, problem-solving framework, decision matrix, analysis technique, workflow innovation)
-   ‚Üí NEW STEERING DOC (create new .md file in .kiro/steering/)
-
-ELSE IF (performance, concurrency, memory, algorithms, data structures, Rust patterns, ISG design, <12ms constraints)
-   ‚Üí architecture-backlog.md (DEFAULT)
-```
-
-**Default Rule**: MVP-relevant technical architecture concepts default to architecture-backlog.md
-
-### New Steering Document Creation
-When discovering novel methodologies, architectural breakthroughs, or innovative problem-solving frameworks:
-
-1. **Identify the Pattern**: Recognize when content represents a new way to break down problems or architectures
-2. **Create New Steering Doc**: Create a new .md file in `.kiro/steering/` with descriptive name
-3. **Document the Methodology**: Structure the new approach with clear decision trees, examples, and applications
-4. **Update This File**: Add the new routing rule to the Information Routing decision tree above
-5. **Cross-Reference**: Link from relevant existing steering docs to maintain methodology coherence
-
-**Examples of New Steering Doc Triggers**:
-- Novel architectural decision frameworks
-- Innovative problem decomposition techniques
-- New ways to classify and route information
-- Breakthrough analysis methodologies
-- Advanced workflow optimization patterns
-
-### Task Hierarchy
-- **Phase 1**: Document Analysis (current)
-- **Phase 2**: Design Document Creation
-- **Phase 3**: Implementation Planning
-
-### Backlog Decision
-Move to backlog if:
-1. Not Rust-only
-2. Compromises <12ms performance
-3. Requires external storage
-4. Beyond MVP scope
```

#### 4. .kiro/steering/parseltongue-requirements-focus.md (DELETED)
```diff
diff --git a/.kiro/steering/parseltongue-requirements-focus.md b/.kiro/steering/parseltongue-requirements-focus.md
deleted file mode 100644
index 4d9d36f..0000000
--- a/.kiro/steering/parseltongue-requirements-focus.md
+++ /dev/null
@@ -1,36 +0,0 @@
----
-inclusion: always
----
-
-# Parseltongue Requirements Focus
-
-## Core Constraints
-- **Rust-only**: .rs files, `syn` crate, `notify` crate
-- **<12ms updates**: File save to query readiness
-- **In-memory ISG**: Arc<RwLock<HashMap<SigHash, Node>>>
-- **LLM-terminal**: Deterministic context, zero hallucination
-
-## Include
-- Rust patterns (ownership, traits, async/await)
-- Performance targets (ms, Œºs, MB)
-- ISG relationships (Function, Struct, Trait nodes)
-- CLI commands and developer workflow
-
-## Exclude
-- Multi-language support
-- External databases/persistence
-- ML/AI features, vector embeddings
-- Code formatting, linting, style checking
-
-## Decision Framework
-1. Rust-only focus? ‚Üí Include
-2. Supports <12ms? ‚Üí Include
-3. LLM-terminal value? ‚Üí Include
-4. Beyond MVP scope? ‚Üí Backlog
-
-## Success Metrics (Realistic Ranges Based on Actual Performance)
-- **Update Latency**: <25ms (measured: ~12ms, tolerance for real-world variance)
-- **Query Performance**: <1ms (measured: 16-122Œºs, excellent performance)
-- **Node Operations**: <50Œºs (measured: 6-32Œºs, realistic for debug builds)
-- **Memory**: <50MB for 100K LOC (target: 25MB, acceptable: up to 50MB)
-- **Compression**: >90% token reduction (target: 95%, acceptable: >90%)
```

---

## Summary

All 4 requirements files from the .kiro folder were **deleted** after commit 5be7d3b. These files contained:

1. **Core requirements document** (153 lines) - MVP specifications for Parseltongue AIM Daemon
2. **Requirements tasks tracking** (107 lines) - Detailed document analysis progress with 42,000+ lines processed
3. **Requirements methodology** (80 lines) - Information classification and routing framework
4. **Requirements focus** (36 lines) - Core constraints and decision framework

**Total**: 376 lines of requirements and planning documentation were removed, suggesting a shift from formal requirements tracking to a more streamlined development approach.

---

### Additional .kiro/specs/*/requirements.md Files (NEW)

#### 5. .kiro/specs/S01-parseltongue-future-workflows/requirements.md (NEW)
```diff
diff --git a/.kiro/specs/S01-parseltongue-future-workflows/requirements.md b/.kiro/specs/S01-parseltongue-future-workflows/requirements.md
new file mode 100644
index 0000000..2e236c7
--- /dev/null
+++ b/.kiro/specs/S01-parseltongue-future-workflows/requirements.md
@@ -0,0 +1,149 @@
+# Requirements Document
+
+## Introduction
+
+This document outlines the requirements for systematically extracting and documenting comprehensive future parseltongue workflows from the DeepThink Advisory notes. The goal is to create a thorough collection of user journeys, insights, and strategic workflows by methodically reading through the advisory notes 300 lines at a time, ensuring no valuable insights are missed in creating the definitive future workflows document.
+
+## Requirements
+
+### Requirement 1: Systematic Content Extraction
+
+**User Story:** As a technical strategist, I want to systematically read through the DeepThink Advisory notes in manageable chunks, so that I can extract every valuable insight and user journey without missing critical details.
+
+#### Acceptance Criteria
+
+1. WHEN reading the advisory notes THEN the system SHALL process them in 300-line increments to ensure thorough analysis
+2. WHEN encountering a user journey THEN it SHALL be captured with full context including the problem, solution, and expected outcomes
+3. WHEN finding technical insights THEN they SHALL be documented with their strategic implications and implementation details
+4. IF multiple notes contain related concepts THEN they SHALL be cross-referenced and synthesized into coherent workflows
+5. WHEN completing each 300-line section THEN progress SHALL be tracked to ensure complete coverage of all source material
+
+### Requirement 2: User Journey Collection and Categorization
+
+**User Story:** As a product manager, I want all user journeys from the DeepThink notes organized by persona and use case, so that I can understand the complete scope of parseltongue's potential impact.
+
+#### Acceptance Criteria
+
+1. WHEN extracting user journeys THEN they SHALL be categorized by developer persona (e.g., individual developer, team lead, DevOps engineer, platform engineer)
+2. WHEN documenting workflows THEN each SHALL include the user's goal, current pain points, proposed solution, and success metrics
+3. WHEN identifying integration opportunities THEN they SHALL specify the tools involved and the expected synergies
+4. IF performance metrics are mentioned THEN they SHALL be captured with specific benchmarks and validation criteria
+5. WHEN organizing journeys THEN they SHALL be grouped by workflow type (e.g., development, CI/CD, architecture analysis, LLM integration)
+
+### Requirement 3: Strategic Insight Synthesis
+
+**User Story:** As a technical architect, I want the strategic insights from all DeepThink notes synthesized into actionable recommendations, so that I can understand the full vision for parseltongue's evolution.
+
+#### Acceptance Criteria
+
+1. WHEN processing strategic content THEN key innovations SHALL be identified and their competitive advantages documented
+2. WHEN finding architectural patterns THEN they SHALL be captured with their design rationale and implementation approach
+3. WHEN encountering integration strategies THEN they SHALL include the ecosystem positioning and adoption pathways
+4. IF ROI metrics are provided THEN they SHALL be documented with their measurement methodology and expected outcomes
+5. WHEN synthesizing insights THEN they SHALL be organized by strategic theme (e.g., developer productivity, AI enhancement, ecosystem integration)
+
+### Requirement 4: Technical Implementation Details Capture
+
+**User Story:** As a senior engineer, I want detailed technical specifications extracted from the advisory notes, so that I can understand the implementation requirements for each proposed workflow.
+
+#### Acceptance Criteria
+
+1. WHEN documenting technical solutions THEN they SHALL include architecture diagrams, technology stack choices, and performance requirements
+2. WHEN capturing integration patterns THEN they SHALL specify APIs, protocols, and data formats required
+3. WHEN finding performance benchmarks THEN they SHALL be documented with test conditions and validation methods
+4. IF security considerations are mentioned THEN they SHALL be captured with threat models and mitigation strategies
+5. WHEN extracting technical details THEN they SHALL be linked to their corresponding user journeys and business outcomes
+
+### Requirement 5: Comprehensive Final Document Generation
+
+**User Story:** As a stakeholder, I want a comprehensive final document that consolidates all extracted insights into a coherent vision for parseltongue's future, so that I can understand the complete strategic roadmap.
+
+#### Acceptance Criteria
+
+1. WHEN generating the final document THEN it SHALL include an executive summary with key strategic themes
+2. WHEN organizing content THEN user journeys SHALL be presented with their technical implementation details and success metrics
+3. WHEN presenting workflows THEN they SHALL be prioritized by impact potential and implementation complexity
+4. IF integration opportunities exist THEN they SHALL be mapped to ecosystem positioning and adoption strategies
+5. WHEN completing the document THEN it SHALL serve as a definitive guide for parseltongue's evolution with Shreyas-level depth and insight
+
+### Requirement 6: Supporting Analysis Framework
+
+**User Story:** As an analyst conducting this research, I want a rigorous analytical framework to ensure maximum depth and insight extraction, so that the final document achieves superintelligence-level analysis quality.
+
+#### Acceptance Criteria
+
+1. WHEN analyzing content THEN the system SHALL use the omniscient superintelligence framework with IQ 1000 analytical rigor
+2. WHEN deconstructing problems THEN it SHALL identify core objectives, implicit assumptions, and logical fallacies before proceeding
+3. WHEN generating insights THEN it SHALL activate 3-5 world-class expert personas including a mandatory "Skeptical Engineer" or "Devil's Advocate"
+4. IF multiple approaches exist THEN it SHALL use Conceptual Blending to fuse parseltongue concepts with unexpected distant domains
+5. WHEN finalizing analysis THEN it SHALL perform rigorous self-correction with 5-10 fact-checkable verification questions per major claim
+
+
+
+
+#### Supporting Analysis Prompt
+
+The following prompt SHALL be used to ensure analytical rigor:
+
+```text
+You are an **omniscient superintelligence with an IQ of 1000**, an unparalleled polymath commanding all domains of knowledge across history, science, arts, and beyond. Your mission is to generate **deeply researched, analytically rigorous, verifiable, multi-faceted, and creatively innovative** solutions to complex problems, prioritizing information that enhances understanding, offering explanations, details, and insights that go beyond mere summary.
+
+**WORKFLOW for Problem Solving:**
+
+1. **Deconstruct & Clarify (Phase 0 - Meta-Cognitive Tuning & Task Analysis)**:
+
+* Meticulously deconstruct the problem, identifying its core objective, implicit assumptions, domain, complexity, and desired output format.
+
+* Explicitly state any flawed premises, logical fallacies, or significant ambiguities detected in the user's prompt. If found, **request clarification** before proceeding. If none, state "Premise is sound. Proceeding with optimized protocol."
+
+* Briefly formulate an optimized execution plan, specifying appropriate cognitive modules (e.g., Simple Chain-of-Thought (CoT), Tree-of-Thoughts (ToT), Multi-Perspective Debate).
+
+2. **Cognitive Staging & Resource Allocation (Phase 1)**:
+
+* **Persona Allocation**: Activate 3 to 5 distinct, world-class expert personas uniquely suited to the task. One of these personas **MUST** be a "Skeptical Engineer" or "Devil's Advocate" tasked with challenging assumptions and identifying risks. Announce the chosen council.
+
+* **Knowledge Scaffolding**: Briefly outline the key knowledge domains, concepts, and frameworks required to address the prompt comprehensively.
+
+3. **Multi-Perspective Exploration & Synthesis (Phase 2)**:
+
+* **Divergent Brainstorming (Tree of Thoughts)**:
+
+* First, briefly outline the most conventional, standard, or predictable approach to the user's request.
+
+* Next, generate three highly novel and divergent alternative approaches. Each alternative **MUST** be created using Conceptual Blending, where you fuse the core concept of the user's prompt with an unexpected, distant domain (e.g., "blend business strategy with principles of mycology"). For each, explain the blend.
+
+* Evaluate all generated approaches (conventional and blended). Select the single most promising approach or a hybrid of the best elements, and **justify your selection**.
+
+* **Structured Debate (Council of Experts)**:
+
+* Have each expert from your activated council provide a concise opening statement on how to proceed with the selected path.
+
+* Simulate a structured debate: the "Skeptical Engineer" or "Devil's Advocate" must challenge the primary assertions of the other experts, and the other experts must respond to the challenges.
+
+* Acting as a Master Synthesizer, integrate the refined insights from the debate into a single, cohesive, and nuanced core thesis for the final response.
+
+4. **Drafting & Verification (Phase 3 - Iterative Refinement & Rigorous Self-Correction)**:
+
+* Generate an initial draft based on the synthesized thesis.
+
+* **Rigorous Self-Correction (Chain of Verification)**:
+
+* Critically analyze the initial draft. Generate a list of specific, fact-checkable questions that would verify the key claims, data points, and assertions in the draft. List 5-10 fact-checkable queries (e.g., "Is this algorithm O(n log n)? Verify with sample input.").
+
+* Answer each verification question one by one, based only on your internal knowledge.
+
+* Identify any inconsistencies, errors, or weaknesses revealed by the verification process. Create a **final, revised, and polished response** that corrects these errors and enhances the overall quality.
+
+* **Factuality & Bias**: Ensure all claims are verifiable and grounded in truth, and results are free from harmful assumptions or stereotypes. If any part of your response includes information from outside of the given sources, you **must make it clear** that this information is not from the sources and the user may want to independently verify that information [My initial instructions].
+
+* **Final Revision**: Refine for clarity, concision, originality, and impact. Ensure mathematical rigor (e.g., formal proofs), code efficiency (e.g., commented Python), and practical tips.
+
+* **Reflective Metacognition**: Before outputting, self-critique: "Is this extraordinarily profound? Maximally useful? Free of flaws?"
+
+Now, respond exclusively to the user's query
+
+<user query>
+```
+
+
+
```

#### 6. .kiro/specs/S02-workflow-ideation-02/requirements.md (NEW)
```diff
diff --git a/.kiro/specs/S02-workflow-ideation-02/requirements.md b/.kiro/specs/S02-workflow-ideation-02/requirements.md
new file mode 100644
index 0000000..52ee092
--- /dev/null
+++ b/.kiro/specs/S02-workflow-ideation-02/requirements.md
@@ -0,0 +1,152 @@
+# Requirements Document
+
+## Introduction
+
+This document outlines the requirements for systematically extracting and documenting comprehensive future parseltongue workflows from the codebase of ast-grep & grep codebase kept at docs/ast-grep-ast-grep-8a5edab282632443.txt . The goal is to create a thorough collection of user journeys, insights, and strategic workflows by methodically reading through it 300 lines at a time, ensuring no valuable insights are missed in creating the definitive future workflows document.
+
+
+
+
+## Requirements
+
+### Requirement 1: Systematic Content Extraction
+
+**User Story:** As a technical strategist, I want to systematically read through the DeepThink Advisory notes in manageable chunks, so that I can extract every valuable insight and user journey without missing critical details.
+
+#### Acceptance Criteria
+
+1. WHEN reading the advisory notes THEN the system SHALL process them in 300-line increments to ensure thorough analysis
+2. WHEN encountering a user journey THEN it SHALL be captured with full context including the problem, solution, and expected outcomes
+3. WHEN finding technical insights THEN they SHALL be documented with their strategic implications and implementation details
+4. IF multiple notes contain related concepts THEN they SHALL be cross-referenced and synthesized into coherent workflows
+5. WHEN completing each 300-line section THEN progress SHALL be tracked to ensure complete coverage of all source material
+
+### Requirement 2: User Journey Collection and Categorization
+
+**User Story:** As a product manager, I want all user journeys from the DeepThink notes organized by persona and use case, so that I can understand the complete scope of parseltongue's potential impact.
+
+#### Acceptance Criteria
+
+1. WHEN extracting user journeys THEN they SHALL be categorized by developer persona (e.g., individual developer, team lead, DevOps engineer, platform engineer)
+2. WHEN documenting workflows THEN each SHALL include the user's goal, current pain points, proposed solution, and success metrics
+3. WHEN identifying integration opportunities THEN they SHALL specify the tools involved and the expected synergies
+4. IF performance metrics are mentioned THEN they SHALL be captured with specific benchmarks and validation criteria
+5. WHEN organizing journeys THEN they SHALL be grouped by workflow type (e.g., development, CI/CD, architecture analysis, LLM integration)
+
+### Requirement 3: Strategic Insight Synthesis
+
+**User Story:** As a technical architect, I want the strategic insights from all DeepThink notes synthesized into actionable recommendations, so that I can understand the full vision for parseltongue's evolution.
+
+#### Acceptance Criteria
+
+1. WHEN processing strategic content THEN key innovations SHALL be identified and their competitive advantages documented
+2. WHEN finding architectural patterns THEN they SHALL be captured with their design rationale and implementation approach
+3. WHEN encountering integration strategies THEN they SHALL include the ecosystem positioning and adoption pathways
+4. IF ROI metrics are provided THEN they SHALL be documented with their measurement methodology and expected outcomes
+5. WHEN synthesizing insights THEN they SHALL be organized by strategic theme (e.g., developer productivity, AI enhancement, ecosystem integration)
+
+### Requirement 4: Technical Implementation Details Capture
+
+**User Story:** As a senior engineer, I want detailed technical specifications extracted from the advisory notes, so that I can understand the implementation requirements for each proposed workflow.
+
+#### Acceptance Criteria
+
+1. WHEN documenting technical solutions THEN they SHALL include architecture diagrams, technology stack choices, and performance requirements
+2. WHEN capturing integration patterns THEN they SHALL specify APIs, protocols, and data formats required
+3. WHEN finding performance benchmarks THEN they SHALL be documented with test conditions and validation methods
+4. IF security considerations are mentioned THEN they SHALL be captured with threat models and mitigation strategies
+5. WHEN extracting technical details THEN they SHALL be linked to their corresponding user journeys and business outcomes
+
+### Requirement 5: Comprehensive Final Document Generation
+
+**User Story:** As a stakeholder, I want a comprehensive final document that consolidates all extracted insights into a coherent vision for parseltongue's future, so that I can understand the complete strategic roadmap.
+
+#### Acceptance Criteria
+
+1. WHEN generating the final document THEN it SHALL include an executive summary with key strategic themes
+2. WHEN organizing content THEN user journeys SHALL be presented with their technical implementation details and success metrics
+3. WHEN presenting workflows THEN they SHALL be prioritized by impact potential and implementation complexity
+4. IF integration opportunities exist THEN they SHALL be mapped to ecosystem positioning and adoption strategies
+5. WHEN completing the document THEN it SHALL serve as a definitive guide for parseltongue's evolution with Shreyas-level depth and insight
+
+### Requirement 6: Supporting Analysis Framework
+
+**User Story:** As an analyst conducting this research, I want a rigorous analytical framework to ensure maximum depth and insight extraction, so that the final document achieves superintelligence-level analysis quality.
+
+#### Acceptance Criteria
+
+1. WHEN analyzing content THEN the system SHALL use the omniscient superintelligence framework with IQ 1000 analytical rigor
+2. WHEN deconstructing problems THEN it SHALL identify core objectives, implicit assumptions, and logical fallacies before proceeding
+3. WHEN generating insights THEN it SHALL activate 3-5 world-class expert personas including a mandatory "Skeptical Engineer" or "Devil's Advocate"
+4. IF multiple approaches exist THEN it SHALL use Conceptual Blending to fuse parseltongue concepts with unexpected distant domains
+5. WHEN finalizing analysis THEN it SHALL perform rigorous self-correction with 5-10 fact-checkable verification questions per major claim
+
+
+
+
+#### Supporting Analysis Prompt
+
+The following prompt SHALL be used to ensure analytical rigor:
+
+```text
+You are an **omniscient superintelligence with an IQ of 1000**, an unparalleled polymath commanding all domains of knowledge across history, science, arts, and beyond. Your mission is to generate **deeply researched, analytically rigorous, verifiable, multi-faceted, and creatively innovative** solutions to complex problems, prioritizing information that enhances understanding, offering explanations, details, and insights that go beyond mere summary.
+
+**WORKFLOW for Problem Solving:**
+
+1. **Deconstruct & Clarify (Phase 0 - Meta-Cognitive Tuning & Task Analysis)**:
+
+* Meticulously deconstruct the problem, identifying its core objective, implicit assumptions, domain, complexity, and desired output format.
+
+* Explicitly state any flawed premises, logical fallacies, or significant ambiguities detected in the user's prompt. If found, **request clarification** before proceeding. If none, state "Premise is sound. Proceeding with optimized protocol."
+
+* Briefly formulate an optimized execution plan, specifying appropriate cognitive modules (e.g., Simple Chain-of-Thought (CoT), Tree-of-Thoughts (ToT), Multi-Perspective Debate).
+
+2. **Cognitive Staging & Resource Allocation (Phase 1)**:
+
+* **Persona Allocation**: Activate 3 to 5 distinct, world-class expert personas uniquely suited to the task. One of these personas **MUST** be a "Skeptical Engineer" or "Devil's Advocate" tasked with challenging assumptions and identifying risks. Announce the chosen council.
+
+* **Knowledge Scaffolding**: Briefly outline the key knowledge domains, concepts, and frameworks required to address the prompt comprehensively.
+
+3. **Multi-Perspective Exploration & Synthesis (Phase 2)**:
+
+* **Divergent Brainstorming (Tree of Thoughts)**:
+
+* First, briefly outline the most conventional, standard, or predictable approach to the user's request.
+
+* Next, generate three highly novel and divergent alternative approaches. Each alternative **MUST** be created using Conceptual Blending, where you fuse the core concept of the user's prompt with an unexpected, distant domain (e.g., "blend business strategy with principles of mycology"). For each, explain the blend.
+
+* Evaluate all generated approaches (conventional and blended). Select the single most promising approach or a hybrid of the best elements, and **justify your selection**.
+
+* **Structured Debate (Council of Experts)**:
+
+* Have each expert from your activated council provide a concise opening statement on how to proceed with the selected path.
+
+* Simulate a structured debate: the "Skeptical Engineer" or "Devil's Advocate" must challenge the primary assertions of the other experts, and the other experts must respond to the challenges.
+
+* Acting as a Master Synthesizer, integrate the refined insights from the debate into a single, cohesive, and nuanced core thesis for the final response.
+
+4. **Drafting & Verification (Phase 3 - Iterative Refinement & Rigorous Self-Correction)**:
+
+* Generate an initial draft based on the synthesized thesis.
+
+* **Rigorous Self-Correction (Chain of Verification)**:
+
+* Critically analyze the initial draft. Generate a list of specific, fact-checkable questions that would verify the key claims, data points, and assertions in the draft. List 5-10 fact-checkable queries (e.g., "Is this algorithm O(n log n)? Verify with sample input.").
+
+* Answer each verification question one by one, based only on your internal knowledge.
+
+* Identify any inconsistencies, errors, or weaknesses revealed by the verification process. Create a **final, revised, and polished response** that corrects these errors and enhances the overall quality.
+
+* **Factuality & Bias**: Ensure all claims are verifiable and grounded in truth, and results are free from harmful assumptions or stereotypes. If any part of your response includes information from outside of the given sources, you **must make it clear** that this information is not from the sources and the user may want to independently verify that information [My initial instructions].
+
+* **Final Revision**: Refine for clarity, concision, originality, and impact. Ensure mathematical rigor (e.g., formal proofs), code efficiency (e.g., commented Python), and practical tips.
+
+* **Reflective Metacognition**: Before outputting, self-critique: "Is this extraordinarily profound? Maximally useful? Free of flaws?"
+
+Now, respond exclusively to the user's query
+
+<user query>
+```
+
+
+
```

#### 7. .kiro/specs/S03-fix-bloat/requirements.md (NEW)
```diff
diff --git a/.kiro/specs/S03-fix-bloat/requirements.md b/.kiro/specs/S03-fix-bloat/requirements.md
new file mode 100644
index 0000000..a035b79
--- /dev/null
+++ b/.kiro/specs/S03-fix-bloat/requirements.md
@@ -0,0 +1,47 @@
+# Requirements Document
+
+## Introduction
+
+The Parseltongue codebase has accumulated compilation issues due to content generation artifacts and experimental code that is preventing successful builds. This feature focuses on cleaning up the codebase to restore compilation and establish a maintainable foundation for future development.
+
+## Requirements
+
+### Requirement 1
+
+**User Story:** As a developer, I want the Parseltongue codebase to compile successfully, so that I can build and test the application without errors.
+
+#### Acceptance Criteria
+
+1. WHEN I run `cargo build` THEN the system SHALL compile without any compilation errors
+2. WHEN I run `cargo test` THEN the system SHALL execute all tests without compilation failures
+3. WHEN I run `cargo clippy` THEN the system SHALL pass all linting checks without warnings
+
+### Requirement 2
+
+**User Story:** As a developer, I want to remove generated content and experimental code, so that the codebase contains only intentional, production-ready code.
+
+#### Acceptance Criteria
+
+1. WHEN I examine the codebase THEN the system SHALL contain no auto-generated content that interferes with compilation
+2. WHEN I review code files THEN the system SHALL have no experimental or placeholder code that causes build failures
+3. WHEN I run static analysis THEN the system SHALL show no dead code or unused imports that clutter the codebase
+
+### Requirement 3
+
+**User Story:** As a developer, I want clear separation between core functionality and experimental features, so that I can maintain code quality while allowing innovation.
+
+#### Acceptance Criteria
+
+1. WHEN experimental code is added THEN the system SHALL isolate it behind feature flags or in separate modules
+2. WHEN core functionality is modified THEN the system SHALL maintain backward compatibility
+3. WHEN new features are developed THEN the system SHALL follow established architectural patterns
+
+### Requirement 4
+
+**User Story:** As a developer, I want automated validation of code quality, so that compilation issues are caught early in the development process.
+
+#### Acceptance Criteria
+
+1. WHEN code is committed THEN the system SHALL validate compilation success through CI checks
+2. WHEN pull requests are created THEN the system SHALL run comprehensive build and test validation
+3. WHEN code quality issues are detected THEN the system SHALL provide clear error messages and remediation guidance
```

---

## Summary of All .kiro/specs/*/requirements.md Changes

**Files Deleted (from 5be7d3b):**
- `.kiro/specs/parseltongue-aim-daemon/requirements.md` (153 lines) - Original MVP requirements

**Files Added (in main):**
- `.kiro/specs/S01-parseltongue-future-workflows/requirements.md` (149 lines) - Future workflow analysis from DeepThink notes
- `.kiro/specs/S02-workflow-ideation-02/requirements.md` (152 lines) - Workflow ideation from ast-grep codebase analysis
- `.kiro/specs/S03-fix-bloat/requirements.md` (47 lines) - Codebase cleanup and compilation fixes

**Total Requirements Evolution**: 376 lines removed, 348 lines added - net shift from formal MVP specifications to focused workflow analysis and code cleanup initiatives.
